<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_xrzk5cfxtw4m-8>li:before{content:"\0025cf   "}.lst-kix_99xvjj4uyl11-5>li:before{content:"\0025a0   "}.lst-kix_99xvjj4uyl11-7>li:before{content:"\0025cb   "}.lst-kix_xrzk5cfxtw4m-7>li:before{content:"\0025cf   "}.lst-kix_99xvjj4uyl11-4>li:before{content:"\0025cb   "}.lst-kix_99xvjj4uyl11-8>li:before{content:"\0025a0   "}.lst-kix_xrzk5cfxtw4m-5>li:before{content:"\0025cf   "}ul.lst-kix_nz0skx7ahmk9-6{list-style-type:none}ul.lst-kix_nz0skx7ahmk9-7{list-style-type:none}.lst-kix_xrzk5cfxtw4m-4>li:before{content:"\0025cf   "}.lst-kix_xrzk5cfxtw4m-6>li:before{content:"\0025cf   "}ul.lst-kix_nz0skx7ahmk9-4{list-style-type:none}ul.lst-kix_nz0skx7ahmk9-5{list-style-type:none}ul.lst-kix_nz0skx7ahmk9-2{list-style-type:none}ul.lst-kix_nz0skx7ahmk9-3{list-style-type:none}ul.lst-kix_nz0skx7ahmk9-0{list-style-type:none}.lst-kix_99xvjj4uyl11-6>li:before{content:"\0025cf   "}ul.lst-kix_nz0skx7ahmk9-1{list-style-type:none}ul.lst-kix_3oqam8vafvbn-3{list-style-type:none}.lst-kix_xrzk5cfxtw4m-1>li:before{content:"\0025cf   "}ul.lst-kix_3oqam8vafvbn-4{list-style-type:none}ul.lst-kix_3oqam8vafvbn-5{list-style-type:none}.lst-kix_xrzk5cfxtw4m-0>li:before{content:"\0025cf   "}.lst-kix_xrzk5cfxtw4m-2>li:before{content:"\0025cf   "}ul.lst-kix_3oqam8vafvbn-6{list-style-type:none}ul.lst-kix_3oqam8vafvbn-7{list-style-type:none}.lst-kix_xrzk5cfxtw4m-3>li:before{content:"\0025cf   "}ul.lst-kix_3oqam8vafvbn-8{list-style-type:none}ul.lst-kix_g1ro4rg087e5-3{list-style-type:none}ul.lst-kix_g1ro4rg087e5-2{list-style-type:none}ul.lst-kix_g1ro4rg087e5-1{list-style-type:none}ul.lst-kix_g1ro4rg087e5-0{list-style-type:none}.lst-kix_woopwfqcy80n-3>li:before{content:"\0025cf   "}ul.lst-kix_g1ro4rg087e5-7{list-style-type:none}ul.lst-kix_g1ro4rg087e5-6{list-style-type:none}ul.lst-kix_g1ro4rg087e5-5{list-style-type:none}ul.lst-kix_g1ro4rg087e5-4{list-style-type:none}.lst-kix_woopwfqcy80n-0>li:before{content:"\0025cf   "}.lst-kix_woopwfqcy80n-4>li:before{content:"\0025cb   "}ul.lst-kix_g1ro4rg087e5-8{list-style-type:none}ul.lst-kix_stgkp8ts1530-0{list-style-type:none}.lst-kix_woopwfqcy80n-1>li:before{content:"\0025cb   "}ul.lst-kix_stgkp8ts1530-1{list-style-type:none}ul.lst-kix_stgkp8ts1530-2{list-style-type:none}.lst-kix_woopwfqcy80n-2>li:before{content:"\0025a0   "}ul.lst-kix_stgkp8ts1530-3{list-style-type:none}ul.lst-kix_stgkp8ts1530-4{list-style-type:none}ul.lst-kix_i83pglsmrtnv-8{list-style-type:none}ul.lst-kix_stgkp8ts1530-5{list-style-type:none}ul.lst-kix_stgkp8ts1530-6{list-style-type:none}ul.lst-kix_stgkp8ts1530-7{list-style-type:none}ul.lst-kix_stgkp8ts1530-8{list-style-type:none}ul.lst-kix_i83pglsmrtnv-4{list-style-type:none}ul.lst-kix_i83pglsmrtnv-5{list-style-type:none}.lst-kix_woopwfqcy80n-8>li:before{content:"\0025a0   "}ul.lst-kix_i83pglsmrtnv-6{list-style-type:none}ul.lst-kix_nz0skx7ahmk9-8{list-style-type:none}ul.lst-kix_i83pglsmrtnv-7{list-style-type:none}.lst-kix_woopwfqcy80n-7>li:before{content:"\0025cb   "}ul.lst-kix_i83pglsmrtnv-0{list-style-type:none}ul.lst-kix_i83pglsmrtnv-1{list-style-type:none}ul.lst-kix_i83pglsmrtnv-2{list-style-type:none}ul.lst-kix_i83pglsmrtnv-3{list-style-type:none}.lst-kix_woopwfqcy80n-5>li:before{content:"\0025a0   "}.lst-kix_woopwfqcy80n-6>li:before{content:"\0025cf   "}ul.lst-kix_hswfzmci3dc-0{list-style-type:none}ul.lst-kix_hswfzmci3dc-1{list-style-type:none}ul.lst-kix_hswfzmci3dc-6{list-style-type:none}.lst-kix_utsnp8bclzp3-0>li:before{content:"\0025cf   "}ul.lst-kix_hswfzmci3dc-7{list-style-type:none}ul.lst-kix_hswfzmci3dc-8{list-style-type:none}.lst-kix_utsnp8bclzp3-1>li:before{content:"\0025cb   "}ul.lst-kix_hswfzmci3dc-2{list-style-type:none}ul.lst-kix_91vk0di44se5-8{list-style-type:none}ul.lst-kix_hswfzmci3dc-3{list-style-type:none}ul.lst-kix_hswfzmci3dc-4{list-style-type:none}ul.lst-kix_91vk0di44se5-6{list-style-type:none}ul.lst-kix_hswfzmci3dc-5{list-style-type:none}ul.lst-kix_91vk0di44se5-7{list-style-type:none}.lst-kix_utsnp8bclzp3-4>li:before{content:"\0025cb   "}.lst-kix_8nxwnwrfsavq-5>li:before{content:"\0025a0   "}.lst-kix_utsnp8bclzp3-3>li:before{content:"\0025cf   "}.lst-kix_utsnp8bclzp3-5>li:before{content:"\0025a0   "}.lst-kix_utsnp8bclzp3-2>li:before{content:"\0025a0   "}.lst-kix_utsnp8bclzp3-6>li:before{content:"\0025cf   "}.lst-kix_8nxwnwrfsavq-6>li:before{content:"\0025cf   "}.lst-kix_8nxwnwrfsavq-7>li:before{content:"\0025cb   "}.lst-kix_8nxwnwrfsavq-8>li:before{content:"\0025a0   "}.lst-kix_utsnp8bclzp3-8>li:before{content:"\0025a0   "}.lst-kix_utsnp8bclzp3-7>li:before{content:"\0025cb   "}.lst-kix_8nxwnwrfsavq-4>li:before{content:"\0025cb   "}.lst-kix_8nxwnwrfsavq-2>li:before{content:"\0025a0   "}.lst-kix_8nxwnwrfsavq-3>li:before{content:"\0025cf   "}.lst-kix_8nxwnwrfsavq-0>li:before{content:"\0027a2   "}.lst-kix_8nxwnwrfsavq-1>li:before{content:"\0025cb   "}ul.lst-kix_99xvjj4uyl11-5{list-style-type:none}ul.lst-kix_99xvjj4uyl11-6{list-style-type:none}ul.lst-kix_99xvjj4uyl11-3{list-style-type:none}ul.lst-kix_99xvjj4uyl11-4{list-style-type:none}ul.lst-kix_99xvjj4uyl11-1{list-style-type:none}ul.lst-kix_99xvjj4uyl11-2{list-style-type:none}ul.lst-kix_99xvjj4uyl11-0{list-style-type:none}ul.lst-kix_99xvjj4uyl11-7{list-style-type:none}ul.lst-kix_99xvjj4uyl11-8{list-style-type:none}.lst-kix_42ndt3nzggvw-7>li:before{content:"\0025cb   "}.lst-kix_hswfzmci3dc-5>li:before{content:"\0025a0   "}.lst-kix_slf0pg4a0ih7-2>li:before{content:"\0025a0   "}.lst-kix_ohkm6rcowih5-5>li:before{content:"\0025a0   "}.lst-kix_hswfzmci3dc-1>li:before{content:"\0025cb   "}.lst-kix_hswfzmci3dc-3>li:before{content:"\0025cf   "}.lst-kix_slf0pg4a0ih7-4>li:before{content:"\0025cb   "}.lst-kix_ohkm6rcowih5-7>li:before{content:"\0025cb   "}ul.lst-kix_hi6xckadgtq6-0{list-style-type:none}.lst-kix_w7uhve9aueba-4>li:before{content:"\0025cb   "}.lst-kix_xnct1xfph69a-1>li:before{content:"\0025cb   "}.lst-kix_slf0pg4a0ih7-6>li:before{content:"\0025cf   "}.lst-kix_ohkm6rcowih5-1>li:before{content:"\0025cb   "}.lst-kix_w7uhve9aueba-0>li:before{content:"\0025cf   "}.lst-kix_w7uhve9aueba-8>li:before{content:"\0025a0   "}.lst-kix_yorg1uctntus-7>li:before{content:"\0025cb   "}.lst-kix_w7uhve9aueba-2>li:before{content:"\0025a0   "}.lst-kix_slf0pg4a0ih7-8>li:before{content:"\0025a0   "}.lst-kix_ohkm6rcowih5-3>li:before{content:"\0025cf   "}.lst-kix_yorg1uctntus-3>li:before{content:"\0025cf   "}.lst-kix_yorg1uctntus-5>li:before{content:"\0025a0   "}.lst-kix_hswfzmci3dc-7>li:before{content:"\0025cb   "}.lst-kix_w7uhve9aueba-6>li:before{content:"\0025cf   "}.lst-kix_stgkp8ts1530-1>li:before{content:"\0025cb   "}.lst-kix_8pfaa7u0e2zg-0>li:before{content:"\0025cf   "}.lst-kix_8pfaa7u0e2zg-2>li:before{content:"\0025a0   "}.lst-kix_i83pglsmrtnv-2>li:before{content:"\0025a0   "}.lst-kix_8pfaa7u0e2zg-4>li:before{content:"\0025cb   "}.lst-kix_8pfaa7u0e2zg-6>li:before{content:"\0025cf   "}.lst-kix_yorg1uctntus-1>li:before{content:"\0025cb   "}.lst-kix_cvtr764jy7v0-3>li:before{content:"\0025cf   "}.lst-kix_i83pglsmrtnv-0>li:before{content:"\0025cf   "}.lst-kix_i83pglsmrtnv-6>li:before{content:"\0025cf   "}.lst-kix_cvtr764jy7v0-1>li:before{content:"\0025cb   "}.lst-kix_i83pglsmrtnv-4>li:before{content:"\0025cb   "}ul.lst-kix_hi6xckadgtq6-1{list-style-type:none}ul.lst-kix_hi6xckadgtq6-2{list-style-type:none}ul.lst-kix_hi6xckadgtq6-3{list-style-type:none}ul.lst-kix_hi6xckadgtq6-4{list-style-type:none}ul.lst-kix_hi6xckadgtq6-5{list-style-type:none}ul.lst-kix_hi6xckadgtq6-6{list-style-type:none}ul.lst-kix_hi6xckadgtq6-7{list-style-type:none}ul.lst-kix_hi6xckadgtq6-8{list-style-type:none}ul.lst-kix_xnct1xfph69a-3{list-style-type:none}ul.lst-kix_xnct1xfph69a-4{list-style-type:none}ul.lst-kix_xnct1xfph69a-5{list-style-type:none}ul.lst-kix_xnct1xfph69a-6{list-style-type:none}ul.lst-kix_xnct1xfph69a-7{list-style-type:none}ul.lst-kix_3oqam8vafvbn-0{list-style-type:none}ul.lst-kix_xnct1xfph69a-8{list-style-type:none}.lst-kix_i83pglsmrtnv-8>li:before{content:"\0025a0   "}ul.lst-kix_3oqam8vafvbn-1{list-style-type:none}ul.lst-kix_3oqam8vafvbn-2{list-style-type:none}.lst-kix_8pfaa7u0e2zg-8>li:before{content:"\0025a0   "}ul.lst-kix_xnct1xfph69a-0{list-style-type:none}ul.lst-kix_xnct1xfph69a-1{list-style-type:none}.lst-kix_99xvjj4uyl11-0>li:before{content:"\0025cf   "}ul.lst-kix_xnct1xfph69a-2{list-style-type:none}.lst-kix_cvtr764jy7v0-5>li:before{content:"\0025a0   "}.lst-kix_99xvjj4uyl11-2>li:before{content:"\0025a0   "}.lst-kix_cvtr764jy7v0-7>li:before{content:"\0025cb   "}.lst-kix_5snmm03p8xq6-5>li:before{content:"\0025a0   "}.lst-kix_5snmm03p8xq6-6>li:before{content:"\0025cf   "}.lst-kix_bigsmldlbgzk-1>li:before{content:"\0025cb   "}.lst-kix_5snmm03p8xq6-1>li:before{content:"\0025cb   "}.lst-kix_bigsmldlbgzk-2>li:before{content:"\0025a0   "}.lst-kix_5snmm03p8xq6-2>li:before{content:"\0025a0   "}.lst-kix_bigsmldlbgzk-6>li:before{content:"\0025cf   "}.lst-kix_bigsmldlbgzk-5>li:before{content:"\0025a0   "}.lst-kix_5gev2o922b7x-0>li:before{content:"\0025cf   "}.lst-kix_5gev2o922b7x-1>li:before{content:"\0025cb   "}.lst-kix_stgkp8ts1530-6>li:before{content:"\0025cf   "}.lst-kix_stgkp8ts1530-5>li:before{content:"\0025a0   "}.lst-kix_5gev2o922b7x-8>li:before{content:"\0025a0   "}.lst-kix_mdf3w72jkmgx-0>li:before{content:"\0027a2   "}.lst-kix_gaab8il5wtid-5>li:before{content:"\0025a0   "}.lst-kix_gaab8il5wtid-6>li:before{content:"\0025cf   "}ul.lst-kix_n1ax58vdrowu-2{list-style-type:none}ul.lst-kix_n1ax58vdrowu-1{list-style-type:none}ul.lst-kix_n1ax58vdrowu-0{list-style-type:none}.lst-kix_xnct1xfph69a-3>li:before{content:"\0025cf   "}.lst-kix_42ndt3nzggvw-2>li:before{content:"\0025a0   "}.lst-kix_xnct1xfph69a-4>li:before{content:"\0025cb   "}ul.lst-kix_n1ax58vdrowu-8{list-style-type:none}.lst-kix_42ndt3nzggvw-3>li:before{content:"\0025cf   "}.lst-kix_xnct1xfph69a-7>li:before{content:"\0025cb   "}ul.lst-kix_n1ax58vdrowu-7{list-style-type:none}ul.lst-kix_n1ax58vdrowu-6{list-style-type:none}ul.lst-kix_n1ax58vdrowu-5{list-style-type:none}.lst-kix_gaab8il5wtid-2>li:before{content:"\0025a0   "}ul.lst-kix_n1ax58vdrowu-4{list-style-type:none}ul.lst-kix_n1ax58vdrowu-3{list-style-type:none}.lst-kix_gaab8il5wtid-1>li:before{content:"\0025cb   "}.lst-kix_5gev2o922b7x-5>li:before{content:"\0025a0   "}.lst-kix_xnct1xfph69a-8>li:before{content:"\0025a0   "}.lst-kix_x7gq6rngbuia-8>li:before{content:"\0025a0   "}.lst-kix_5gev2o922b7x-4>li:before{content:"\0025cb   "}.lst-kix_64f0218k1822-8>li:before{content:"\0025a0   "}.lst-kix_pxlscdvf5lia-8>li:before{content:"\0025a0   "}.lst-kix_s8kajunwpmir-5>li:before{content:"\0025cf   "}.lst-kix_64f0218k1822-0>li:before{content:"\0025cf   "}.lst-kix_pxlscdvf5lia-0>li:before{content:"\0025cf   "}.lst-kix_ohkm6rcowih5-8>li:before{content:"\0025a0   "}.lst-kix_x7gq6rngbuia-5>li:before{content:"\0025a0   "}.lst-kix_42ndt3nzggvw-6>li:before{content:"\0025cf   "}.lst-kix_slf0pg4a0ih7-1>li:before{content:"\0025cb   "}.lst-kix_bb18g463362n-1>li:before{content:"\0025cb   "}.lst-kix_hswfzmci3dc-2>li:before{content:"\0025a0   "}.lst-kix_64f0218k1822-4>li:before{content:"\0025cb   "}.lst-kix_x7gq6rngbuia-1>li:before{content:"\0025cb   "}.lst-kix_w7uhve9aueba-5>li:before{content:"\0025a0   "}.lst-kix_ohkm6rcowih5-0>li:before{content:"\0025cf   "}.lst-kix_slf0pg4a0ih7-5>li:before{content:"\0025a0   "}.lst-kix_xnct1xfph69a-0>li:before{content:"\0025cf   "}.lst-kix_w7uhve9aueba-1>li:before{content:"\0025cb   "}.lst-kix_ohkm6rcowih5-4>li:before{content:"\0025cb   "}.lst-kix_80fk82fj1hcs-6>li:before{content:"\0025cf   "}.lst-kix_yorg1uctntus-8>li:before{content:"\0025a0   "}.lst-kix_hswfzmci3dc-6>li:before{content:"\0025cf   "}ul.lst-kix_91vk0di44se5-4{list-style-type:none}.lst-kix_mdf3w72jkmgx-3>li:before{content:"\0025cf   "}ul.lst-kix_91vk0di44se5-5{list-style-type:none}.lst-kix_80fk82fj1hcs-2>li:before{content:"\0025a0   "}ul.lst-kix_91vk0di44se5-2{list-style-type:none}ul.lst-kix_91vk0di44se5-3{list-style-type:none}ul.lst-kix_91vk0di44se5-0{list-style-type:none}ul.lst-kix_91vk0di44se5-1{list-style-type:none}.lst-kix_mdf3w72jkmgx-7>li:before{content:"\0025cb   "}.lst-kix_stgkp8ts1530-2>li:before{content:"\0025a0   "}.lst-kix_otug9mhs4rmx-8>li:before{content:"\0025a0   "}.lst-kix_s8kajunwpmir-1>li:before{content:"\0025cf   "}.lst-kix_yorg1uctntus-4>li:before{content:"\0025cb   "}.lst-kix_8pfaa7u0e2zg-1>li:before{content:"\0025cb   "}ul.lst-kix_8pvyoajrhbtr-2{list-style-type:none}ul.lst-kix_5qkugsusxliw-0{list-style-type:none}ul.lst-kix_8pvyoajrhbtr-3{list-style-type:none}ul.lst-kix_5qkugsusxliw-1{list-style-type:none}ul.lst-kix_8pvyoajrhbtr-0{list-style-type:none}ul.lst-kix_5qkugsusxliw-2{list-style-type:none}ul.lst-kix_8pvyoajrhbtr-1{list-style-type:none}ul.lst-kix_5qkugsusxliw-3{list-style-type:none}ul.lst-kix_5qkugsusxliw-4{list-style-type:none}ul.lst-kix_5qkugsusxliw-5{list-style-type:none}ul.lst-kix_5qkugsusxliw-6{list-style-type:none}ul.lst-kix_5qkugsusxliw-7{list-style-type:none}.lst-kix_8pfaa7u0e2zg-5>li:before{content:"\0025a0   "}ul.lst-kix_5qkugsusxliw-8{list-style-type:none}.lst-kix_i83pglsmrtnv-1>li:before{content:"\0025cb   "}.lst-kix_yorg1uctntus-0>li:before{content:"\0025cf   "}.lst-kix_bf3umtcrzzqw-3>li:before{content:"\0025cf   "}.lst-kix_bf3umtcrzzqw-7>li:before{content:"\0025cb   "}.lst-kix_g1ro4rg087e5-5>li:before{content:"\0025a0   "}.lst-kix_3n0bx46jp1u4-0>li:before{content:"\0025cf   "}.lst-kix_cvtr764jy7v0-0>li:before{content:"\0025cf   "}ul.lst-kix_8pvyoajrhbtr-8{list-style-type:none}ul.lst-kix_8pvyoajrhbtr-6{list-style-type:none}ul.lst-kix_8pvyoajrhbtr-7{list-style-type:none}.lst-kix_i83pglsmrtnv-5>li:before{content:"\0025a0   "}ul.lst-kix_8pvyoajrhbtr-4{list-style-type:none}ul.lst-kix_8pvyoajrhbtr-5{list-style-type:none}ul.lst-kix_3t7g0caoicba-2{list-style-type:none}ul.lst-kix_3t7g0caoicba-3{list-style-type:none}ul.lst-kix_5snmm03p8xq6-7{list-style-type:none}ul.lst-kix_3t7g0caoicba-4{list-style-type:none}ul.lst-kix_5snmm03p8xq6-8{list-style-type:none}ul.lst-kix_3t7g0caoicba-5{list-style-type:none}.lst-kix_bb18g463362n-5>li:before{content:"\0025a0   "}ul.lst-kix_3t7g0caoicba-0{list-style-type:none}ul.lst-kix_3t7g0caoicba-1{list-style-type:none}ul.lst-kix_5snmm03p8xq6-1{list-style-type:none}ul.lst-kix_pxlscdvf5lia-8{list-style-type:none}ul.lst-kix_5snmm03p8xq6-2{list-style-type:none}ul.lst-kix_pxlscdvf5lia-6{list-style-type:none}ul.lst-kix_5snmm03p8xq6-0{list-style-type:none}ul.lst-kix_pxlscdvf5lia-7{list-style-type:none}ul.lst-kix_5snmm03p8xq6-5{list-style-type:none}ul.lst-kix_pxlscdvf5lia-4{list-style-type:none}ul.lst-kix_5snmm03p8xq6-6{list-style-type:none}ul.lst-kix_pxlscdvf5lia-5{list-style-type:none}ul.lst-kix_5snmm03p8xq6-3{list-style-type:none}ul.lst-kix_pxlscdvf5lia-2{list-style-type:none}ul.lst-kix_5snmm03p8xq6-4{list-style-type:none}ul.lst-kix_pxlscdvf5lia-3{list-style-type:none}ul.lst-kix_pxlscdvf5lia-0{list-style-type:none}.lst-kix_3n0bx46jp1u4-4>li:before{content:"\0025cb   "}ul.lst-kix_pxlscdvf5lia-1{list-style-type:none}.lst-kix_cvtr764jy7v0-4>li:before{content:"\0025cb   "}.lst-kix_g1ro4rg087e5-1>li:before{content:"\0025cb   "}.lst-kix_3n0bx46jp1u4-8>li:before{content:"\0025a0   "}.lst-kix_pxlscdvf5lia-4>li:before{content:"\0025cb   "}.lst-kix_99xvjj4uyl11-3>li:before{content:"\0025cf   "}.lst-kix_cvtr764jy7v0-8>li:before{content:"\0025a0   "}ul.lst-kix_3t7g0caoicba-6{list-style-type:none}ul.lst-kix_3t7g0caoicba-7{list-style-type:none}ul.lst-kix_3t7g0caoicba-8{list-style-type:none}.lst-kix_5hd2wrb71s38-1>li:before{content:"\0025cb   "}ul.lst-kix_8b09c5wbiylz-3{list-style-type:none}ul.lst-kix_8b09c5wbiylz-4{list-style-type:none}.lst-kix_m2rwr78ppx87-8>li:before{content:"\0025a0   "}ul.lst-kix_8b09c5wbiylz-5{list-style-type:none}ul.lst-kix_8b09c5wbiylz-6{list-style-type:none}.lst-kix_hi6xckadgtq6-5>li:before{content:"\0025a0   "}ul.lst-kix_8b09c5wbiylz-7{list-style-type:none}ul.lst-kix_8b09c5wbiylz-8{list-style-type:none}.lst-kix_m2rwr78ppx87-6>li:before{content:"\0025cf   "}.lst-kix_91vk0di44se5-0>li:before{content:"\0025cf   "}.lst-kix_hi6xckadgtq6-8>li:before{content:"\0025a0   "}.lst-kix_m2dc47g2g6zf-3>li:before{content:"\0025cf   "}.lst-kix_91vk0di44se5-5>li:before{content:"\0025cf   "}.lst-kix_hi6xckadgtq6-7>li:before{content:"\0025cb   "}.lst-kix_m2dc47g2g6zf-4>li:before{content:"\0025cb   "}.lst-kix_m2dc47g2g6zf-6>li:before{content:"\0025cf   "}ul.lst-kix_15risnu9xchg-5{list-style-type:none}ul.lst-kix_15risnu9xchg-6{list-style-type:none}.lst-kix_m2rwr78ppx87-0>li:before{content:"\0025cf   "}ul.lst-kix_15risnu9xchg-7{list-style-type:none}ul.lst-kix_15risnu9xchg-8{list-style-type:none}.lst-kix_5hd2wrb71s38-7>li:before{content:"\0025cb   "}.lst-kix_5hd2wrb71s38-6>li:before{content:"\0025cf   "}.lst-kix_hi6xckadgtq6-0>li:before{content:"\0025cf   "}ul.lst-kix_15risnu9xchg-0{list-style-type:none}.lst-kix_91vk0di44se5-3>li:before{content:"\0025cf   "}ul.lst-kix_15risnu9xchg-1{list-style-type:none}ul.lst-kix_15risnu9xchg-2{list-style-type:none}ul.lst-kix_8b09c5wbiylz-0{list-style-type:none}.lst-kix_5hd2wrb71s38-4>li:before{content:"\0025cb   "}ul.lst-kix_15risnu9xchg-3{list-style-type:none}ul.lst-kix_8b09c5wbiylz-1{list-style-type:none}ul.lst-kix_15risnu9xchg-4{list-style-type:none}ul.lst-kix_8b09c5wbiylz-2{list-style-type:none}.lst-kix_91vk0di44se5-2>li:before{content:"\0025cf   "}.lst-kix_hi6xckadgtq6-2>li:before{content:"\0025a0   "}.lst-kix_sfz3dfa4n7w2-2>li:before{content:"\0025a0   "}.lst-kix_sfz3dfa4n7w2-5>li:before{content:"\0025a0   "}.lst-kix_99524jad4mmq-6>li:before{content:"\0025cf   "}.lst-kix_99524jad4mmq-8>li:before{content:"\0025a0   "}.lst-kix_91vk0di44se5-8>li:before{content:"\0025cf   "}.lst-kix_m2rwr78ppx87-3>li:before{content:"\0025cf   "}.lst-kix_sfz3dfa4n7w2-0>li:before{content:"\0025cf   "}.lst-kix_m2dc47g2g6zf-1>li:before{content:"\0025cb   "}.lst-kix_m2rwr78ppx87-5>li:before{content:"\0025a0   "}ul.lst-kix_sznnvmk8su1j-5{list-style-type:none}ul.lst-kix_sznnvmk8su1j-6{list-style-type:none}.lst-kix_u4gcy9afd1io-2>li:before{content:"\0025a0   "}.lst-kix_u4gcy9afd1io-3>li:before{content:"\0025cf   "}ul.lst-kix_sznnvmk8su1j-3{list-style-type:none}ul.lst-kix_sznnvmk8su1j-4{list-style-type:none}ul.lst-kix_5jkev5cbxpxc-0{list-style-type:none}ul.lst-kix_sznnvmk8su1j-7{list-style-type:none}.lst-kix_u4gcy9afd1io-5>li:before{content:"\0025a0   "}ul.lst-kix_sznnvmk8su1j-8{list-style-type:none}.lst-kix_otug9mhs4rmx-2>li:before{content:"\0025a0   "}ul.lst-kix_sznnvmk8su1j-1{list-style-type:none}.lst-kix_99524jad4mmq-0>li:before{content:"\0027a2   "}ul.lst-kix_sznnvmk8su1j-2{list-style-type:none}.lst-kix_otug9mhs4rmx-4>li:before{content:"\0025cb   "}ul.lst-kix_sznnvmk8su1j-0{list-style-type:none}.lst-kix_99524jad4mmq-5>li:before{content:"\0025a0   "}ul.lst-kix_5jkev5cbxpxc-3{list-style-type:none}.lst-kix_sfz3dfa4n7w2-7>li:before{content:"\0025cb   "}ul.lst-kix_5jkev5cbxpxc-4{list-style-type:none}ul.lst-kix_5jkev5cbxpxc-1{list-style-type:none}.lst-kix_sfz3dfa4n7w2-8>li:before{content:"\0025a0   "}ul.lst-kix_5jkev5cbxpxc-2{list-style-type:none}ul.lst-kix_5jkev5cbxpxc-7{list-style-type:none}.lst-kix_u4gcy9afd1io-8>li:before{content:"\0025a0   "}ul.lst-kix_5jkev5cbxpxc-8{list-style-type:none}ul.lst-kix_5jkev5cbxpxc-5{list-style-type:none}.lst-kix_99524jad4mmq-3>li:before{content:"\0025cf   "}ul.lst-kix_5jkev5cbxpxc-6{list-style-type:none}ul.lst-kix_4chf7gspsjev-5{list-style-type:none}ul.lst-kix_4chf7gspsjev-6{list-style-type:none}ul.lst-kix_4chf7gspsjev-3{list-style-type:none}ul.lst-kix_4chf7gspsjev-4{list-style-type:none}ul.lst-kix_4chf7gspsjev-1{list-style-type:none}ul.lst-kix_4chf7gspsjev-2{list-style-type:none}ul.lst-kix_4chf7gspsjev-0{list-style-type:none}.lst-kix_u4gcy9afd1io-0>li:before{content:"\0025cf   "}ul.lst-kix_4chf7gspsjev-7{list-style-type:none}ul.lst-kix_4chf7gspsjev-8{list-style-type:none}ul.lst-kix_tdg51sdobgli-3{list-style-type:none}ul.lst-kix_tdg51sdobgli-2{list-style-type:none}ul.lst-kix_tdg51sdobgli-1{list-style-type:none}ul.lst-kix_tdg51sdobgli-0{list-style-type:none}.lst-kix_hg8waqqzu11r-5>li:before{content:"\0025a0   "}.lst-kix_s8kajunwpmir-4>li:before{content:"\0025cf   "}.lst-kix_eqih6is7mr7q-6>li:before{content:"\0025cf   "}.lst-kix_64f0218k1822-7>li:before{content:"\0025cb   "}.lst-kix_ssxyep2erusf-0>li:before{content:"\0025cf   "}.lst-kix_x7gq6rngbuia-6>li:before{content:"\0025cf   "}.lst-kix_kz0xo3ln4wpk-5>li:before{content:"\0025a0   "}.lst-kix_bb18g463362n-0>li:before{content:"\0025cf   "}.lst-kix_kz0xo3ln4wpk-7>li:before{content:"\0025cb   "}.lst-kix_ssxyep2erusf-6>li:before{content:"\0025cf   "}.lst-kix_hg8waqqzu11r-7>li:before{content:"\0025cb   "}.lst-kix_64f0218k1822-5>li:before{content:"\0025a0   "}ul.lst-kix_tdg51sdobgli-8{list-style-type:none}.lst-kix_eqih6is7mr7q-0>li:before{content:"\0025cf   "}ul.lst-kix_tdg51sdobgli-7{list-style-type:none}ul.lst-kix_tdg51sdobgli-6{list-style-type:none}ul.lst-kix_tdg51sdobgli-5{list-style-type:none}ul.lst-kix_tdg51sdobgli-4{list-style-type:none}.lst-kix_tdg51sdobgli-1>li:before{content:"\0025cb   "}.lst-kix_x7gq6rngbuia-0>li:before{content:"\0025cf   "}.lst-kix_bf3umtcrzzqw-0>li:before{content:"\0025cf   "}.lst-kix_bf3umtcrzzqw-2>li:before{content:"\0025a0   "}.lst-kix_ssxyep2erusf-8>li:before{content:"\0025a0   "}.lst-kix_80fk82fj1hcs-7>li:before{content:"\0025cb   "}.lst-kix_80fk82fj1hcs-1>li:before{content:"\0025cb   "}.lst-kix_otug9mhs4rmx-7>li:before{content:"\0025cb   "}ul.lst-kix_9l57ldnvor4v-7{list-style-type:none}ul.lst-kix_u4gcy9afd1io-8{list-style-type:none}.lst-kix_mdf3w72jkmgx-4>li:before{content:"\0025cb   "}ul.lst-kix_9l57ldnvor4v-8{list-style-type:none}.lst-kix_otug9mhs4rmx-5>li:before{content:"\0025a0   "}ul.lst-kix_u4gcy9afd1io-6{list-style-type:none}.lst-kix_dllt5p3q6235-4>li:before{content:"\0025cb   "}ul.lst-kix_u4gcy9afd1io-7{list-style-type:none}ul.lst-kix_u4gcy9afd1io-4{list-style-type:none}.lst-kix_mdf3w72jkmgx-6>li:before{content:"\0025cf   "}ul.lst-kix_u4gcy9afd1io-5{list-style-type:none}ul.lst-kix_u4gcy9afd1io-2{list-style-type:none}ul.lst-kix_u4gcy9afd1io-3{list-style-type:none}.lst-kix_eqih6is7mr7q-8>li:before{content:"\0025a0   "}.lst-kix_s8kajunwpmir-2>li:before{content:"\0025cf   "}ul.lst-kix_cvtr764jy7v0-0{list-style-type:none}ul.lst-kix_u4gcy9afd1io-0{list-style-type:none}ul.lst-kix_u4gcy9afd1io-1{list-style-type:none}ul.lst-kix_cvtr764jy7v0-2{list-style-type:none}.lst-kix_dllt5p3q6235-2>li:before{content:"\0025a0   "}ul.lst-kix_cvtr764jy7v0-1{list-style-type:none}ul.lst-kix_cvtr764jy7v0-4{list-style-type:none}ul.lst-kix_bigsmldlbgzk-5{list-style-type:none}ul.lst-kix_cvtr764jy7v0-3{list-style-type:none}ul.lst-kix_bigsmldlbgzk-6{list-style-type:none}ul.lst-kix_cvtr764jy7v0-6{list-style-type:none}ul.lst-kix_bigsmldlbgzk-7{list-style-type:none}ul.lst-kix_cvtr764jy7v0-5{list-style-type:none}ul.lst-kix_bigsmldlbgzk-8{list-style-type:none}ul.lst-kix_5gev2o922b7x-7{list-style-type:none}ul.lst-kix_cvtr764jy7v0-8{list-style-type:none}ul.lst-kix_bigsmldlbgzk-1{list-style-type:none}.lst-kix_g1ro4rg087e5-8>li:before{content:"\0025a0   "}ul.lst-kix_5gev2o922b7x-8{list-style-type:none}ul.lst-kix_cvtr764jy7v0-7{list-style-type:none}ul.lst-kix_bigsmldlbgzk-2{list-style-type:none}.lst-kix_ykbty9k091s5-8>li:before{content:"\0025a0   "}ul.lst-kix_bigsmldlbgzk-3{list-style-type:none}ul.lst-kix_bigsmldlbgzk-4{list-style-type:none}ul.lst-kix_9l57ldnvor4v-3{list-style-type:none}ul.lst-kix_9l57ldnvor4v-4{list-style-type:none}.lst-kix_bf3umtcrzzqw-8>li:before{content:"\0025a0   "}ul.lst-kix_9l57ldnvor4v-5{list-style-type:none}ul.lst-kix_9l57ldnvor4v-6{list-style-type:none}ul.lst-kix_9l57ldnvor4v-0{list-style-type:none}.lst-kix_ykbty9k091s5-6>li:before{content:"\0025cf   "}.lst-kix_3n0bx46jp1u4-3>li:before{content:"\0025cf   "}ul.lst-kix_9l57ldnvor4v-1{list-style-type:none}ul.lst-kix_9l57ldnvor4v-2{list-style-type:none}ul.lst-kix_wa5eplpvj99e-1{list-style-type:none}ul.lst-kix_wa5eplpvj99e-0{list-style-type:none}.lst-kix_ykbty9k091s5-0>li:before{content:"\0025cf   "}.lst-kix_3n0bx46jp1u4-1>li:before{content:"\0025cb   "}ul.lst-kix_wa5eplpvj99e-5{list-style-type:none}ul.lst-kix_5gev2o922b7x-3{list-style-type:none}ul.lst-kix_wa5eplpvj99e-4{list-style-type:none}.lst-kix_g1ro4rg087e5-6>li:before{content:"\0025cf   "}ul.lst-kix_5gev2o922b7x-4{list-style-type:none}ul.lst-kix_wa5eplpvj99e-3{list-style-type:none}ul.lst-kix_5gev2o922b7x-5{list-style-type:none}.lst-kix_v8i0z8oris9z-2>li:before{content:"\0025a0   "}ul.lst-kix_wa5eplpvj99e-2{list-style-type:none}ul.lst-kix_5gev2o922b7x-6{list-style-type:none}ul.lst-kix_bigsmldlbgzk-0{list-style-type:none}.lst-kix_qzcbcbds5p8j-5>li:before{content:"\0025cf   "}.lst-kix_wa5eplpvj99e-8>li:before{content:"\0025a0   "}ul.lst-kix_wa5eplpvj99e-8{list-style-type:none}ul.lst-kix_5gev2o922b7x-0{list-style-type:none}ul.lst-kix_wa5eplpvj99e-7{list-style-type:none}ul.lst-kix_5gev2o922b7x-1{list-style-type:none}ul.lst-kix_wa5eplpvj99e-6{list-style-type:none}ul.lst-kix_5gev2o922b7x-2{list-style-type:none}ul.lst-kix_hg8waqqzu11r-1{list-style-type:none}ul.lst-kix_ykbty9k091s5-0{list-style-type:none}ul.lst-kix_slf0pg4a0ih7-8{list-style-type:none}.lst-kix_i6q6s2czm8np-0>li:before{content:"\0027a2   "}ul.lst-kix_hg8waqqzu11r-2{list-style-type:none}ul.lst-kix_ykbty9k091s5-1{list-style-type:none}.lst-kix_bb18g463362n-8>li:before{content:"\0025a0   "}ul.lst-kix_hg8waqqzu11r-3{list-style-type:none}ul.lst-kix_slf0pg4a0ih7-6{list-style-type:none}ul.lst-kix_hg8waqqzu11r-4{list-style-type:none}ul.lst-kix_slf0pg4a0ih7-7{list-style-type:none}.lst-kix_tdg51sdobgli-3>li:before{content:"\0025cf   "}ul.lst-kix_ykbty9k091s5-4{list-style-type:none}ul.lst-kix_slf0pg4a0ih7-4{list-style-type:none}ul.lst-kix_ykbty9k091s5-5{list-style-type:none}.lst-kix_bb18g463362n-6>li:before{content:"\0025cf   "}ul.lst-kix_slf0pg4a0ih7-5{list-style-type:none}ul.lst-kix_ykbty9k091s5-2{list-style-type:none}ul.lst-kix_slf0pg4a0ih7-2{list-style-type:none}ul.lst-kix_hg8waqqzu11r-0{list-style-type:none}ul.lst-kix_ykbty9k091s5-3{list-style-type:none}.lst-kix_5jkev5cbxpxc-2>li:before{content:"\0025a0   "}ul.lst-kix_slf0pg4a0ih7-3{list-style-type:none}.lst-kix_qzcbcbds5p8j-3>li:before{content:"\0025cf   "}ul.lst-kix_ykbty9k091s5-8{list-style-type:none}.lst-kix_z8dy5ehg0j6m-4>li:before{content:"\0025cb   "}ul.lst-kix_ykbty9k091s5-6{list-style-type:none}.lst-kix_v8i0z8oris9z-4>li:before{content:"\0025cb   "}ul.lst-kix_ykbty9k091s5-7{list-style-type:none}ul.lst-kix_hg8waqqzu11r-5{list-style-type:none}.lst-kix_z8dy5ehg0j6m-2>li:before{content:"\0025a0   "}.lst-kix_i6q6s2czm8np-2>li:before{content:"\0025a0   "}ul.lst-kix_hg8waqqzu11r-6{list-style-type:none}ul.lst-kix_hg8waqqzu11r-7{list-style-type:none}ul.lst-kix_hg8waqqzu11r-8{list-style-type:none}.lst-kix_pxlscdvf5lia-1>li:before{content:"\0025cb   "}.lst-kix_i6q6s2czm8np-8>li:before{content:"\0025a0   "}.lst-kix_pxlscdvf5lia-3>li:before{content:"\0025cf   "}.lst-kix_g1ro4rg087e5-0>li:before{content:"\0025cf   "}.lst-kix_wa5eplpvj99e-2>li:before{content:"\0025a0   "}ul.lst-kix_slf0pg4a0ih7-0{list-style-type:none}ul.lst-kix_slf0pg4a0ih7-1{list-style-type:none}.lst-kix_wa5eplpvj99e-0>li:before{content:"\0025cf   "}.lst-kix_5jkev5cbxpxc-8>li:before{content:"\0025a0   "}.lst-kix_1c2feo7jow6-6>li:before{content:"\0025cf   "}.lst-kix_3oqam8vafvbn-2>li:before{content:"\0025a0   "}.lst-kix_3oqam8vafvbn-5>li:before{content:"\0025a0   "}.lst-kix_nvbe1nnkagj0-4>li:before{content:"\0025cf   "}.lst-kix_5snmm03p8xq6-7>li:before{content:"\0025cb   "}.lst-kix_nfy673qapcua-6>li:before{content:"\0025cf   "}.lst-kix_ee0rw9sgmv1o-0>li:before{content:"\0025cf   "}.lst-kix_nvbe1nnkagj0-7>li:before{content:"\0025cf   "}ul.lst-kix_nfy673qapcua-0{list-style-type:none}.lst-kix_n1ax58vdrowu-4>li:before{content:"\0025cb   "}.lst-kix_1c2feo7jow6-1>li:before{content:"\0025cb   "}ul.lst-kix_woopwfqcy80n-4{list-style-type:none}.lst-kix_bym5woclzz8f-5>li:before{content:"\0025a0   "}ul.lst-kix_woopwfqcy80n-5{list-style-type:none}.lst-kix_jxs8idtrfx9u-7>li:before{content:"\0025cb   "}ul.lst-kix_woopwfqcy80n-2{list-style-type:none}.lst-kix_ee0rw9sgmv1o-8>li:before{content:"\0025a0   "}ul.lst-kix_woopwfqcy80n-3{list-style-type:none}ul.lst-kix_woopwfqcy80n-8{list-style-type:none}ul.lst-kix_woopwfqcy80n-6{list-style-type:none}ul.lst-kix_woopwfqcy80n-7{list-style-type:none}ul.lst-kix_nfy673qapcua-2{list-style-type:none}.lst-kix_5snmm03p8xq6-4>li:before{content:"\0025cb   "}ul.lst-kix_nfy673qapcua-1{list-style-type:none}.lst-kix_bigsmldlbgzk-4>li:before{content:"\0025cb   "}ul.lst-kix_nfy673qapcua-4{list-style-type:none}ul.lst-kix_nfy673qapcua-3{list-style-type:none}ul.lst-kix_woopwfqcy80n-0{list-style-type:none}ul.lst-kix_nfy673qapcua-6{list-style-type:none}ul.lst-kix_woopwfqcy80n-1{list-style-type:none}.lst-kix_jxs8idtrfx9u-2>li:before{content:"\0025a0   "}.lst-kix_nfy673qapcua-3>li:before{content:"\0025cf   "}ul.lst-kix_nfy673qapcua-5{list-style-type:none}.lst-kix_5qkugsusxliw-3>li:before{content:"\0025cf   "}ul.lst-kix_nfy673qapcua-8{list-style-type:none}ul.lst-kix_nfy673qapcua-7{list-style-type:none}.lst-kix_bigsmldlbgzk-7>li:before{content:"\0025cb   "}.lst-kix_ysk5buayro0y-0>li:before{content:"\0027a2   "}.lst-kix_ysk5buayro0y-3>li:before{content:"\0025cf   "}.lst-kix_q24skcnjivy0-4>li:before{content:"\0025cb   "}.lst-kix_ee0rw9sgmv1o-3>li:before{content:"\0025cf   "}.lst-kix_q24skcnjivy0-1>li:before{content:"\0025cb   "}.lst-kix_15risnu9xchg-8>li:before{content:"\0025a0   "}.lst-kix_8pvyoajrhbtr-2>li:before{content:"\0025a0   "}.lst-kix_5qkugsusxliw-6>li:before{content:"\0025cf   "}.lst-kix_ysk5buayro0y-8>li:before{content:"\0025a0   "}.lst-kix_stgkp8ts1530-4>li:before{content:"\0025cb   "}.lst-kix_5gev2o922b7x-6>li:before{content:"\0025cf   "}.lst-kix_15risnu9xchg-3>li:before{content:"\0025cf   "}.lst-kix_8pvyoajrhbtr-5>li:before{content:"\0025a0   "}.lst-kix_tq2qp7s2zqek-5>li:before{content:"\0025a0   "}.lst-kix_stgkp8ts1530-7>li:before{content:"\0025cb   "}.lst-kix_gaab8il5wtid-4>li:before{content:"\0025cb   "}.lst-kix_15risnu9xchg-0>li:before{content:"\0025cf   "}.lst-kix_ascolikyamcy-4>li:before{content:"\0025cb   "}.lst-kix_42ndt3nzggvw-5>li:before{content:"\0025a0   "}.lst-kix_tq2qp7s2zqek-2>li:before{content:"\0025a0   "}.lst-kix_xnct1xfph69a-6>li:before{content:"\0025cf   "}.lst-kix_n1ax58vdrowu-1>li:before{content:"\0025cb   "}.lst-kix_5gev2o922b7x-3>li:before{content:"\0025cf   "}.lst-kix_42ndt3nzggvw-0>li:before{content:"\0027a2   "}.lst-kix_hg8waqqzu11r-2>li:before{content:"\0025a0   "}.lst-kix_4chf7gspsjev-7>li:before{content:"\0025cb   "}.lst-kix_s8kajunwpmir-7>li:before{content:"\0025cf   "}.lst-kix_eqih6is7mr7q-3>li:before{content:"\0025cf   "}.lst-kix_42ndt3nzggvw-8>li:before{content:"\0025a0   "}ul.lst-kix_80fk82fj1hcs-3{list-style-type:none}ul.lst-kix_80fk82fj1hcs-4{list-style-type:none}ul.lst-kix_80fk82fj1hcs-5{list-style-type:none}ul.lst-kix_80fk82fj1hcs-6{list-style-type:none}.lst-kix_64f0218k1822-2>li:before{content:"\0025a0   "}ul.lst-kix_80fk82fj1hcs-0{list-style-type:none}ul.lst-kix_80fk82fj1hcs-1{list-style-type:none}.lst-kix_hswfzmci3dc-4>li:before{content:"\0025cb   "}ul.lst-kix_80fk82fj1hcs-2{list-style-type:none}ul.lst-kix_jxs8idtrfx9u-2{list-style-type:none}.lst-kix_bb18g463362n-3>li:before{content:"\0025cf   "}ul.lst-kix_jxs8idtrfx9u-1{list-style-type:none}ul.lst-kix_jxs8idtrfx9u-0{list-style-type:none}.lst-kix_x7gq6rngbuia-3>li:before{content:"\0025cf   "}.lst-kix_ssxyep2erusf-3>li:before{content:"\0025cf   "}ul.lst-kix_80fk82fj1hcs-7{list-style-type:none}ul.lst-kix_jxs8idtrfx9u-6{list-style-type:none}ul.lst-kix_80fk82fj1hcs-8{list-style-type:none}ul.lst-kix_jxs8idtrfx9u-5{list-style-type:none}ul.lst-kix_jxs8idtrfx9u-4{list-style-type:none}ul.lst-kix_jxs8idtrfx9u-3{list-style-type:none}.lst-kix_slf0pg4a0ih7-7>li:before{content:"\0025cb   "}ul.lst-kix_jxs8idtrfx9u-8{list-style-type:none}.lst-kix_ohkm6rcowih5-2>li:before{content:"\0025a0   "}ul.lst-kix_jxs8idtrfx9u-7{list-style-type:none}.lst-kix_ascolikyamcy-7>li:before{content:"\0025cb   "}.lst-kix_gaab8il5wtid-7>li:before{content:"\0025cb   "}.lst-kix_w7uhve9aueba-3>li:before{content:"\0025cf   "}.lst-kix_9l57ldnvor4v-2>li:before{content:"\0025a0   "}ul.lst-kix_ee0rw9sgmv1o-1{list-style-type:none}ul.lst-kix_ee0rw9sgmv1o-0{list-style-type:none}.lst-kix_dllt5p3q6235-7>li:before{content:"\0025cb   "}.lst-kix_mdf3w72jkmgx-1>li:before{content:"\0025cb   "}ul.lst-kix_5hd2wrb71s38-0{list-style-type:none}.lst-kix_ncyelpl3aut1-8>li:before{content:"\0025a0   "}.lst-kix_yorg1uctntus-6>li:before{content:"\0025cf   "}ul.lst-kix_5hd2wrb71s38-7{list-style-type:none}ul.lst-kix_5hd2wrb71s38-8{list-style-type:none}ul.lst-kix_5hd2wrb71s38-5{list-style-type:none}.lst-kix_80fk82fj1hcs-4>li:before{content:"\0025cb   "}ul.lst-kix_5hd2wrb71s38-6{list-style-type:none}ul.lst-kix_5hd2wrb71s38-3{list-style-type:none}ul.lst-kix_5hd2wrb71s38-4{list-style-type:none}ul.lst-kix_5hd2wrb71s38-1{list-style-type:none}ul.lst-kix_5hd2wrb71s38-2{list-style-type:none}.lst-kix_8pfaa7u0e2zg-3>li:before{content:"\0025cf   "}.lst-kix_wa5eplpvj99e-5>li:before{content:"\0025a0   "}.lst-kix_ncyelpl3aut1-0>li:before{content:"\0027a2   "}.lst-kix_sznnvmk8su1j-0>li:before{content:"\0025cf   "}.lst-kix_cvtr764jy7v0-2>li:before{content:"\0025a0   "}.lst-kix_bf3umtcrzzqw-5>li:before{content:"\0025a0   "}.lst-kix_g1ro4rg087e5-3>li:before{content:"\0025cf   "}.lst-kix_i83pglsmrtnv-7>li:before{content:"\0025cb   "}.lst-kix_nz0skx7ahmk9-8>li:before{content:"\0025a0   "}.lst-kix_qzcbcbds5p8j-8>li:before{content:"\0025cf   "}.lst-kix_ykbty9k091s5-3>li:before{content:"\0025cf   "}ul.lst-kix_ee0rw9sgmv1o-3{list-style-type:none}ul.lst-kix_42ndt3nzggvw-1{list-style-type:none}ul.lst-kix_ee0rw9sgmv1o-2{list-style-type:none}ul.lst-kix_42ndt3nzggvw-2{list-style-type:none}.lst-kix_8b09c5wbiylz-6>li:before{content:"\0025cf   "}ul.lst-kix_ee0rw9sgmv1o-5{list-style-type:none}ul.lst-kix_42ndt3nzggvw-3{list-style-type:none}.lst-kix_bym5woclzz8f-2>li:before{content:"\0025a0   "}ul.lst-kix_ee0rw9sgmv1o-4{list-style-type:none}ul.lst-kix_42ndt3nzggvw-4{list-style-type:none}ul.lst-kix_ee0rw9sgmv1o-7{list-style-type:none}ul.lst-kix_42ndt3nzggvw-5{list-style-type:none}ul.lst-kix_ee0rw9sgmv1o-6{list-style-type:none}ul.lst-kix_42ndt3nzggvw-6{list-style-type:none}ul.lst-kix_42ndt3nzggvw-7{list-style-type:none}ul.lst-kix_ee0rw9sgmv1o-8{list-style-type:none}ul.lst-kix_42ndt3nzggvw-8{list-style-type:none}.lst-kix_qjs84hdj3als-7>li:before{content:"\0025cb   "}.lst-kix_nz0skx7ahmk9-0>li:before{content:"\0025cf   "}.lst-kix_qzcbcbds5p8j-0>li:before{content:"\0025cf   "}.lst-kix_v8i0z8oris9z-7>li:before{content:"\0025cb   "}.lst-kix_5jkev5cbxpxc-5>li:before{content:"\0025a0   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_tdg51sdobgli-6>li:before{content:"\0025cf   "}.lst-kix_z8dy5ehg0j6m-7>li:before{content:"\0025cb   "}.lst-kix_3n0bx46jp1u4-6>li:before{content:"\0025cf   "}.lst-kix_kz0xo3ln4wpk-2>li:before{content:"\0025a0   "}.lst-kix_i6q6s2czm8np-5>li:before{content:"\0025a0   "}.lst-kix_pxlscdvf5lia-6>li:before{content:"\0025cf   "}.lst-kix_99xvjj4uyl11-1>li:before{content:"\0025cb   "}.lst-kix_dmf65t97trau-4>li:before{content:"\0025cb   "}ul.lst-kix_mdf3w72jkmgx-1{list-style-type:none}ul.lst-kix_mdf3w72jkmgx-0{list-style-type:none}ul.lst-kix_mdf3w72jkmgx-5{list-style-type:none}ul.lst-kix_mdf3w72jkmgx-4{list-style-type:none}.lst-kix_3t7g0caoicba-5>li:before{content:"\0025a0   "}ul.lst-kix_mdf3w72jkmgx-3{list-style-type:none}ul.lst-kix_mdf3w72jkmgx-2{list-style-type:none}.lst-kix_3t7g0caoicba-6>li:before{content:"\0025cf   "}ul.lst-kix_mdf3w72jkmgx-8{list-style-type:none}ul.lst-kix_mdf3w72jkmgx-7{list-style-type:none}ul.lst-kix_mdf3w72jkmgx-6{list-style-type:none}.lst-kix_3t7g0caoicba-8>li:before{content:"\0025a0   "}.lst-kix_3t7g0caoicba-7>li:before{content:"\0025cb   "}ul.lst-kix_qzcbcbds5p8j-6{list-style-type:none}.lst-kix_kj34ho6oavv6-7>li:before{content:"\0025cb   "}ul.lst-kix_qzcbcbds5p8j-7{list-style-type:none}.lst-kix_54ck8ch4bn7o-0>li:before{content:"\0027a2   "}ul.lst-kix_qzcbcbds5p8j-8{list-style-type:none}.lst-kix_kj34ho6oavv6-6>li:before{content:"\0025cf   "}.lst-kix_kj34ho6oavv6-8>li:before{content:"\0025a0   "}.lst-kix_54ck8ch4bn7o-1>li:before{content:"\0025cb   "}ul.lst-kix_42ndt3nzggvw-0{list-style-type:none}.lst-kix_kj34ho6oavv6-3>li:before{content:"\0025cf   "}.lst-kix_54ck8ch4bn7o-3>li:before{content:"\0025cf   "}.lst-kix_54ck8ch4bn7o-4>li:before{content:"\0025cb   "}ul.lst-kix_qzcbcbds5p8j-0{list-style-type:none}.lst-kix_kj34ho6oavv6-4>li:before{content:"\0025cb   "}ul.lst-kix_qzcbcbds5p8j-1{list-style-type:none}.lst-kix_sznnvmk8su1j-8>li:before{content:"\0025a0   "}ul.lst-kix_qzcbcbds5p8j-2{list-style-type:none}.lst-kix_kj34ho6oavv6-5>li:before{content:"\0025a0   "}ul.lst-kix_qzcbcbds5p8j-3{list-style-type:none}.lst-kix_54ck8ch4bn7o-2>li:before{content:"\0025a0   "}ul.lst-kix_qzcbcbds5p8j-4{list-style-type:none}ul.lst-kix_qzcbcbds5p8j-5{list-style-type:none}.lst-kix_sznnvmk8su1j-7>li:before{content:"\0025cb   "}.lst-kix_3t7g0caoicba-4>li:before{content:"\0025cb   "}ul.lst-kix_kz0xo3ln4wpk-1{list-style-type:none}ul.lst-kix_kz0xo3ln4wpk-0{list-style-type:none}.lst-kix_3t7g0caoicba-3>li:before{content:"\0025cf   "}.lst-kix_sznnvmk8su1j-4>li:before{content:"\0025cb   "}.lst-kix_sznnvmk8su1j-6>li:before{content:"\0025cf   "}.lst-kix_3t7g0caoicba-2>li:before{content:"\0025a0   "}ul.lst-kix_kz0xo3ln4wpk-5{list-style-type:none}ul.lst-kix_kz0xo3ln4wpk-4{list-style-type:none}ul.lst-kix_kz0xo3ln4wpk-3{list-style-type:none}ul.lst-kix_kz0xo3ln4wpk-2{list-style-type:none}.lst-kix_sznnvmk8su1j-5>li:before{content:"\0025a0   "}.lst-kix_3t7g0caoicba-0>li:before{content:"\0025cf   "}.lst-kix_3t7g0caoicba-1>li:before{content:"\0025cb   "}.lst-kix_sznnvmk8su1j-2>li:before{content:"\0025a0   "}.lst-kix_sznnvmk8su1j-3>li:before{content:"\0025cf   "}ul.lst-kix_ncyelpl3aut1-3{list-style-type:none}ul.lst-kix_ncyelpl3aut1-2{list-style-type:none}ul.lst-kix_ncyelpl3aut1-1{list-style-type:none}ul.lst-kix_ncyelpl3aut1-0{list-style-type:none}ul.lst-kix_8pfaa7u0e2zg-1{list-style-type:none}ul.lst-kix_kz0xo3ln4wpk-8{list-style-type:none}ul.lst-kix_8pfaa7u0e2zg-2{list-style-type:none}ul.lst-kix_kz0xo3ln4wpk-7{list-style-type:none}ul.lst-kix_kz0xo3ln4wpk-6{list-style-type:none}ul.lst-kix_8pfaa7u0e2zg-0{list-style-type:none}.lst-kix_kj34ho6oavv6-0>li:before{content:"\0027a2   "}.lst-kix_kj34ho6oavv6-2>li:before{content:"\0025a0   "}.lst-kix_kj34ho6oavv6-1>li:before{content:"\0025cb   "}ul.lst-kix_qjs84hdj3als-2{list-style-type:none}ul.lst-kix_qjs84hdj3als-3{list-style-type:none}ul.lst-kix_qjs84hdj3als-0{list-style-type:none}ul.lst-kix_qjs84hdj3als-1{list-style-type:none}ul.lst-kix_qjs84hdj3als-8{list-style-type:none}ul.lst-kix_ncyelpl3aut1-8{list-style-type:none}ul.lst-kix_qjs84hdj3als-6{list-style-type:none}ul.lst-kix_ncyelpl3aut1-7{list-style-type:none}ul.lst-kix_qjs84hdj3als-7{list-style-type:none}ul.lst-kix_ncyelpl3aut1-6{list-style-type:none}ul.lst-kix_qjs84hdj3als-4{list-style-type:none}ul.lst-kix_ncyelpl3aut1-5{list-style-type:none}ul.lst-kix_qjs84hdj3als-5{list-style-type:none}ul.lst-kix_ncyelpl3aut1-4{list-style-type:none}ul.lst-kix_64f0218k1822-7{list-style-type:none}.lst-kix_dmf65t97trau-1>li:before{content:"\0025cb   "}ul.lst-kix_dmf65t97trau-2{list-style-type:none}.lst-kix_4chf7gspsjev-6>li:before{content:"\0025cf   "}ul.lst-kix_64f0218k1822-6{list-style-type:none}ul.lst-kix_dmf65t97trau-1{list-style-type:none}ul.lst-kix_64f0218k1822-5{list-style-type:none}ul.lst-kix_dmf65t97trau-4{list-style-type:none}ul.lst-kix_64f0218k1822-4{list-style-type:none}ul.lst-kix_dmf65t97trau-3{list-style-type:none}ul.lst-kix_64f0218k1822-3{list-style-type:none}.lst-kix_dmf65t97trau-3>li:before{content:"\0025cf   "}ul.lst-kix_dmf65t97trau-6{list-style-type:none}.lst-kix_4chf7gspsjev-4>li:before{content:"\0025cb   "}.lst-kix_4chf7gspsjev-8>li:before{content:"\0025a0   "}ul.lst-kix_64f0218k1822-2{list-style-type:none}ul.lst-kix_dmf65t97trau-5{list-style-type:none}ul.lst-kix_64f0218k1822-1{list-style-type:none}ul.lst-kix_dmf65t97trau-8{list-style-type:none}ul.lst-kix_64f0218k1822-0{list-style-type:none}ul.lst-kix_dmf65t97trau-7{list-style-type:none}ul.lst-kix_8nxwnwrfsavq-1{list-style-type:none}ul.lst-kix_8nxwnwrfsavq-0{list-style-type:none}ul.lst-kix_8nxwnwrfsavq-3{list-style-type:none}ul.lst-kix_8nxwnwrfsavq-2{list-style-type:none}ul.lst-kix_dmf65t97trau-0{list-style-type:none}ul.lst-kix_8nxwnwrfsavq-5{list-style-type:none}ul.lst-kix_64f0218k1822-8{list-style-type:none}ul.lst-kix_8nxwnwrfsavq-4{list-style-type:none}ul.lst-kix_v8i0z8oris9z-1{list-style-type:none}ul.lst-kix_v8i0z8oris9z-0{list-style-type:none}.lst-kix_4chf7gspsjev-0>li:before{content:"\0027a2   "}ul.lst-kix_v8i0z8oris9z-3{list-style-type:none}ul.lst-kix_v8i0z8oris9z-2{list-style-type:none}ul.lst-kix_v8i0z8oris9z-5{list-style-type:none}ul.lst-kix_v8i0z8oris9z-4{list-style-type:none}.lst-kix_4chf7gspsjev-2>li:before{content:"\0025a0   "}ul.lst-kix_v8i0z8oris9z-7{list-style-type:none}ul.lst-kix_v8i0z8oris9z-6{list-style-type:none}ul.lst-kix_v8i0z8oris9z-8{list-style-type:none}.lst-kix_9l57ldnvor4v-3>li:before{content:"\0025cf   "}ul.lst-kix_yorg1uctntus-8{list-style-type:none}ul.lst-kix_yorg1uctntus-7{list-style-type:none}.lst-kix_8b09c5wbiylz-7>li:before{content:"\0025cf   "}ul.lst-kix_yorg1uctntus-6{list-style-type:none}ul.lst-kix_yorg1uctntus-5{list-style-type:none}ul.lst-kix_yorg1uctntus-4{list-style-type:none}.lst-kix_9l57ldnvor4v-7>li:before{content:"\0025cb   "}ul.lst-kix_yorg1uctntus-3{list-style-type:none}ul.lst-kix_yorg1uctntus-2{list-style-type:none}ul.lst-kix_yorg1uctntus-1{list-style-type:none}ul.lst-kix_yorg1uctntus-0{list-style-type:none}.lst-kix_9l57ldnvor4v-1>li:before{content:"\0025cb   "}ul.lst-kix_8nxwnwrfsavq-7{list-style-type:none}ul.lst-kix_8nxwnwrfsavq-6{list-style-type:none}ul.lst-kix_z8dy5ehg0j6m-1{list-style-type:none}ul.lst-kix_8nxwnwrfsavq-8{list-style-type:none}ul.lst-kix_z8dy5ehg0j6m-0{list-style-type:none}ul.lst-kix_z8dy5ehg0j6m-3{list-style-type:none}ul.lst-kix_z8dy5ehg0j6m-2{list-style-type:none}ul.lst-kix_z8dy5ehg0j6m-5{list-style-type:none}ul.lst-kix_z8dy5ehg0j6m-4{list-style-type:none}ul.lst-kix_z8dy5ehg0j6m-7{list-style-type:none}ul.lst-kix_z8dy5ehg0j6m-6{list-style-type:none}ul.lst-kix_z8dy5ehg0j6m-8{list-style-type:none}.lst-kix_9l57ldnvor4v-5>li:before{content:"\0025a0   "}.lst-kix_ncyelpl3aut1-1>li:before{content:"\0025cb   "}.lst-kix_ncyelpl3aut1-3>li:before{content:"\0025cf   "}ul.lst-kix_otug9mhs4rmx-0{list-style-type:none}ul.lst-kix_otug9mhs4rmx-1{list-style-type:none}.lst-kix_sznnvmk8su1j-1>li:before{content:"\0025cb   "}ul.lst-kix_bym5woclzz8f-3{list-style-type:none}ul.lst-kix_bym5woclzz8f-4{list-style-type:none}ul.lst-kix_bym5woclzz8f-5{list-style-type:none}.lst-kix_ncyelpl3aut1-5>li:before{content:"\0025a0   "}.lst-kix_ncyelpl3aut1-7>li:before{content:"\0025cb   "}ul.lst-kix_bym5woclzz8f-6{list-style-type:none}ul.lst-kix_bym5woclzz8f-7{list-style-type:none}ul.lst-kix_bym5woclzz8f-8{list-style-type:none}.lst-kix_54ck8ch4bn7o-7>li:before{content:"\0025cb   "}.lst-kix_nz0skx7ahmk9-7>li:before{content:"\0025cb   "}.lst-kix_8b09c5wbiylz-3>li:before{content:"\0025cf   "}ul.lst-kix_bym5woclzz8f-0{list-style-type:none}.lst-kix_54ck8ch4bn7o-5>li:before{content:"\0025a0   "}ul.lst-kix_bym5woclzz8f-1{list-style-type:none}ul.lst-kix_bym5woclzz8f-2{list-style-type:none}.lst-kix_8b09c5wbiylz-5>li:before{content:"\0025cf   "}.lst-kix_qjs84hdj3als-4>li:before{content:"\0025cb   "}.lst-kix_bym5woclzz8f-3>li:before{content:"\0025cf   "}.lst-kix_qjs84hdj3als-6>li:before{content:"\0025cf   "}.lst-kix_nz0skx7ahmk9-1>li:before{content:"\0025cb   "}.lst-kix_bym5woclzz8f-1>li:before{content:"\0025cb   "}.lst-kix_qjs84hdj3als-8>li:before{content:"\0025a0   "}.lst-kix_8b09c5wbiylz-1>li:before{content:"\0025cf   "}.lst-kix_dmf65t97trau-7>li:before{content:"\0025cb   "}.lst-kix_nz0skx7ahmk9-5>li:before{content:"\0025a0   "}.lst-kix_dmf65t97trau-5>li:before{content:"\0025a0   "}ul.lst-kix_otug9mhs4rmx-2{list-style-type:none}ul.lst-kix_otug9mhs4rmx-3{list-style-type:none}.lst-kix_nz0skx7ahmk9-3>li:before{content:"\0025cf   "}ul.lst-kix_otug9mhs4rmx-4{list-style-type:none}ul.lst-kix_otug9mhs4rmx-5{list-style-type:none}ul.lst-kix_otug9mhs4rmx-6{list-style-type:none}ul.lst-kix_otug9mhs4rmx-7{list-style-type:none}ul.lst-kix_otug9mhs4rmx-8{list-style-type:none}.lst-kix_3oqam8vafvbn-0>li:before{content:"\0025cf   "}ul.lst-kix_kj34ho6oavv6-8{list-style-type:none}.lst-kix_1c2feo7jow6-7>li:before{content:"\0025cb   "}ul.lst-kix_kj34ho6oavv6-5{list-style-type:none}ul.lst-kix_kj34ho6oavv6-4{list-style-type:none}.lst-kix_3oqam8vafvbn-3>li:before{content:"\0025cf   "}ul.lst-kix_kj34ho6oavv6-7{list-style-type:none}ul.lst-kix_kj34ho6oavv6-6{list-style-type:none}.lst-kix_3oqam8vafvbn-4>li:before{content:"\0025cb   "}ul.lst-kix_kj34ho6oavv6-1{list-style-type:none}.lst-kix_nvbe1nnkagj0-1>li:before{content:"\0025cf   "}ul.lst-kix_kj34ho6oavv6-0{list-style-type:none}.lst-kix_1c2feo7jow6-3>li:before{content:"\0025cf   "}ul.lst-kix_kj34ho6oavv6-3{list-style-type:none}ul.lst-kix_kj34ho6oavv6-2{list-style-type:none}.lst-kix_nvbe1nnkagj0-2>li:before{content:"\0025cf   "}.lst-kix_q24skcnjivy0-7>li:before{content:"\0025cb   "}.lst-kix_1c2feo7jow6-4>li:before{content:"\0025cb   "}.lst-kix_ee0rw9sgmv1o-1>li:before{content:"\0025cb   "}.lst-kix_nfy673qapcua-5>li:before{content:"\0025cf   "}.lst-kix_nfy673qapcua-4>li:before{content:"\0025cf   "}.lst-kix_nfy673qapcua-8>li:before{content:"\0025cf   "}.lst-kix_n1ax58vdrowu-7>li:before{content:"\0025cb   "}.lst-kix_n1ax58vdrowu-6>li:before{content:"\0025cf   "}.lst-kix_nvbe1nnkagj0-6>li:before{content:"\0025cf   "}.lst-kix_1c2feo7jow6-0>li:before{content:"\0027a2   "}.lst-kix_nvbe1nnkagj0-5>li:before{content:"\0025cf   "}.lst-kix_5jkev5cbxpxc-0>li:before{content:"\0025cf   "}.lst-kix_5qkugsusxliw-0>li:before{content:"\0025cf   "}ul.lst-kix_99524jad4mmq-0{list-style-type:none}.lst-kix_jxs8idtrfx9u-8>li:before{content:"\0025a0   "}ul.lst-kix_99524jad4mmq-1{list-style-type:none}.lst-kix_bym5woclzz8f-4>li:before{content:"\0025cb   "}ul.lst-kix_99524jad4mmq-4{list-style-type:none}.lst-kix_nfy673qapcua-0>li:before{content:"\0025cf   "}.lst-kix_jxs8idtrfx9u-5>li:before{content:"\0025a0   "}ul.lst-kix_99524jad4mmq-5{list-style-type:none}.lst-kix_5qkugsusxliw-1>li:before{content:"\0025cb   "}ul.lst-kix_99524jad4mmq-2{list-style-type:none}.lst-kix_nfy673qapcua-1>li:before{content:"\0025cf   "}ul.lst-kix_99524jad4mmq-3{list-style-type:none}.lst-kix_qjs84hdj3als-2>li:before{content:"\0025a0   "}.lst-kix_5qkugsusxliw-4>li:before{content:"\0025cb   "}ul.lst-kix_99524jad4mmq-8{list-style-type:none}.lst-kix_jxs8idtrfx9u-4>li:before{content:"\0025cb   "}.lst-kix_qjs84hdj3als-1>li:before{content:"\0025cb   "}.lst-kix_bym5woclzz8f-8>li:before{content:"\0025a0   "}ul.lst-kix_99524jad4mmq-6{list-style-type:none}ul.lst-kix_99524jad4mmq-7{list-style-type:none}.lst-kix_bym5woclzz8f-7>li:before{content:"\0025cb   "}.lst-kix_3oqam8vafvbn-8>li:before{content:"\0025a0   "}.lst-kix_ysk5buayro0y-1>li:before{content:"\0025cb   "}.lst-kix_q24skcnjivy0-6>li:before{content:"\0025cf   "}.lst-kix_5qkugsusxliw-8>li:before{content:"\0025a0   "}.lst-kix_ee0rw9sgmv1o-2>li:before{content:"\0025a0   "}.lst-kix_3oqam8vafvbn-7>li:before{content:"\0025cb   "}.lst-kix_5qkugsusxliw-5>li:before{content:"\0025a0   "}ul.lst-kix_3n0bx46jp1u4-2{list-style-type:none}ul.lst-kix_3n0bx46jp1u4-3{list-style-type:none}.lst-kix_q24skcnjivy0-2>li:before{content:"\0025a0   "}ul.lst-kix_3n0bx46jp1u4-0{list-style-type:none}.lst-kix_ee0rw9sgmv1o-6>li:before{content:"\0025cf   "}ul.lst-kix_3n0bx46jp1u4-1{list-style-type:none}.lst-kix_15risnu9xchg-6>li:before{content:"\0025cf   "}.lst-kix_ysk5buayro0y-2>li:before{content:"\0025a0   "}.lst-kix_q24skcnjivy0-3>li:before{content:"\0025cf   "}.lst-kix_1c2feo7jow6-8>li:before{content:"\0025a0   "}.lst-kix_ee0rw9sgmv1o-5>li:before{content:"\0025a0   "}ul.lst-kix_3n0bx46jp1u4-8{list-style-type:none}.lst-kix_15risnu9xchg-2>li:before{content:"\0025a0   "}ul.lst-kix_3n0bx46jp1u4-6{list-style-type:none}ul.lst-kix_3n0bx46jp1u4-7{list-style-type:none}ul.lst-kix_3n0bx46jp1u4-4{list-style-type:none}ul.lst-kix_3n0bx46jp1u4-5{list-style-type:none}.lst-kix_15risnu9xchg-1>li:before{content:"\0025cb   "}.lst-kix_15risnu9xchg-5>li:before{content:"\0025a0   "}.lst-kix_8pvyoajrhbtr-3>li:before{content:"\0025cf   "}.lst-kix_ysk5buayro0y-5>li:before{content:"\0025a0   "}.lst-kix_8pvyoajrhbtr-4>li:before{content:"\0025cb   "}ul.lst-kix_m2dc47g2g6zf-0{list-style-type:none}ul.lst-kix_m2dc47g2g6zf-1{list-style-type:none}.lst-kix_ysk5buayro0y-6>li:before{content:"\0025cf   "}ul.lst-kix_m2dc47g2g6zf-2{list-style-type:none}ul.lst-kix_m2dc47g2g6zf-3{list-style-type:none}ul.lst-kix_m2dc47g2g6zf-4{list-style-type:none}.lst-kix_tq2qp7s2zqek-4>li:before{content:"\0025cb   "}ul.lst-kix_m2dc47g2g6zf-5{list-style-type:none}.lst-kix_jxs8idtrfx9u-0>li:before{content:"\0025cf   "}ul.lst-kix_m2dc47g2g6zf-6{list-style-type:none}.lst-kix_tq2qp7s2zqek-3>li:before{content:"\0025cf   "}ul.lst-kix_m2dc47g2g6zf-7{list-style-type:none}.lst-kix_8pvyoajrhbtr-8>li:before{content:"\0025a0   "}ul.lst-kix_m2dc47g2g6zf-8{list-style-type:none}.lst-kix_jxs8idtrfx9u-1>li:before{content:"\0025cb   "}.lst-kix_8pvyoajrhbtr-7>li:before{content:"\0025cb   "}.lst-kix_tq2qp7s2zqek-8>li:before{content:"\0025a0   "}.lst-kix_tq2qp7s2zqek-7>li:before{content:"\0025cb   "}.lst-kix_ascolikyamcy-5>li:before{content:"\0025a0   "}.lst-kix_n1ax58vdrowu-2>li:before{content:"\0025a0   "}.lst-kix_n1ax58vdrowu-3>li:before{content:"\0025cf   "}.lst-kix_ascolikyamcy-2>li:before{content:"\0025a0   "}.lst-kix_ascolikyamcy-1>li:before{content:"\0025cb   "}ul.lst-kix_nvbe1nnkagj0-2{list-style-type:none}ul.lst-kix_ssxyep2erusf-1{list-style-type:none}ul.lst-kix_nvbe1nnkagj0-3{list-style-type:none}ul.lst-kix_ssxyep2erusf-0{list-style-type:none}ul.lst-kix_nvbe1nnkagj0-0{list-style-type:none}ul.lst-kix_ssxyep2erusf-3{list-style-type:none}ul.lst-kix_nvbe1nnkagj0-1{list-style-type:none}ul.lst-kix_ssxyep2erusf-2{list-style-type:none}ul.lst-kix_nvbe1nnkagj0-6{list-style-type:none}ul.lst-kix_nvbe1nnkagj0-7{list-style-type:none}ul.lst-kix_nvbe1nnkagj0-4{list-style-type:none}ul.lst-kix_nvbe1nnkagj0-5{list-style-type:none}.lst-kix_tq2qp7s2zqek-0>li:before{content:"\0025cf   "}ul.lst-kix_ssxyep2erusf-8{list-style-type:none}ul.lst-kix_nvbe1nnkagj0-8{list-style-type:none}.lst-kix_hg8waqqzu11r-0>li:before{content:"\0025cf   "}ul.lst-kix_ssxyep2erusf-5{list-style-type:none}ul.lst-kix_ssxyep2erusf-4{list-style-type:none}ul.lst-kix_ssxyep2erusf-7{list-style-type:none}ul.lst-kix_ssxyep2erusf-6{list-style-type:none}.lst-kix_4chf7gspsjev-5>li:before{content:"\0025a0   "}.lst-kix_hg8waqqzu11r-4>li:before{content:"\0025cb   "}ul.lst-kix_ohkm6rcowih5-8{list-style-type:none}ul.lst-kix_ohkm6rcowih5-7{list-style-type:none}.lst-kix_dmf65t97trau-2>li:before{content:"\0025a0   "}ul.lst-kix_ohkm6rcowih5-4{list-style-type:none}.lst-kix_ssxyep2erusf-1>li:before{content:"\0025cb   "}ul.lst-kix_ohkm6rcowih5-3{list-style-type:none}ul.lst-kix_ohkm6rcowih5-6{list-style-type:none}ul.lst-kix_ohkm6rcowih5-5{list-style-type:none}ul.lst-kix_ohkm6rcowih5-0{list-style-type:none}ul.lst-kix_ohkm6rcowih5-2{list-style-type:none}ul.lst-kix_ohkm6rcowih5-1{list-style-type:none}.lst-kix_eqih6is7mr7q-5>li:before{content:"\0025a0   "}.lst-kix_ssxyep2erusf-5>li:before{content:"\0025a0   "}.lst-kix_4chf7gspsjev-1>li:before{content:"\0025cb   "}.lst-kix_kz0xo3ln4wpk-8>li:before{content:"\0025a0   "}.lst-kix_hg8waqqzu11r-8>li:before{content:"\0025a0   "}.lst-kix_eqih6is7mr7q-1>li:before{content:"\0025cb   "}.lst-kix_9l57ldnvor4v-4>li:before{content:"\0025cb   "}ul.lst-kix_1c2feo7jow6-6{list-style-type:none}.lst-kix_8b09c5wbiylz-8>li:before{content:"\0025cf   "}ul.lst-kix_1c2feo7jow6-7{list-style-type:none}ul.lst-kix_1c2feo7jow6-8{list-style-type:none}.lst-kix_tdg51sdobgli-0>li:before{content:"\0025cf   "}ul.lst-kix_1c2feo7jow6-2{list-style-type:none}.lst-kix_9l57ldnvor4v-0>li:before{content:"\0027a2   "}ul.lst-kix_1c2feo7jow6-3{list-style-type:none}ul.lst-kix_1c2feo7jow6-4{list-style-type:none}ul.lst-kix_1c2feo7jow6-5{list-style-type:none}.lst-kix_dllt5p3q6235-5>li:before{content:"\0025a0   "}.lst-kix_9l57ldnvor4v-8>li:before{content:"\0025a0   "}ul.lst-kix_1c2feo7jow6-0{list-style-type:none}ul.lst-kix_1c2feo7jow6-1{list-style-type:none}ul.lst-kix_8pfaa7u0e2zg-7{list-style-type:none}ul.lst-kix_8pfaa7u0e2zg-8{list-style-type:none}ul.lst-kix_8pfaa7u0e2zg-5{list-style-type:none}ul.lst-kix_8pfaa7u0e2zg-6{list-style-type:none}ul.lst-kix_8pfaa7u0e2zg-3{list-style-type:none}ul.lst-kix_8pfaa7u0e2zg-4{list-style-type:none}.lst-kix_dllt5p3q6235-1>li:before{content:"\0025cb   "}.lst-kix_wa5eplpvj99e-7>li:before{content:"\0025cb   "}.lst-kix_8pvyoajrhbtr-0>li:before{content:"\0025cf   "}.lst-kix_wa5eplpvj99e-3>li:before{content:"\0025cf   "}.lst-kix_ykbty9k091s5-5>li:before{content:"\0025a0   "}.lst-kix_ncyelpl3aut1-6>li:before{content:"\0025cf   "}.lst-kix_8b09c5wbiylz-4>li:before{content:"\0025cf   "}.lst-kix_ykbty9k091s5-1>li:before{content:"\0025cb   "}.lst-kix_54ck8ch4bn7o-6>li:before{content:"\0025cf   "}.lst-kix_qzcbcbds5p8j-6>li:before{content:"\0025cf   "}.lst-kix_v8i0z8oris9z-1>li:before{content:"\0025cb   "}.lst-kix_ncyelpl3aut1-2>li:before{content:"\0025a0   "}.lst-kix_qjs84hdj3als-5>li:before{content:"\0025a0   "}ul.lst-kix_eqih6is7mr7q-8{list-style-type:none}.lst-kix_z8dy5ehg0j6m-1>li:before{content:"\0025cb   "}ul.lst-kix_ascolikyamcy-6{list-style-type:none}.lst-kix_i6q6s2czm8np-3>li:before{content:"\0025cf   "}ul.lst-kix_ascolikyamcy-7{list-style-type:none}ul.lst-kix_ascolikyamcy-8{list-style-type:none}.lst-kix_tdg51sdobgli-4>li:before{content:"\0025cb   "}ul.lst-kix_eqih6is7mr7q-2{list-style-type:none}ul.lst-kix_eqih6is7mr7q-3{list-style-type:none}.lst-kix_bym5woclzz8f-0>li:before{content:"\0025cf   "}ul.lst-kix_eqih6is7mr7q-0{list-style-type:none}.lst-kix_5jkev5cbxpxc-3>li:before{content:"\0025cf   "}ul.lst-kix_eqih6is7mr7q-1{list-style-type:none}ul.lst-kix_eqih6is7mr7q-6{list-style-type:none}ul.lst-kix_eqih6is7mr7q-7{list-style-type:none}.lst-kix_qzcbcbds5p8j-2>li:before{content:"\0025cf   "}ul.lst-kix_eqih6is7mr7q-4{list-style-type:none}.lst-kix_v8i0z8oris9z-5>li:before{content:"\0025a0   "}ul.lst-kix_eqih6is7mr7q-5{list-style-type:none}.lst-kix_kz0xo3ln4wpk-4>li:before{content:"\0025cb   "}.lst-kix_nz0skx7ahmk9-6>li:before{content:"\0025cf   "}.lst-kix_5jkev5cbxpxc-7>li:before{content:"\0025cb   "}.lst-kix_i6q6s2czm8np-7>li:before{content:"\0025cb   "}.lst-kix_8b09c5wbiylz-0>li:before{content:"\0025cf   "}.lst-kix_dmf65t97trau-6>li:before{content:"\0025cf   "}.lst-kix_tdg51sdobgli-8>li:before{content:"\0025a0   "}ul.lst-kix_ascolikyamcy-2{list-style-type:none}ul.lst-kix_ascolikyamcy-3{list-style-type:none}.lst-kix_kz0xo3ln4wpk-0>li:before{content:"\0025cf   "}.lst-kix_nz0skx7ahmk9-2>li:before{content:"\0025a0   "}ul.lst-kix_ascolikyamcy-4{list-style-type:none}.lst-kix_z8dy5ehg0j6m-5>li:before{content:"\0025a0   "}ul.lst-kix_ascolikyamcy-5{list-style-type:none}ul.lst-kix_ascolikyamcy-0{list-style-type:none}ul.lst-kix_ascolikyamcy-1{list-style-type:none}ul.lst-kix_sfz3dfa4n7w2-8{list-style-type:none}.lst-kix_m2rwr78ppx87-7>li:before{content:"\0025cb   "}.lst-kix_91vk0di44se5-1>li:before{content:"\0025cf   "}.lst-kix_hi6xckadgtq6-3>li:before{content:"\0025cf   "}.lst-kix_5hd2wrb71s38-3>li:before{content:"\0025cf   "}ul.lst-kix_sfz3dfa4n7w2-4{list-style-type:none}ul.lst-kix_sfz3dfa4n7w2-5{list-style-type:none}.lst-kix_5hd2wrb71s38-2>li:before{content:"\0025a0   "}ul.lst-kix_sfz3dfa4n7w2-6{list-style-type:none}ul.lst-kix_sfz3dfa4n7w2-7{list-style-type:none}.lst-kix_hi6xckadgtq6-4>li:before{content:"\0025cb   "}ul.lst-kix_sfz3dfa4n7w2-0{list-style-type:none}ul.lst-kix_sfz3dfa4n7w2-1{list-style-type:none}ul.lst-kix_sfz3dfa4n7w2-2{list-style-type:none}ul.lst-kix_sfz3dfa4n7w2-3{list-style-type:none}.lst-kix_m2rwr78ppx87-1>li:before{content:"\0025cb   "}.lst-kix_5hd2wrb71s38-0>li:before{content:"\0025cf   "}.lst-kix_5hd2wrb71s38-8>li:before{content:"\0025a0   "}ul.lst-kix_bb18g463362n-0{list-style-type:none}.lst-kix_91vk0di44se5-6>li:before{content:"\0025cf   "}.lst-kix_hi6xckadgtq6-6>li:before{content:"\0025cf   "}ul.lst-kix_bb18g463362n-1{list-style-type:none}ul.lst-kix_bb18g463362n-2{list-style-type:none}.lst-kix_m2dc47g2g6zf-5>li:before{content:"\0025a0   "}.lst-kix_m2dc47g2g6zf-7>li:before{content:"\0025cb   "}ul.lst-kix_bb18g463362n-3{list-style-type:none}ul.lst-kix_bb18g463362n-4{list-style-type:none}ul.lst-kix_bb18g463362n-5{list-style-type:none}ul.lst-kix_bb18g463362n-6{list-style-type:none}ul.lst-kix_w7uhve9aueba-8{list-style-type:none}ul.lst-kix_bb18g463362n-7{list-style-type:none}ul.lst-kix_bb18g463362n-8{list-style-type:none}.lst-kix_91vk0di44se5-4>li:before{content:"\0025cf   "}.lst-kix_5hd2wrb71s38-5>li:before{content:"\0025a0   "}.lst-kix_hi6xckadgtq6-1>li:before{content:"\0025cb   "}.lst-kix_m2dc47g2g6zf-8>li:before{content:"\0025a0   "}.lst-kix_sfz3dfa4n7w2-3>li:before{content:"\0025cf   "}.lst-kix_sfz3dfa4n7w2-4>li:before{content:"\0025cb   "}.lst-kix_sfz3dfa4n7w2-1>li:before{content:"\0025cb   "}ul.lst-kix_w7uhve9aueba-6{list-style-type:none}ul.lst-kix_x7gq6rngbuia-0{list-style-type:none}ul.lst-kix_w7uhve9aueba-7{list-style-type:none}ul.lst-kix_x7gq6rngbuia-1{list-style-type:none}ul.lst-kix_w7uhve9aueba-4{list-style-type:none}ul.lst-kix_x7gq6rngbuia-2{list-style-type:none}ul.lst-kix_w7uhve9aueba-5{list-style-type:none}ul.lst-kix_x7gq6rngbuia-3{list-style-type:none}ul.lst-kix_w7uhve9aueba-2{list-style-type:none}ul.lst-kix_x7gq6rngbuia-4{list-style-type:none}ul.lst-kix_w7uhve9aueba-3{list-style-type:none}ul.lst-kix_x7gq6rngbuia-5{list-style-type:none}ul.lst-kix_w7uhve9aueba-0{list-style-type:none}ul.lst-kix_x7gq6rngbuia-6{list-style-type:none}.lst-kix_99524jad4mmq-7>li:before{content:"\0025cb   "}ul.lst-kix_w7uhve9aueba-1{list-style-type:none}ul.lst-kix_x7gq6rngbuia-7{list-style-type:none}ul.lst-kix_x7gq6rngbuia-8{list-style-type:none}.lst-kix_91vk0di44se5-7>li:before{content:"\0025cf   "}.lst-kix_m2rwr78ppx87-2>li:before{content:"\0025a0   "}.lst-kix_m2dc47g2g6zf-2>li:before{content:"\0025a0   "}.lst-kix_m2rwr78ppx87-4>li:before{content:"\0025cb   "}.lst-kix_m2dc47g2g6zf-0>li:before{content:"\0025cf   "}ul.lst-kix_i6q6s2czm8np-0{list-style-type:none}ul.lst-kix_i6q6s2czm8np-1{list-style-type:none}ul.lst-kix_i6q6s2czm8np-2{list-style-type:none}ul.lst-kix_i6q6s2czm8np-3{list-style-type:none}.lst-kix_otug9mhs4rmx-1>li:before{content:"\0025cb   "}.lst-kix_u4gcy9afd1io-1>li:before{content:"\0025cb   "}ul.lst-kix_i6q6s2czm8np-8{list-style-type:none}.lst-kix_otug9mhs4rmx-3>li:before{content:"\0025cf   "}ul.lst-kix_i6q6s2czm8np-4{list-style-type:none}.lst-kix_u4gcy9afd1io-4>li:before{content:"\0025cb   "}ul.lst-kix_i6q6s2czm8np-5{list-style-type:none}ul.lst-kix_i6q6s2czm8np-6{list-style-type:none}ul.lst-kix_i6q6s2czm8np-7{list-style-type:none}.lst-kix_99524jad4mmq-2>li:before{content:"\0025a0   "}.lst-kix_99524jad4mmq-1>li:before{content:"\0025cb   "}.lst-kix_u4gcy9afd1io-6>li:before{content:"\0025cf   "}.lst-kix_sfz3dfa4n7w2-6>li:before{content:"\0025cf   "}.lst-kix_99524jad4mmq-4>li:before{content:"\0025cb   "}.lst-kix_otug9mhs4rmx-0>li:before{content:"\0027a2   "}.lst-kix_u4gcy9afd1io-7>li:before{content:"\0025cb   "}ul.lst-kix_m2rwr78ppx87-0{list-style-type:none}ul.lst-kix_m2rwr78ppx87-2{list-style-type:none}ul.lst-kix_m2rwr78ppx87-1{list-style-type:none}ul.lst-kix_m2rwr78ppx87-4{list-style-type:none}ul.lst-kix_m2rwr78ppx87-3{list-style-type:none}ul.lst-kix_m2rwr78ppx87-6{list-style-type:none}ul.lst-kix_m2rwr78ppx87-5{list-style-type:none}ul.lst-kix_m2rwr78ppx87-8{list-style-type:none}ul.lst-kix_m2rwr78ppx87-7{list-style-type:none}.lst-kix_hg8waqqzu11r-3>li:before{content:"\0025cf   "}.lst-kix_s8kajunwpmir-8>li:before{content:"\0025cf   "}.lst-kix_eqih6is7mr7q-2>li:before{content:"\0025a0   "}.lst-kix_eqih6is7mr7q-4>li:before{content:"\0025cb   "}.lst-kix_hg8waqqzu11r-1>li:before{content:"\0025cb   "}.lst-kix_pxlscdvf5lia-7>li:before{content:"\0025cb   "}.lst-kix_s8kajunwpmir-6>li:before{content:"\0025cf   "}ul.lst-kix_xrzk5cfxtw4m-0{list-style-type:none}.lst-kix_64f0218k1822-1>li:before{content:"\0025cb   "}ul.lst-kix_xrzk5cfxtw4m-1{list-style-type:none}.lst-kix_bb18g463362n-2>li:before{content:"\0025a0   "}.lst-kix_ssxyep2erusf-2>li:before{content:"\0025a0   "}.lst-kix_x7gq6rngbuia-4>li:before{content:"\0025cb   "}ul.lst-kix_xrzk5cfxtw4m-8{list-style-type:none}.lst-kix_bb18g463362n-4>li:before{content:"\0025cb   "}ul.lst-kix_xrzk5cfxtw4m-6{list-style-type:none}ul.lst-kix_xrzk5cfxtw4m-7{list-style-type:none}ul.lst-kix_xrzk5cfxtw4m-4{list-style-type:none}.lst-kix_64f0218k1822-3>li:before{content:"\0025cf   "}ul.lst-kix_xrzk5cfxtw4m-5{list-style-type:none}.lst-kix_ssxyep2erusf-4>li:before{content:"\0025cb   "}ul.lst-kix_xrzk5cfxtw4m-2{list-style-type:none}.lst-kix_x7gq6rngbuia-2>li:before{content:"\0025a0   "}ul.lst-kix_xrzk5cfxtw4m-3{list-style-type:none}ul.lst-kix_utsnp8bclzp3-7{list-style-type:none}ul.lst-kix_utsnp8bclzp3-8{list-style-type:none}ul.lst-kix_utsnp8bclzp3-5{list-style-type:none}.lst-kix_ascolikyamcy-6>li:before{content:"\0025cf   "}ul.lst-kix_utsnp8bclzp3-6{list-style-type:none}.lst-kix_gaab8il5wtid-8>li:before{content:"\0025a0   "}.lst-kix_dllt5p3q6235-8>li:before{content:"\0025a0   "}ul.lst-kix_utsnp8bclzp3-0{list-style-type:none}.lst-kix_ascolikyamcy-8>li:before{content:"\0025a0   "}ul.lst-kix_utsnp8bclzp3-3{list-style-type:none}ul.lst-kix_utsnp8bclzp3-4{list-style-type:none}ul.lst-kix_utsnp8bclzp3-1{list-style-type:none}.lst-kix_dllt5p3q6235-6>li:before{content:"\0025cf   "}ul.lst-kix_utsnp8bclzp3-2{list-style-type:none}.lst-kix_s8kajunwpmir-0>li:before{content:"\0025cf   "}.lst-kix_mdf3w72jkmgx-2>li:before{content:"\0025a0   "}.lst-kix_dllt5p3q6235-0>li:before{content:"\0025cf   "}.lst-kix_80fk82fj1hcs-5>li:before{content:"\0025a0   "}.lst-kix_80fk82fj1hcs-3>li:before{content:"\0025cf   "}.lst-kix_wa5eplpvj99e-6>li:before{content:"\0025cf   "}.lst-kix_mdf3w72jkmgx-8>li:before{content:"\0025a0   "}.lst-kix_ykbty9k091s5-4>li:before{content:"\0025cb   "}.lst-kix_wa5eplpvj99e-4>li:before{content:"\0025cb   "}.lst-kix_qzcbcbds5p8j-7>li:before{content:"\0025cf   "}.lst-kix_bf3umtcrzzqw-4>li:before{content:"\0025cb   "}.lst-kix_bf3umtcrzzqw-6>li:before{content:"\0025cf   "}.lst-kix_v8i0z8oris9z-0>li:before{content:"\0025cf   "}.lst-kix_g1ro4rg087e5-4>li:before{content:"\0025cb   "}.lst-kix_ykbty9k091s5-2>li:before{content:"\0025a0   "}.lst-kix_z8dy5ehg0j6m-0>li:before{content:"\0025cf   "}.lst-kix_v8i0z8oris9z-6>li:before{content:"\0025cf   "}.lst-kix_v8i0z8oris9z-8>li:before{content:"\0025a0   "}.lst-kix_tdg51sdobgli-5>li:before{content:"\0025a0   "}.lst-kix_tdg51sdobgli-7>li:before{content:"\0025cb   "}.lst-kix_qzcbcbds5p8j-1>li:before{content:"\0025cf   "}.lst-kix_5jkev5cbxpxc-4>li:before{content:"\0025cb   "}.lst-kix_z8dy5ehg0j6m-8>li:before{content:"\0025a0   "}.lst-kix_g1ro4rg087e5-2>li:before{content:"\0025a0   "}.lst-kix_kz0xo3ln4wpk-3>li:before{content:"\0025cf   "}.lst-kix_3n0bx46jp1u4-5>li:before{content:"\0025a0   "}.lst-kix_5jkev5cbxpxc-6>li:before{content:"\0025cf   "}.lst-kix_pxlscdvf5lia-5>li:before{content:"\0025a0   "}.lst-kix_i6q6s2czm8np-4>li:before{content:"\0025cb   "}.lst-kix_kz0xo3ln4wpk-1>li:before{content:"\0025cb   "}.lst-kix_z8dy5ehg0j6m-6>li:before{content:"\0025cf   "}.lst-kix_i6q6s2czm8np-6>li:before{content:"\0025cf   "}.lst-kix_3n0bx46jp1u4-7>li:before{content:"\0025cb   "}.lst-kix_nvbe1nnkagj0-0>li:before{content:"\0025cf   "}.lst-kix_1c2feo7jow6-5>li:before{content:"\0025a0   "}ul.lst-kix_dllt5p3q6235-5{list-style-type:none}ul.lst-kix_dllt5p3q6235-4{list-style-type:none}ul.lst-kix_dllt5p3q6235-3{list-style-type:none}.lst-kix_n1ax58vdrowu-8>li:before{content:"\0025a0   "}ul.lst-kix_dllt5p3q6235-2{list-style-type:none}.lst-kix_q24skcnjivy0-8>li:before{content:"\0025a0   "}ul.lst-kix_dllt5p3q6235-8{list-style-type:none}.lst-kix_nvbe1nnkagj0-3>li:before{content:"\0025cf   "}.lst-kix_5snmm03p8xq6-8>li:before{content:"\0025a0   "}ul.lst-kix_dllt5p3q6235-7{list-style-type:none}ul.lst-kix_dllt5p3q6235-6{list-style-type:none}.lst-kix_1c2feo7jow6-2>li:before{content:"\0025a0   "}ul.lst-kix_tq2qp7s2zqek-1{list-style-type:none}ul.lst-kix_tq2qp7s2zqek-0{list-style-type:none}ul.lst-kix_dllt5p3q6235-1{list-style-type:none}.lst-kix_nfy673qapcua-7>li:before{content:"\0025cf   "}ul.lst-kix_dllt5p3q6235-0{list-style-type:none}.lst-kix_3oqam8vafvbn-1>li:before{content:"\0025cb   "}.lst-kix_n1ax58vdrowu-5>li:before{content:"\0025a0   "}ul.lst-kix_tq2qp7s2zqek-7{list-style-type:none}.lst-kix_5snmm03p8xq6-0>li:before{content:"\0025cf   "}ul.lst-kix_tq2qp7s2zqek-6{list-style-type:none}.lst-kix_bym5woclzz8f-6>li:before{content:"\0025cf   "}.lst-kix_bigsmldlbgzk-0>li:before{content:"\0025cf   "}ul.lst-kix_tq2qp7s2zqek-8{list-style-type:none}ul.lst-kix_tq2qp7s2zqek-3{list-style-type:none}ul.lst-kix_tq2qp7s2zqek-2{list-style-type:none}.lst-kix_q24skcnjivy0-0>li:before{content:"\0025cf   "}ul.lst-kix_tq2qp7s2zqek-5{list-style-type:none}.lst-kix_ee0rw9sgmv1o-7>li:before{content:"\0025cb   "}ul.lst-kix_tq2qp7s2zqek-4{list-style-type:none}.lst-kix_nvbe1nnkagj0-8>li:before{content:"\0025cf   "}.lst-kix_bigsmldlbgzk-3>li:before{content:"\0025cf   "}.lst-kix_5snmm03p8xq6-3>li:before{content:"\0025cf   "}.lst-kix_jxs8idtrfx9u-3>li:before{content:"\0025cf   "}.lst-kix_nfy673qapcua-2>li:before{content:"\0025cf   "}.lst-kix_qjs84hdj3als-0>li:before{content:"\0025cf   "}.lst-kix_5qkugsusxliw-2>li:before{content:"\0025a0   "}.lst-kix_q24skcnjivy0-5>li:before{content:"\0025a0   "}.lst-kix_5qkugsusxliw-7>li:before{content:"\0025cb   "}.lst-kix_bigsmldlbgzk-8>li:before{content:"\0025a0   "}.lst-kix_3oqam8vafvbn-6>li:before{content:"\0025cf   "}.lst-kix_ee0rw9sgmv1o-4>li:before{content:"\0025cb   "}.lst-kix_jxs8idtrfx9u-6>li:before{content:"\0025cf   "}.lst-kix_15risnu9xchg-7>li:before{content:"\0025cb   "}.lst-kix_8pvyoajrhbtr-1>li:before{content:"\0025cb   "}.lst-kix_15risnu9xchg-4>li:before{content:"\0025cb   "}.lst-kix_ysk5buayro0y-7>li:before{content:"\0025cb   "}.lst-kix_stgkp8ts1530-3>li:before{content:"\0025cf   "}.lst-kix_ysk5buayro0y-4>li:before{content:"\0025cb   "}.lst-kix_stgkp8ts1530-8>li:before{content:"\0025a0   "}.lst-kix_5gev2o922b7x-7>li:before{content:"\0025cb   "}.lst-kix_8pvyoajrhbtr-6>li:before{content:"\0025cf   "}.lst-kix_gaab8il5wtid-3>li:before{content:"\0025cf   "}.lst-kix_tq2qp7s2zqek-6>li:before{content:"\0025cf   "}ul.lst-kix_s8kajunwpmir-5{list-style-type:none}ul.lst-kix_s8kajunwpmir-4{list-style-type:none}ul.lst-kix_s8kajunwpmir-7{list-style-type:none}.lst-kix_42ndt3nzggvw-1>li:before{content:"\0025cb   "}ul.lst-kix_s8kajunwpmir-6{list-style-type:none}.lst-kix_n1ax58vdrowu-0>li:before{content:"\0025cf   "}ul.lst-kix_s8kajunwpmir-8{list-style-type:none}.lst-kix_ascolikyamcy-3>li:before{content:"\0025cf   "}.lst-kix_42ndt3nzggvw-4>li:before{content:"\0025cb   "}.lst-kix_tq2qp7s2zqek-1>li:before{content:"\0025cb   "}.lst-kix_ascolikyamcy-0>li:before{content:"\0027a2   "}.lst-kix_xnct1xfph69a-5>li:before{content:"\0025a0   "}ul.lst-kix_s8kajunwpmir-1{list-style-type:none}ul.lst-kix_s8kajunwpmir-0{list-style-type:none}.lst-kix_slf0pg4a0ih7-0>li:before{content:"\0027a2   "}ul.lst-kix_s8kajunwpmir-3{list-style-type:none}ul.lst-kix_s8kajunwpmir-2{list-style-type:none}.lst-kix_x7gq6rngbuia-7>li:before{content:"\0025cb   "}.lst-kix_gaab8il5wtid-0>li:before{content:"\0027a2   "}.lst-kix_5gev2o922b7x-2>li:before{content:"\0025a0   "}ul.lst-kix_bf3umtcrzzqw-2{list-style-type:none}ul.lst-kix_bf3umtcrzzqw-1{list-style-type:none}ul.lst-kix_bf3umtcrzzqw-0{list-style-type:none}ul.lst-kix_bf3umtcrzzqw-6{list-style-type:none}.lst-kix_hswfzmci3dc-0>li:before{content:"\0025cf   "}.lst-kix_4chf7gspsjev-3>li:before{content:"\0025cf   "}ul.lst-kix_bf3umtcrzzqw-5{list-style-type:none}.lst-kix_64f0218k1822-6>li:before{content:"\0025cf   "}ul.lst-kix_bf3umtcrzzqw-4{list-style-type:none}ul.lst-kix_bf3umtcrzzqw-3{list-style-type:none}.lst-kix_eqih6is7mr7q-7>li:before{content:"\0025cb   "}ul.lst-kix_bf3umtcrzzqw-8{list-style-type:none}ul.lst-kix_bf3umtcrzzqw-7{list-style-type:none}.lst-kix_dmf65t97trau-0>li:before{content:"\0025cf   "}.lst-kix_dmf65t97trau-8>li:before{content:"\0025a0   "}.lst-kix_kz0xo3ln4wpk-6>li:before{content:"\0025cf   "}.lst-kix_slf0pg4a0ih7-3>li:before{content:"\0025cf   "}.lst-kix_hg8waqqzu11r-6>li:before{content:"\0025cf   "}.lst-kix_ohkm6rcowih5-6>li:before{content:"\0025cf   "}.lst-kix_tdg51sdobgli-2>li:before{content:"\0025a0   "}.lst-kix_xnct1xfph69a-2>li:before{content:"\0025a0   "}ul.lst-kix_gaab8il5wtid-2{list-style-type:none}ul.lst-kix_gaab8il5wtid-3{list-style-type:none}ul.lst-kix_gaab8il5wtid-0{list-style-type:none}ul.lst-kix_gaab8il5wtid-1{list-style-type:none}.lst-kix_80fk82fj1hcs-8>li:before{content:"\0025a0   "}.lst-kix_w7uhve9aueba-7>li:before{content:"\0025cb   "}.lst-kix_9l57ldnvor4v-6>li:before{content:"\0025cf   "}.lst-kix_bf3umtcrzzqw-1>li:before{content:"\0025cb   "}.lst-kix_ssxyep2erusf-7>li:before{content:"\0025cb   "}.lst-kix_dllt5p3q6235-3>li:before{content:"\0025cf   "}.lst-kix_otug9mhs4rmx-6>li:before{content:"\0025cf   "}.lst-kix_80fk82fj1hcs-0>li:before{content:"\0027a2   "}ul.lst-kix_54ck8ch4bn7o-8{list-style-type:none}ul.lst-kix_54ck8ch4bn7o-7{list-style-type:none}.lst-kix_s8kajunwpmir-3>li:before{content:"\0025cf   "}.lst-kix_yorg1uctntus-2>li:before{content:"\0025a0   "}.lst-kix_hswfzmci3dc-8>li:before{content:"\0025a0   "}ul.lst-kix_54ck8ch4bn7o-2{list-style-type:none}ul.lst-kix_54ck8ch4bn7o-1{list-style-type:none}ul.lst-kix_gaab8il5wtid-8{list-style-type:none}ul.lst-kix_54ck8ch4bn7o-0{list-style-type:none}ul.lst-kix_gaab8il5wtid-6{list-style-type:none}ul.lst-kix_54ck8ch4bn7o-6{list-style-type:none}.lst-kix_mdf3w72jkmgx-5>li:before{content:"\0025a0   "}ul.lst-kix_gaab8il5wtid-7{list-style-type:none}ul.lst-kix_54ck8ch4bn7o-5{list-style-type:none}ul.lst-kix_gaab8il5wtid-4{list-style-type:none}ul.lst-kix_54ck8ch4bn7o-4{list-style-type:none}ul.lst-kix_gaab8il5wtid-5{list-style-type:none}ul.lst-kix_54ck8ch4bn7o-3{list-style-type:none}.lst-kix_ykbty9k091s5-7>li:before{content:"\0025cb   "}.lst-kix_stgkp8ts1530-0>li:before{content:"\0025cf   "}.lst-kix_ncyelpl3aut1-4>li:before{content:"\0025cb   "}.lst-kix_8b09c5wbiylz-2>li:before{content:"\0025cf   "}.lst-kix_54ck8ch4bn7o-8>li:before{content:"\0025a0   "}.lst-kix_3n0bx46jp1u4-2>li:before{content:"\0025a0   "}.lst-kix_qzcbcbds5p8j-4>li:before{content:"\0025cf   "}.lst-kix_v8i0z8oris9z-3>li:before{content:"\0025cf   "}.lst-kix_g1ro4rg087e5-7>li:before{content:"\0025cb   "}.lst-kix_i83pglsmrtnv-3>li:before{content:"\0025cf   "}.lst-kix_qjs84hdj3als-3>li:before{content:"\0025cf   "}.lst-kix_i6q6s2czm8np-1>li:before{content:"\0025cb   "}.lst-kix_bb18g463362n-7>li:before{content:"\0025cb   "}.lst-kix_5jkev5cbxpxc-1>li:before{content:"\0025cb   "}ul.lst-kix_q24skcnjivy0-0{list-style-type:none}ul.lst-kix_q24skcnjivy0-2{list-style-type:none}ul.lst-kix_q24skcnjivy0-1{list-style-type:none}ul.lst-kix_q24skcnjivy0-4{list-style-type:none}.lst-kix_z8dy5ehg0j6m-3>li:before{content:"\0025cf   "}ul.lst-kix_q24skcnjivy0-3{list-style-type:none}ul.lst-kix_q24skcnjivy0-6{list-style-type:none}ul.lst-kix_q24skcnjivy0-5{list-style-type:none}ul.lst-kix_q24skcnjivy0-8{list-style-type:none}ul.lst-kix_q24skcnjivy0-7{list-style-type:none}.lst-kix_pxlscdvf5lia-2>li:before{content:"\0025a0   "}ul.lst-kix_ysk5buayro0y-8{list-style-type:none}.lst-kix_cvtr764jy7v0-6>li:before{content:"\0025cf   "}.lst-kix_wa5eplpvj99e-1>li:before{content:"\0025cb   "}ul.lst-kix_ysk5buayro0y-6{list-style-type:none}.lst-kix_8pfaa7u0e2zg-7>li:before{content:"\0025cb   "}ul.lst-kix_ysk5buayro0y-7{list-style-type:none}.lst-kix_nz0skx7ahmk9-4>li:before{content:"\0025cb   "}ul.lst-kix_ysk5buayro0y-4{list-style-type:none}ul.lst-kix_ysk5buayro0y-5{list-style-type:none}ul.lst-kix_ysk5buayro0y-2{list-style-type:none}ul.lst-kix_ysk5buayro0y-3{list-style-type:none}ul.lst-kix_ysk5buayro0y-0{list-style-type:none}ul.lst-kix_ysk5buayro0y-1{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c26{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#efefef;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#000000;border-bottom-style:solid}.c24{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#efefef;border-left-style:solid;border-bottom-width:1pt;width:393pt;border-top-color:#000000;border-bottom-style:solid}.c61{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:251.2pt;border-top-color:#000000;border-bottom-style:solid}.c90{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:203.2pt;border-top-color:#000000;border-bottom-style:solid}.c83{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:98.2pt;border-top-color:#000000;border-bottom-style:solid}.c34{-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Arial";font-style:normal}.c6{background-color:#ffff00;color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:italic}.c43{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c12{color:#e69138;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:italic}.c18{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center;height:11pt}.c29{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c21{padding-top:12pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c20{color:#674ea7;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c15{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c19{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c0{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c59{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c27{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:italic}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c16{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c25{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center;height:11pt}.c48{padding-top:0pt;padding-bottom:16pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c51{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c28{background-color:#00ff00;color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c64{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c98{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c40{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c31{margin-left:54pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c35{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c38{font-weight:400;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c46{padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c17{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c1{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c42{-webkit-text-decoration-skip:none;color:#e69138;text-decoration:underline;text-decoration-skip-ink:none}.c86{margin-left:-38.2pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c63{font-weight:400;vertical-align:baseline;font-size:13pt;font-family:"Arial"}.c30{text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c23{border-spacing:0;border-collapse:collapse;margin-right:auto}.c74{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c10{color:#674ea7;font-size:14pt;font-weight:700}.c94{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c78{vertical-align:baseline;font-family:"Arial";font-style:normal}.c9{color:#38761d;font-size:14pt;font-weight:700}.c32{color:#1c4587;font-size:22pt;font-weight:700}.c45{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.c102{-webkit-text-decoration-skip:none;text-decoration:line-through;text-decoration-skip-ink:none}.c65{color:#000000;font-weight:400;font-size:10pt}.c39{color:#9900ff;font-size:14pt}.c2{padding:0;margin:0}.c22{font-weight:700;font-style:italic}.c49{color:#1c4587;font-weight:400}.c54{color:#000000;font-size:40pt}.c8{color:inherit;text-decoration:inherit}.c13{margin-left:72pt;padding-left:0pt}.c56{color:#ff0000;font-size:9pt}.c67{color:#0000ff;font-size:16pt}.c99{width:33%;height:1px}.c85{orphans:2;widows:2}.c5{margin-left:36pt;padding-left:0pt}.c73{font-weight:400;font-size:15pt}.c44{color:#1967d2;font-size:14pt}.c11{font-size:13pt;font-weight:700}.c87{color:#ff0000}.c72{font-size:11pt}.c37{font-weight:700}.c82{font-weight:400}.c93{height:20pt}.c58{font-size:12pt}.c80{font-size:25pt}.c70{color:#1155cc}.c97{color:#999999}.c77{font-size:30pt}.c69{background-color:#ff9900}.c57{background-color:#00ffff}.c50{color:#a64d79}.c96{font-size:40pt}.c53{color:#1c4587}.c81{color:#38761d}.c100{font-size:15pt}.c66{background-color:#00ff00}.c60{font-size:20pt}.c101{color:#434343}.c75{color:#000000}.c47{font-size:14pt}.c41{font-size:18pt}.c52{color:#9900ff}.c33{height:11pt}.c7{height:0pt}.c76{font-size:9pt}.c84{color:#e69138}.c92{color:#1967d2}.c88{height:14pt}.c55{font-style:italic}.c89{color:#674ea7}.c71{font-size:16pt}.c36{color:#666666}.c95{color:#0000ff}.c79{font-size:19pt}.c68{background-color:#ff00ff}.c62{text-decoration:none}.c91{font-size:10pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c94 doc-content"><div><p class="c51 c33"><span class="c4"></span></p></div><p class="c3 c33"><span class="c30 c37 c54"></span></p><p class="c64"><span class="c30 c54 c37">The Generative AI </span></p><p class="c64"><span class="c30 c54 c37">Ethics </span></p><p class="c64"><span class="c30 c54 c37">Playbook</span></p><p class="c18"><span class="c30 c37 c80 c75"></span></p><p class="c18"><span class="c30 c37 c75 c80"></span></p><p class="c18"><span class="c30 c37 c80 c75"></span></p><p class="c64"><span class="c30 c37 c75 c47">Jessie J. Smith, Wesley Hanwen Deng, </span></p><p class="c64"><span class="c30 c37 c75 c47">William H. Smith, Maarten Sap, </span></p><p class="c64"><span class="c30 c37 c47 c75">Nicole DeCario, Jesse Dodge</span></p><p class="c18"><span class="c30 c37 c80 c75"></span></p><p class="c18"><span class="c30 c37 c80 c75"></span></p><p class="c3 c33"><span class="c30 c37 c80 c75"></span></p><p class="c25"><span class="c30 c37 c80 c75"></span></p><p class="c18"><span class="c30 c37 c80 c87"></span></p><hr style="page-break-before:always;display:none;"><p class="c18"><span class="c30 c37 c80 c97"></span></p><p class="c3"><span class="c30 c37 c75 c79">Table of Contents</span></p><p class="c3 c33"><span class="c30 c37 c75 c79"></span></p><p class="c46"><span class="c78 c45 c72 c37 c36"><a class="c8" href="#h.thgr8cxkbvsl">Introduction&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3</a></span></p><p class="c40"><span class="c38 c45 c36"><a class="c8" href="#h.xwe7xyuqq9ei">Navigating Through The Playbook&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5</a></span></p><p class="c40"><span class="c38 c45 c36"><a class="c8" href="#h.1u9prylf3ptq">Risks &amp; Harms&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5</a></span></p><p class="c46"><span class="c1 c78 c72 c37"><a class="c8" href="#h.k77hicxwqq1z">Problem Formulation&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8</a></span></p><p class="c40"><span class="c38 c1"><a class="c8" href="#h.i8n0ligmzfm6">Transparency &amp; Documentation Checklist&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9</a></span></p><p class="c31"><span class="c38 c1"><a class="c8" href="#h.nxeh0qiicod2">Identifying Problematic Tasks&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10</a></span></p><p class="c31"><span class="c38 c1"><a class="c8" href="#h.k1ac9zdugcik">Intended Use&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;14</a></span></p><p class="c31"><span class="c38 c1"><a class="c8" href="#h.zcqafm5nageb">Auditing Research Questions&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16</a></span></p><p class="c46"><span class="c78 c45 c72 c37 c52"><a class="c8" href="#h.s4qvt2eeu8b6">Dataset&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;18</a></span></p><p class="c40"><span class="c38 c45 c52"><a class="c8" href="#h.63t55ltdi1rn">Transparency &amp; Documentation Checklist&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;19</a></span></p><p class="c31"><span class="c38 c45 c52"><a class="c8" href="#h.i7b3lav4w7b2">Bias &amp; Diversity&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;20</a></span></p><p class="c31"><span class="c38 c45 c52"><a class="c8" href="#h.og35q642jfc6">Exclusion Criteria&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;23</a></span></p><p class="c31"><span class="c38 c45 c52"><a class="c8" href="#h.6bh4thjpmg98">Data Quality&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;26</a></span></p><p class="c31"><span class="c38 c45 c52"><a class="c8" href="#h.suf1io3w85su">Data Collection&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;28</a></span></p><p class="c46"><span class="c78 c45 c72 c37 c81"><a class="c8" href="#h.4rkoug7h5qeh">Model Design&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;31</a></span></p><p class="c40"><span class="c38 c45 c81"><a class="c8" href="#h.h7r6zpciseg6">Transparency &amp; Documentation Checklist&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;32</a></span></p><p class="c31"><span class="c38 c45 c81"><a class="c8" href="#h.d5pmk1b9mln7">Model Design Bias &amp; Diversity&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;33</a></span></p><p class="c46"><span class="c42 c78 c72 c37"><a class="c8" href="#h.ygi6jm11l2da">Model Training&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;36</a></span></p><p class="c40"><span class="c38 c42"><a class="c8" href="#h.idcrsqibujv9">Transparency &amp; Documentation Checklist&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;37</a></span></p><p class="c31"><span class="c38 c42"><a class="c8" href="#h.y70kr9wh3fi7">Environmental Impact&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;38</a></span></p><p class="c31"><span class="c38 c42"><a class="c8" href="#h.5dqnewax72r0">Evaluation During Training&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;40</a></span></p><p class="c31"><span class="c38 c42"><a class="c8" href="#h.yigvcaogz85o">Biases From Objective Function&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;42</a></span></p><p class="c46"><span class="c78 c45 c72 c37 c50"><a class="c8" href="#h.t5fhdsqwi37v">Model Evaluation&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;44</a></span></p><p class="c40"><span class="c38 c45 c50"><a class="c8" href="#h.53x9d0xwn97m">Transparency &amp; Documentation Checklist&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;45</a></span></p><p class="c31"><span class="c38 c45 c50"><a class="c8" href="#h.ug7hppc74ogp">Biases From Evaluation Choices&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;46</a></span></p><p class="c31"><span class="c38 c45 c50"><a class="c8" href="#h.b0vgzb6xgpym">Measuring Bias&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;48</a></span></p><p class="c31"><span class="c38 c45 c50"><a class="c8" href="#h.pk6e20monrx6">Evaluating Problematic Outputs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;52</a></span></p><p class="c31"><span class="c38 c45 c50"><a class="c8" href="#h.e8r84e5mhv5x">Measuring Societal Harm&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;56</a></span></p><p class="c46"><span class="c78 c45 c72 c37 c89"><a class="c8" href="#h.77xzxk80tguh">Model Use &amp; Monitoring&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;59</a></span></p><p class="c40"><span class="c38 c45 c89"><a class="c8" href="#h.djgt6qygpmml">Transparency &amp; Documentation Checklist&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;60</a></span></p><p class="c31"><span class="c38 c45 c89"><a class="c8" href="#h.ntis6v3astby">Refusals and safeguards&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;61</a></span></p><p class="c31"><span class="c38 c45 c89"><a class="c8" href="#h.a01wsow1ftqv">Harms from Use&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;64</a></span></p><p class="c31"><span class="c38 c45 c89"><a class="c8" href="#h.gua72c6fk6ns">Appeals &amp; Recourse to Humans&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;66</a></span></p><h1 class="c29" id="h.thgr8cxkbvsl"><span class="c32">Introduction</span></h1><p class="c3"><span>Welcome to the Generative AI Ethics Playbook. In this playbook, you will find guidance for improving the ethics of your machine learning systems in the domains of computer vision, language modeling, and generative AI broadly. The goal of this playbook is to help you diagnose potential harms that could arise within your </span><span>design, development, and use of datasets and models in AI</span><span>, while providing concrete guidance and resources for mitigation strategies to reduce the negative impact of those potential harms. </span><span>To the best of our knowledge, this playbook provides a mix of current best research practices and ethics practices.</span></p><h4 class="c16" id="h.82bva3t1lwvl"><span class="c30 c37 c58 c36">Intended Audience / Users</span></h4><p class="c3"><span>This playbook is designed for AI/ML practitioners who are building or using technologies in the domains of computer vision, generative AI, and/or language modeling. At a high-level, this includes </span><span>generative</span><span class="c4">, multimodal machine learning models such as:</span></p><ul class="c2 lst-kix_3t7g0caoicba-0 start"><li class="c3 c5 li-bullet-0"><span>Text-to-text </span><span>models</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Image-to-image models</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Image-to-text models</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Text-to-image models</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Text-to-video models</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Video-to-video models</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c37">AI/ML practitioners include ML researchers, ML practitioners, ML engineers, people who work with ML products or internal ML tools, people who do exploratory ML work, and people who design/develop/help deploy ML models in academia or industry settings. </span><span>We suggest that ethics be incorporated from the beginning of a project, and one method to do this is to explicitly select a set of people to critically examine the decisions made at every stage. This set of people can be the &ldquo;responsible party&rdquo; for ethics, and can navigate through the relevant sections of this playbook for your teams&rsquo; work.</span></p><h4 class="c16" id="h.h82wzo6ow8mc"><span class="c30 c37 c58 c36">What is a playbook?</span></h4><p class="c3"><span class="c4">We define &ldquo;Playbook&rdquo; as a set of guidelines, case-studies, and references that can be utilized to help you diagnose potential ethical concerns that can arise in your ML/AI system design, research, datasets, models, and/or uses.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c4">The premise of this playbook is that AI practitioners have considerable&mdash;although of course not perfect&mdash;agency to influence the social impacts of their research and design. We encourage you to embrace that agency by considering the impacts of your implicit and explicit design decisions. Throughout the playbook we provide specific guidance to help you know which decisions might be the most appropriate to reduce harm for your AI/ML system.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span>As you use this playbook, we encourage you to document </span><span class="c37">why </span><span>you are making the decisions you make, </span><span class="c37">why </span><span>you did </span><span class="c55">not</span><span>&nbsp;make certain decisions, and what the resulting trade offs might be. &nbsp;This documentation </span><span>will come in handy</span><span class="c4">&nbsp;for communicating your work to people who might make use of or might be impacted by your research or products. Details for how to document your decisions are provided throughout the playbook.</span></p><h4 class="c16" id="h.vf6jxdbcg6po"><span class="c30 c37 c58 c36">Why would you want to use this playbook?</span></h4><p class="c3"><span class="c22 c35">This playbook could potentially help you:</span></p><ul class="c2 lst-kix_otug9mhs4rmx-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">Surface any ethical implications of your AI research or AI products.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Consider the potential negative societal impacts of your work.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Learn which mitigation strategies might be most appropriate to adopt to improve ethical concerns related to your AI research and/or development.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Strengthen your research design and methods.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Prepare an impact statement to include in your publications, as is increasingly suggested or required by publication venues like conferences, journals, etc.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Prepare an impact statement to share with product teams.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Improve documentation and transparency in your decision-making process.</span></li></ul><h4 class="c16" id="h.ctpb734j5dcs"><span class="c37 c58">What is ethics?</span><span class="c30 c37 c58 c36">&nbsp;</span></h4><p class="c3"><span>We recognize that ethics is a highly subjective topic, and designed this playbook with this in mind. In short, ethics can be thought of as &ldquo;doing the right thing,&rdquo; &ldquo;mitigating harm,&rdquo; or &ldquo;treating people with justice and equality.&rdquo; This playbook is designed under the premise of </span><span class="c37">value pluralism</span><span>, which posits that different people have different values, and will come to different conclusions about whether or not something is right or wrong. This is a useful reminder to turn back to when using this playbook, as certain benefits or harms of your AI might arise&mdash;we will not be providing guidance about what is right or wrong, or better or worse. Instead, we will be providing you with the tools and resources you might need to make ethical choices, but we aren&#39;t going to make those choices for you. Instead, this playbook is intended to </span><span class="c55">encourage</span><span class="c4">&nbsp;reflection, documentation, transparency, and more ethical and informed decision-making.</span></p><h4 class="c16" id="h.xdzsa76cn1j1"><span class="c30 c37 c58 c36">AI Ethics Principles Statement</span></h4><p class="c3"><span>We, the research team who has developed this playbook, hold several basic principles about AI research and its impact (adapted from </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://ai.google/responsibility/principles/&amp;sa=D&amp;source=editors&amp;ust=1734479425838246&amp;usg=AOvVaw37ln8wlh_XVqSG_sYERENM">Google&rsquo;s Objectives for AI applications</a></span><span class="c4">).</span></p><p class="c3"><span class="c19">We maintain that AI design, development, and research ought to:</span></p><ul class="c2 lst-kix_42ndt3nzggvw-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">Be socially beneficial.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Identify and reduce harm whenever possible.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Avoid creating or reinforcing unfair bias.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Be built and tested for safety.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Be accountable to people through transparent documentation and recourse.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Uphold high standards of scientific excellence.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c4">The advice and mitigation strategies given in this playbook are intended to showcase how to apply these principles in practice.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span>We hope you find this playbook useful! Should you have any questions, feel free to reach out to the research lead via email at Jessie.Smith-1@colorado.edu.</span><hr style="page-break-before:always;display:none;"></p><h2 class="c59" id="h.xwe7xyuqq9ei"><span class="c30 c37 c53 c71">Navigating Through The Playbook</span></h2><p class="c3"><span>To navigate through this playbook, we suggest that you focus on the stage(s) of the AI lifecycle that are most relevant to your current project</span><span>&nbsp;status</span><span class="c4">. Though we introduce these stages in a particular order, we also note that these stages are iterative and cyclical, and often feed into one another throughout the AI lifecycle. The stages of the AI lifecycle are:</span></p><ul class="c2 lst-kix_54ck8ch4bn7o-0 start"><li class="c3 c5 li-bullet-0"><span class="c1 c37"><a class="c8" href="#h.k77hicxwqq1z">Problem Formulation</a></span><span class="c4">: The earliest stages of AI development, very few concrete design decisions have been made at this point.</span></li><li class="c3 c5 li-bullet-0"><span class="c1 c37"><a class="c8" href="#h.s4qvt2eeu8b6">Dataset</a></span><span class="c4">: Collecting, curating, cleaning, annotating, and/or using datasets.</span></li><li class="c3 c5 li-bullet-0"><span class="c1 c37"><a class="c8" href="#h.4rkoug7h5qeh">Model Design</a></span><span class="c4">: Technical decisions for your model, refining the objectives.</span></li><li class="c3 c5 li-bullet-0"><span class="c1 c37"><a class="c8" href="#h.ygi6jm11l2da">Model Training</a></span><span>: The process of prompting, </span><span>training, and </span><span>finetuning</span><span class="c4">&nbsp;the model with data.</span></li><li class="c3 c5 li-bullet-0"><span class="c1 c37"><a class="c8" href="#h.t5fhdsqwi37v">Model Evaluation</a></span><span class="c4">: Selecting and incorporating metrics to assess implications of model behavior.</span></li><li class="c3 c5 li-bullet-0"><span class="c1 c37"><a class="c8" href="#h.77xzxk80tguh">Model Use &amp; Monitoring</a></span><span>: Implementing safeguards and responding to risks that can arise from the model&rsquo;s interaction with users.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c4">In each chapter of this playbook (each stage of the AI lifecycle), you will be shown the most relevant ethics/harm topics of interest related to that stage. Within each &ldquo;Topic of Interest&rdquo;, there will be associated ethical considerations, examples and case-studies of harms, as well as mitigation strategies for you to browse. Each lifecycle stage has an associated &ldquo;Transparency and Documentation Checklist&rdquo; as well. The design and general flow of the playbook is shown in the following figure.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c64"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 156.00px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 156.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c18"><span class="c4"></span></p><p class="c3"><span>The </span><span class="c37">Common Considerations</span><span>&nbsp;that we provide in this playbook</span><span class="c4">&nbsp;are, based on our experience, the most common and pressing ethical questions to ask for a given topic. The goal of these questions is to surface harms that might emerge throughout the AI lifecycle. They are not complete and comprehensive; if you have a consideration or question that comes up that is not mentioned in this playbook, please let us know! Many considerations are context specific and might not yet have associated solutions or mitigation strategies. If your consideration does not have an associated mitigation strategy, we see this as a welcome invitation for future work.</span></p><h2 class="c59" id="h.1u9prylf3ptq"><span>Risks &amp; Harms</span></h2><p class="c3"><span>In this playbook, we primarily focus on the most common harms that can occur in computer vision, generative AI, and language models</span><span>, adapted from </span><span>(</span><span class="c70"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2210.05791.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425843796&amp;usg=AOvVaw3qCz-tB0BfZYuqMKIeNZ19">1</a></span><span>, </span><span class="c70"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/pdf/10.1145/3531146.3533088?trk%3Dpublic_post_comment-text&amp;sa=D&amp;source=editors&amp;ust=1734479425844797&amp;usg=AOvVaw3XnxLp5MHA1lNTooV99qcx">2</a></span><span>)</span><span>. Below is a list of each of these types of harms and their associated sub-types, to help guide you as they are referenced throughout the examples brought up in this playbook. If you would like to search for examples of harms based on these harm-types, you can also utilize the associated hashtags and colors related to each harm-type below.</span></p><h4 class="c16" id="h.n1wjjq7mn3l9"><span class="c30 c37 c58 c36">Representational Harms</span></h4><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c4">Assumptions and beliefs about social groups that can reproduce unjust societal hierarchies. This can lead to inequality of algorithmic experience and visibility.</span></p><ul class="c2 lst-kix_g1ro4rg087e5-0 start"><li class="c3 c5 li-bullet-0"><span class="c37">Stereotyping social groups</span><span class="c4">&nbsp;&rarr; the system&rsquo;s outputs reflect beliefs about characteristics, attributes, and behaviors of certain groups of people.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Demeaning social groups </span><span class="c4">&rarr; the system&rsquo;s outputs demean, marginalize, or oppress certain groups of people. This can also include outputting hate speech or offensive language.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Erasing social groups </span><span class="c4">&rarr; the system fails to recognize people or attributes that belong to specific groups. This is a more extreme form of stereotyping, capturing the extremes of under or over-representation.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Denying people the opportunity to self-identify </span><span class="c4">&rarr; the system classifies or represents humans automatically and does not allow autonomy for these classifications to be corrected.</span></li></ul><h4 class="c16" id="h.z7bzm52px9fd"><span class="c30 c37 c58 c36">Allocative Harms</span></h4><p class="c3"><span class="c22 c68">#AllocativeHarm</span></p><p class="c3"><span class="c4">Problems that arise from unequal distribution of algorithmic decisions/outputs for different groups of people. This can lead to disparate impact when benefits, information, or resources are systematically withheld from certain people.</span></p><ul class="c2 lst-kix_g1ro4rg087e5-0"><li class="c3 c5 li-bullet-0"><span class="c37">Opportunity loss &rarr; </span><span class="c4">the system enables disparate access to information or resources that are needed to equitably participate in society.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Economic loss</span><span>&nbsp;&rarr; when inequality in access to resources leads to negative economic implications.</span></li></ul><h4 class="c16" id="h.hy1grsi4i7zz"><span class="c37">Quality of Service Harms</span></h4><p class="c3"><span class="c22 c57">#QualityOfServiceHarm</span></p><p class="c3"><span class="c4">When a system underperforms for certain groups of people based on their social characteristics such as ethnicity or gender identity.</span></p><ul class="c2 lst-kix_g1ro4rg087e5-0"><li class="c3 c5 li-bullet-0"><span class="c37">Alienation</span><span class="c4">&nbsp;&rarr; model fails to acknowledge someone&rsquo;s identity characteristics. This can lead to annoyance, disappointment, frustration, or anger.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Increased labor</span><span class="c4">&nbsp;&rarr; certain social groups have to put in extra efforts to make the model work for them as well as it does for others.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Service or benefit loss</span><span>&nbsp;&rarr; when the benefit of using the model is diminished or lost for certain social groups because it performs worse for them based on their identity.</span></li></ul><h4 class="c16" id="h.9u4fwy7t0f6j"><span class="c30 c37 c58 c36">Interpersonal Harms</span></h4><p class="c3"><span class="c22 c66">#InterpersonalHarm</span></p><p class="c3"><span class="c4">Harm that arises when algorithmic systems negatively impact relations between people or communities.</span></p><ul class="c2 lst-kix_g1ro4rg087e5-0"><li class="c3 c5 li-bullet-0"><span class="c37">Loss of agency/social control</span><span class="c4">&nbsp;&rarr; the use of a model harms someone&rsquo;s individual autonomy.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Technology-facilitated violence / malicious uses</span><span>&nbsp;&rarr; the violence caused by individuals using generated outputs to perform harassment or violence against others.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Diminished health and well-being</span><span class="c4">&nbsp;&rarr; when generated outputs by the model manipulate users&rsquo; emotions or exploit their behavior. This can cause emotional harm and distress.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Privacy violations </span><span>&rarr; when generated outputs contain private information which is discovered and used by an end-user.</span></li></ul><h4 class="c16" id="h.x0qb83149ehe"><span class="c30 c37 c36 c58">Societal Harms</span></h4><p class="c3"><span class="c22 c69">#SocietalHarm</span></p><p class="c3"><span class="c4">When a system leads to indirect or downstream harms, or amplifies pre existing systematic inequalities.</span></p><ul class="c2 lst-kix_g1ro4rg087e5-0"><li class="c3 c5 li-bullet-0"><span class="c37">Information harms</span><span class="c4">&nbsp;&rarr; when models create misinformation, disinformation, and malinformation, or when disinformation is cheaper and more effective as a result of a model/system.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Culture harms</span><span class="c4">&nbsp;&rarr; when models harm cultural communication, cultural property, and social values.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Political &amp; civil harms</span><span class="c4">&nbsp;&rarr; when models conflict with human rights or disproportionately target and harm people of color.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Macro socio-economic harms</span><span class="c4">&nbsp;&rarr; when models create increased imbalances in socio-economic relations at the societal level.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Environmental harms</span><span class="c4">&nbsp;&rarr; when the system&rsquo;s development, deployment, or use leads to adverse changes to the environment such as depletion or contamination of natural resources.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Downstream harms </span><span class="c4">&rarr; when unintended model use leads to unpredictable but real downstream harms as a result of model use in an inappropriate context.</span></li></ul><p class="c3 c33"><span class="c14"></span></p><p class="c3"><span>Though the instantiations and implications of these harms may differ depending on the context of the system and the stage of the AI lifecycle,</span><span>&nbsp;we find these risks and harms to be the most prolific and important to focus on</span><span>. Throughout this playbook, we tag each case-study with an associated harm category as defined above.</span></p><p class="c3 c33"><span class="c14"></span></p><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c30 c37 c70 c96">Stage #1</span></p><h1 class="c29" id="h.k77hicxwqq1z"><span class="c30 c82 c77 c92">Problem Formulation</span></h1><p class="c48 subtitle" id="h.cb38ws8googe"><span class="c41">Conceptualizing and Designing Tasks</span></p><p class="c3"><span>The problem formulation stage of the AI </span><span>lifecycle</span><span>&nbsp;includes decisions that are made </span><span class="c55">early </span><span class="c4">in the research process. At this stage, very little or no technical work has been done yet; the team is conceptualizing the task at hand, developing research questions, and/or determining the intended use of the system.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span>We note that this stage of the AI lifecycle might involve more </span><span>non-technical stakeholders</span><span>, such as business executives, product managers, legal and compliance officers, marketing and sales teams, or human resources personnel. Although transparency is important in every stage of the AI lifecycle, we especially encourage transparency and documentation about decisions made at this stage of AI research and development, as these early decisions will ultimately affect ethical impact in every subsequent stage of the </span><span>lifecycle</span><span>.</span></p><hr><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c30 c82 c75 c71">Topics of Interest</span></p><ul class="c2 lst-kix_w7uhve9aueba-0 start"><li class="c3 c5 li-bullet-0"><span class="c1 c37"><a class="c8" href="#h.nxeh0qiicod2">Identifying Problematic Tasks</a></span></li><li class="c3 c5 li-bullet-0"><span class="c1 c37"><a class="c8" href="#h.k1ac9zdugcik">Intended Use</a></span></li><li class="c3 c5 li-bullet-0"><span class="c1 c37"><a class="c8" href="#h.zcqafm5nageb">Auditing Research Questions</a></span></li></ul><hr><p class="c3 c33"><span class="c38 c53 c62"></span></p><hr style="page-break-before:always;display:none;"><h3 class="c43 c88" id="h.b2jk3fkghvsn"><span class="c30 c49 c47"></span></h3><h2 class="c59" id="h.i8n0ligmzfm6"><span class="c37 c95">Transparency &amp; Documentation Checklist</span></h2><p class="c48 subtitle" id="h.meyxxr34gxi7"><span class="c30 c73 c36">Problem Formulation</span></p><p class="c3"><span class="c4">In this stage of the AI lifecycle, we recommend that you discuss and document the following:</span></p><ul class="c2 lst-kix_nvbe1nnkagj0-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">Document who on the research team is responsible for using this ethics playbook to document and improve ethical impact of this technology</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Consider the history of problem-solving in this context, and whether data-driven approaches or automation has previously exacerbated unfairness or injustice.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Describe any/all benefits that could arise from your model / task</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Describe any/all risks or harms that could arise from your model / task</span></li><li class="c3 c5 li-bullet-0"><span class="c4">List all the nuance you may lose when translating your goals into a concrete machine learning task</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Explicitly articulate the intended use of your model. Come up with a clear task description and document this.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Document the explicit, specific unintended (and problematic, inappropriate, or harmful) uses of your model.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Specify whether intended use is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts).</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Specify the efforts to limit the potential use to circumstances in which the data/models could be used safely (such as an accompanying data/model statement).</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Create code/model release so the public can determine if there is unethical intended use or unintended use.</span></li></ul><hr style="page-break-before:always;display:none;"><h3 class="c43 c88" id="h.gol6e9tzcyoo"><span class="c30 c49 c47"></span></h3><h3 class="c43" id="h.nxeh0qiicod2"><span class="c37 c53 c60">Identifying </span><span class="c37 c53 c60">Problematic Tasks</span></h3><p class="c3"><span class="c78 c45 c37 c75 c47">Definitions / Relevant Terms</span></p><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c37">Formulating a Task: </span><span class="c4">A specification of problem(s) via a mapping of the input/output space of an ML model, and/or a specification of how the model is intended to be used within a particular domain.</span></p><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.g8tu41bjmemr"><span class="c44 c37">Common considerations</span></h4><ul class="c2 lst-kix_xnct1xfph69a-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">What is my motivation for using AI/ML, and do I have to use AI/ML for this task or problem? </span></li><li class="c3 c5 li-bullet-0"><span>Does my model have the potential to cause harm to people, regardless of if it fails or succeeds?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Should my model be used to augment humans&#39; ability to perform a task, or should my model be used to automate the task? If my model is being used to augment humans&rsquo; ability to perform a task, should my model require a human-in-the-loop intervention?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Is there a history of unfair, biased, or failed data collection, technical interventions or tools in this domain? If so, what does my project do differently?</span></li></ul><h4 class="c16" id="h.g7oemh762cww"><span class="c30 c37 c44">Examples of harms and implications</span></h4><h5 class="c21" id="h.p946q9zgazcl"><span class="c11">Example #1: Predicting Sexual Orientation from Face Photos</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm </span></p><p class="c3"><span class="c14">&rarr; Denying people the opportunity to self-identify</span></p><p class="c3"><span class="c28 c22">#InterpersonalHarm </span></p><p class="c3"><span class="c55">&rarr; Technology facilitated violence / malicious uses</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Researchers at Stanford trained a model to predict sexual orientation from a photo of someone&rsquo;s face (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://osf.io/preprints/psyarxiv/hv28a/&amp;sa=D&amp;source=editors&amp;ust=1734479425863755&amp;usg=AOvVaw1-1GAv9WxX7bBnJjZz15EX">source</a></span><span>).</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Sexual orientation cannot be revealed by measuring the size and shape of a person&#39;s eyes, nose, and face, and misclassification of sexual orientation can lead to harmful outcomes (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html&amp;sa=D&amp;source=editors&amp;ust=1734479425866261&amp;usg=AOvVaw1xkhzkF8MEwuvKDUblFU10">source</a></span><span class="c4">). For example, automatically labeling someone&rsquo;s sexuality without their consent can have life altering and sometimes life ending consequences (e.g., young LGBTQ+ folks often get kicked out of their homes, in many countries it&#39;s illegal to be queer, etc.)</span></p><p class="c3"><span class="c14">We note that the authors of this work described their motivation by saying that governments were already doing this, and thus they were exposing the fact that this may be possible. However, they did not show that governments were doing this, and instead might have unintentionally introduced a new task. Additionally, the authors failed to account for social science and gender &amp; sexuality literature before tackling this task; it is likely that they would have done things differently or even not done this if this literature had been engaged with, and critical questions about the nature of the task had been brought up early in the research process.</span></p></td></tr></table><h5 class="c21" id="h.x7clwmt7jud3"><span class="c11">Example #2: </span><span class="c11">Chatbots as Romantic Companions</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c28 c22">#InterpersonalHarm</span></p><p class="c3"><span class="c14">&rarr; Diminished health and well-being</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Since the launch of ChatGPT and the new resurgence of chatbots generally, there have been reports of people using chatbots as companions, and reports of some people dating their chatbots, and even falling in love with them (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://time.com/6257790/ai-chatbots-love/&amp;sa=D&amp;source=editors&amp;ust=1734479425871070&amp;usg=AOvVaw2J7uTsPE4I7TOVD0oWOg5p">source</a></span><span>).</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>These systems are often operationalized as keeping people in conversations and increasing their immediate sense of happiness. This can create social dependency, and has led to instances of people even falling in love with a chatbot, and being heartbroken because of chatbots (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.expressnews.com/business/article/XiaoIce-robot-users-have-ended-up-in-therapy-for-16414790.php&amp;sa=D&amp;source=editors&amp;ust=1734479425873486&amp;usg=AOvVaw0P9j3ySvE-QJIcAlnO8H8a">source</a></span><span class="c4">).</span></p></td></tr></table><h5 class="c21" id="h.o9jmg715vrh0"><span class="c11">Example #3: Automatic prison term prediction</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c69">#SocietalHarm</span></p><p class="c3"><span class="c14">&rarr; Political &amp; civil harms</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c55">Researchers developed a model to automate prison term predictions </span><span class="c45">(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://doi.org/10.18653/v1/D19-1667&amp;sa=D&amp;source=editors&amp;ust=1734479425877040&amp;usg=AOvVaw05KZqzD9nWPFiZEIPTXcTM">source</a></span><span class="c45">)</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This is an example of a task that had neutral intended use: the authors of this work were developing this task for science, but not for it to be deployed in real-world settings. Even in scenarios where there is no </span><span class="c55">intended </span><span>harm, but potential </span><span class="c55">unanticipated </span><span>harm that can still be foreseen, it is best practice to not pursue this task. Alternatively, even if there are potential </span><span class="c55">positive </span><span>benefits from the task (e.g., One could imagine the learned model would be very useful for identifying biases in prison sentences, in looking for favoritism in judges, etc), there is always potential for mission creep from modeling a phenomenon and using it for a decision support process. In other words, good intentions are a great start, but without critically acknowledging and weighing the potential pros and cons of implementing such a system, negative consequences can still result (e.g., if the model were used to inform the Supreme Court, rather than automate decision-making, what weight should judges give the system? And what biases has the model learned which could lead to inequities in sentencing? It is arguable that decisions regarding human freedom, and even potentially life and death, require greater consideration than that afforded by an algorithm, that is, that they should not be used at all (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2020.acl-main.261.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425879093&amp;usg=AOvVaw3JKpRqsb6P8IlMyT2ewavk">source</a></span><span>)).</span></p></td></tr></table><p class="c3 c33"><span class="c4"></span></p><h5 class="c21" id="h.hyptabrimltx"><span class="c11">Example #4: Text-to-Image Models</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c69">#SocietalHarm</span></p><p class="c3"><span class="c14">&rarr; Macro socio-economic harms</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c4">It has been shown that text-to-image models allow people to create their own images in place of an artist or a human creator. Although this might allow for greater efficiency for end-users, it also might undermine artists.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Creating text-to-image models might undermine creative economies and systematically hurt these groups by preventing them from generating income, which can lead to loss of financial opportunity </span><span>(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3531146.3533088&amp;sa=D&amp;source=editors&amp;ust=1734479425883857&amp;usg=AOvVaw0O3y0m3391AVre4NCbs_Fo">source</a></span><span>)</span><span>.</span></p></td></tr></table><h4 class="c16" id="h.rjqiboblndwy"><span class="c44 c37">Mitigation strategies</span></h4><p class="c3"><span>We note that this section of the playbook introduces many methods to identify the potential risks, harms, and unintended consequences of your technology. However, we do not provide explicit guidance for how to determine which consequences warrant new research design. We encourage you to critically evaluate the benefits and risks of your work with your team, to document every potential risk, and to determine when certain tasks might be too risky to pursue. </span><span>We also note that these questions are useful to discuss and document at the problem formulation stage, since the earlier one can foresee issues later in the pipeline, the better likelihood of reducing those issues. However, the mitigation strategies for these concerns might occur at later stages of the AI pipeline. We recommend that you document mitigations that will be necessary later on to keep track of this planned future work.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c45 c37">Uncovering the Potential Benefits / Harms of Your Technology</span></p><p class="c3"><span class="c4">Create a list of benefits and harms (as exhaustively as possible) that arise from your technology and consider whether it&#39;s worth proceeding. We recommend that you discuss the following questions to help you uncover the potential benefits and harms:</span></p><ul class="c2 lst-kix_woopwfqcy80n-0 start"><li class="c3 c5 li-bullet-0"><span>Is this a new task? Or is this an existing task?</span><span class="c4">&nbsp;If this is an existing task, what harms have surfaced in relevant past literature?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">How did you decide what you are going to use AI for? What are the implications of this?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">If there are harms and risks that arise from your approach, what are different ways could you operationalize the task given your broader goal at hand?</span></li><li class="c3 c5 li-bullet-0"><span>Could this model have p</span><span>otential malicious or unintended harmful effects and uses</span><span class="c4">&nbsp;(e.g., disinformation, generating fake profiles, surveillance)?</span></li><li class="c3 c5 li-bullet-0"><span>What is this model&rsquo;s environmental impact? Is there any way to reduce the environmental impact (e.g., by training and deploying smaller models)? Also refer to </span><span class="c1"><a class="c8" href="#h.y70kr9wh3fi7">Environmental Impact</a></span><span class="c4">&nbsp;for more strategies outlined later in this playbook.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What are the fairness considerations for this model? For example, will you be developing and/or deploying technologies that could further disadvantage or exclude historically disadvantaged groups?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What are the privacy considerations of this model or research (e.g., does this research attempt to conduct model/data stealing)?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Does this model or research have any security considerations worth noting and planning for (e.g., adversarial attacks)?</span></li><li class="c3 c5 li-bullet-0"><span>Does the research contribute to </span><span>overgeneralization, bias confirmation, under or overexposure</span><span>&nbsp;of specific languages, topics, or applications at the expense of others? For example, does the system work better for white American males than it does for women or citizens of Latino or Arabic descent? Or for this model context, would a false answer be worse than no answer? (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/P16-2096.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425888230&amp;usg=AOvVaw0u4GTsX0bvpiQ8aABEUiRz">Hovy and Spruit, 2016</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span>Consider different stakeholders that could be impacted by your work. Is it possible that research benefits some stakeholders while harming others? Does it pay special attention to vulnerable or marginalized communities? Does the research lead to exclusion of certain groups?</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c34">Methods of Accountability</span></p><ul class="c2 lst-kix_5snmm03p8xq6-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">Papers that accompany model releases should have ethics statements that provide structure for the program committee to assess the paper for ethical compliance.</span></li><li class="c3 c5 li-bullet-0"><span>Legal solutions like the </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://gdpr-info.eu/&amp;sa=D&amp;source=editors&amp;ust=1734479425889817&amp;usg=AOvVaw0hBLl-dxUmQeBkMmQUkz_w">General Data Protection Regulation</a></span><span class="c4">&nbsp;(EU GDPR) might offer guidance for best practices to mitigate potential harms.</span></li><li class="c3 c5 li-bullet-0"><span>Allow and encourage independent third party audits of the code and model (e.g., through code or model releases), so the public can determine if there is unethical primary use, secondary use, or unintended</span><span>&nbsp;use</span><span>.</span></li><li class="c3 c5 li-bullet-0"><span>We also note that there</span><span>&nbsp;are certain contexts where it is appropriate for models to </span><span class="c55">automate</span><span>&nbsp;human tasks, and certain contexts where it is more appropriate for models to </span><span class="c55">augment</span><span class="c4">&nbsp;human tasks. Human-in-the-loop machine learning is when machines are able to aid humans in their decision making processes, without replacing the human&rsquo;s ability to discern outputs from a model. In high-stakes scenarios, full automation without human intervention is at higher risk for causing harm, and human-in-the-loop methods may be more appropriate.</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c45 c37">What to Do if You Uncover Potential Harms / Risks</span></p><ul class="c2 lst-kix_5snmm03p8xq6-0"><li class="c3 c5 li-bullet-0"><span class="c4">Before deciding to continue or discontinue your models&rsquo; creation, consider participatory design and/or talking to users who are most likely to be negatively impacted by your technology before formulating or conceptualizing your task. If you don&#39;t have the resources to do this, we recommend you engage with relevant literature in the ML ethics/fairness discipline that focuses on your task or target audience.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Consider if your organization or campus has experts who might be helpful to consult with, whether researchers in humanities or social science domains who would understand historical precedents or data biases, ethics review boards, or other forms of technology ethics expertise.</span></li></ul><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c30 c72 c37 c95"></span></p><h3 class="c43" id="h.k1ac9zdugcik"><span class="c37 c53 c60">Intended Use</span></h3><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.1mafbsolahyl"><span class="c30 c44 c37">Common considerations</span></h4><ul class="c2 lst-kix_jxs8idtrfx9u-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">Are we creating a model that is going to be released to the public? Could this models&rsquo; intended use be misinterpreted by the public?</span></li><li class="c3 c5 li-bullet-0"><span>What are the limitations of the intended use for this model?</span><span>&nbsp;(e.g.</span><span>, How</span><span class="c4">&nbsp;transferable is this model? Are there specific future uses we should warn against?)</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What is the intended use of this model? What are potential unintended uses of this model? (e.g., What problem(s) does this model intend to solve? What does this model intentionally make more challenging? Are there social or ethical tradeoffs in these choices?)</span></li></ul><h4 class="c16" id="h.9b6ya6yi40gs"><span class="c30 c44 c37">Examples of harms and implications</span></h4><h5 class="c21" id="h.kmlm4ohp8au"><span class="c11">Example #1: </span><span class="c11">The Generalizable / Transferable Model</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c69">#SocietalHarm</span></p><p class="c3"><span class="c55">&rarr; </span><span class="c14">Downstream harms</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Imagine someone makes a model available for public use, and this model could be used for more generalizable settings, but the creators of the model do not include a statement about its intended and unintended uses.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c4">The underlying issue with this example is transferability, the notion that someone might try to transfer a model to a different (and potentially less applicable or riskier) domain. If generalizability is claimed, people and/or the media might interpret the model to be more generalizable than it actually is. If the specific intended use that is tied to the design and training of the specific model is not stated, it could be misused and lead to unintended harm.</span></p></td></tr></table><h5 class="c21" id="h.44fps4jh57e6"><span class="c11">Example #2: </span><span class="c30 c11 c36">Using NLP To Make Fake Reviews</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c28 c22">#InterpersonalHarm</span></p><p class="c3"><span class="c14">&rarr; Technology-facilitated violence / malicious uses</span></p><p class="c3"><span class="c35 c22 c69">#SocietalHarm</span></p><p class="c3"><span class="c14">&rarr; Information harms</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/P16-2057&amp;sa=D&amp;source=editors&amp;ust=1734479425900573&amp;usg=AOvVaw0CFJoHNAnpCxycsgqb4gRR">A recent study</a></span><span>&nbsp;showed that NLP techniques could be used to detect fake reviews.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This study also showed that this same method could be used to </span><span class="c55">generate</span><span>&nbsp;fake reviews. If the researchers had only published the model without foresight about this potential unintended use, this </span><span>could have led to</span><span>&nbsp;harm (perpetuation of misinformation) (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/P16-2096.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425902812&amp;usg=AOvVaw2pfigt3P1o_oagBfswVlz1">source</a></span><span class="c4">). We note that awareness of unintended use does not necessarily mitigate its potential harms, but can help guide researchers towards mitigation strategies to prevent this use from occurring. This is an example of why it is important to be aware of how people might appropriate NLP technology for their own purposes.</span></p></td></tr></table><h5 class="c21" id="h.wekad8eemq9a"><span class="c11">Example #3: </span><span class="c30 c11 c36">Using NLP to Generate Propaganda</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c28 c22">#InterpersonalHarm</span></p><p class="c3"><span class="c14">&rarr; Technology-facilitated violence / malicious uses</span></p><p class="c3"><span class="c35 c22 c69">#SocietalHarm</span></p><p class="c3"><span class="c14">&rarr; Information harms</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>GPT2</span><span>&nbsp;was created as a general </span><span>language model. Some people found ways to fine-tune GPT2 to generate </span><span>propaganda</span><span>&nbsp;(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.middlebury.edu/institute/academics/centers-initiatives/ctec/ctec-publications-0/industrialization-terrorist-propaganda&amp;sa=D&amp;source=editors&amp;ust=1734479425907846&amp;usg=AOvVaw3yg2IQmSI83Uz6OTtj9ZXY">source</a></span><span>)</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Publicly releasing models that could be used to generate fake propaganda can lead to massive misinformation and can lead to political, democratic, and legal harm. </span><span class="c55">Note:</span><span>&nbsp;this use of GPT2 was &ldquo;</span><span class="c55">not received well by the scientific community, with some attributing this decision to an attempt to create hype around their research. The backlash ultimately made OpenAI reconsider their approach, and release the models in stages over 9 months</span><span>&rdquo; (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2020.acl-main.261.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425909914&amp;usg=AOvVaw2oXdT74XDOqxc_8Fyv-T3l">source</a></span><span class="c4">).</span></p></td></tr></table><h4 class="c16" id="h.ax851dxx6pr6"><span class="c30 c44 c37">Mitigation strategies</span></h4><p class="c3"><span class="c34">Strategies to explicitly articulate the intended use</span></p><ul class="c2 lst-kix_5hd2wrb71s38-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">Come up with a clear task description and document this.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Do not overclaim the generalizability of the research &ndash; this could lead to misinterpretations of how it should be used.</span></li><li class="c3 c5 li-bullet-0"><span>Document whether intended use is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts) (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclrollingreview.org/responsibleNLPresearch/&amp;sa=D&amp;source=editors&amp;ust=1734479425911549&amp;usg=AOvVaw2JrBI3VibqdI8jiyVC2Uv4">source</a></span><span class="c4">)</span></li><li class="c3 c5 li-bullet-0"><span>Make sure data and/or pretrained models are released under </span><span>a specified license</span><span class="c4">&nbsp;that is compatible with the conditions under which access to data was granted (in particular, derivatives of data accessed for research purposes should not be deployed in the real world as anything other than a research prototype, especially commercially).</span></li><li class="c3 c5 li-bullet-0"><span>Document the efforts to limit the potential use to circumstances in which the data/models could be used safely (such as an accompanying data/model statement). (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclrollingreview.org/responsibleNLPresearch/&amp;sa=D&amp;source=editors&amp;ust=1734479425912801&amp;usg=AOvVaw2s8Mm-rISw6B0XRGpXGhiu">source</a></span><span class="c4">)</span></li><li class="c3 c5 li-bullet-0"><span>When defining the task: </span><span>Do not mismatch between the intended use of the models and the intended use of the training datasets</span><span>.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/factsheets-model-inventory.html?context%3Dcpdaas&amp;sa=D&amp;source=editors&amp;ust=1734479425913997&amp;usg=AOvVaw3aFTMgEcG5pfnr0eiTp5vm">AI Factsheets</a></span><span>&nbsp;is a useful tool that you can use to share the intended use of models and to allow organization members to request additional uses for a model with clear documentation and transparency practices.</span><hr style="page-break-before:always;display:none;"></li></ul><h3 class="c43" id="h.zcqafm5nageb"><span class="c37 c53 c60">Auditing Research Questions</span></h3><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.tsdt5jl448fs"><span class="c44 c37">Common considerations</span></h4><ul class="c2 lst-kix_xnct1xfph69a-0"><li class="c3 c5 li-bullet-0"><span class="c4">Could my research questions lead to potential harm?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">How can I improve my research questions to reduce potential harm?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Why do I want to find answers to my research questions? Is this knowledge valuable to attain and worth any potential negative consequences?</span></li><li class="c3 c5 li-bullet-0"><span>If I am successful at answering my research questions, what impact could this have on others? </span></li></ul><h4 class="c16" id="h.ag2q4o6o10z9"><span class="c44 c37">Examples of harms and implications</span></h4><h5 class="c21" id="h.gv7omobd2vne"><span class="c11">Example #1: </span><span class="c30 c11 c36">Research to improve realistic image generation</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c69">#SocietalHarm</span></p><p class="c3"><span class="c14">&rarr; Information harms</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Imagine a team of machine learning researchers </span><span>embarks</span><span class="c4">&nbsp;on a project to advance the field of image generation by developing a novel approach using generative adversarial networks (GANs). As they delve into their research, they brainstorm specific research questions such as:</span></p><ul class="c2 lst-kix_5qkugsusxliw-0 start"><li class="c3 c5 li-bullet-0"><span class="c37">RQ1: </span><span class="c4">How can we improve the fidelity and diversity of generated images to achieve more realistic outputs?</span></li><li class="c3 c5 li-bullet-0"><span class="c37">RQ2: </span><span>What techniques can be developed to enhance the scalability and efficiency of training large-scale image generation models?</span></li></ul></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c4">The pursuit of improving the fidelity and diversity of generated images without considering ethical implications could lead to the creation of highly realistic deep fake content, exacerbating the spread of misinformation and undermining trust in visual media. Without including ethical considerations in the research question design, the research has a greater risk of contributing to these harms.</span></p></td></tr></table><h4 class="c16" id="h.k4y1y3yn8amw"><span class="c44 c37">Mitigation strategies</span></h4><p class="c3"><span class="c45 c37">Evaluating Proposed Research Plan</span></p><p class="c3"><span>Utilize these questions from the &quot;Heilmeier Catechism&quot; to help you think through and evaluate your proposed research (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.darpa.mil/work-with-us/heilmeier-catechism&amp;sa=D&amp;source=editors&amp;ust=1734479425921579&amp;usg=AOvVaw0U9y-hBkVG7I2qEPxv54ny">source</a></span><span class="c4">):</span></p><ul class="c2 lst-kix_tdg51sdobgli-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">What are you trying to do? Articulate your objectives using absolutely no jargon.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">How is it done today, and what are the limits of current practice?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What is new in your approach and why do you think it will be successful?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Who cares? If you are successful, what difference will it make?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What are the risks?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">How much will it cost?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">How long will it take?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What are the midterm and final &ldquo;exams&rdquo; to check for success?</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c34">Critically Examining the Impacts Of Your Research</span></p><ul class="c2 lst-kix_stgkp8ts1530-0 start"><li class="c3 c5 li-bullet-0"><span>The </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://tarotcardsoftech.artefactgroup.com/&amp;sa=D&amp;source=editors&amp;ust=1734479425923939&amp;usg=AOvVaw1ZdoVmTo2hxqdrULgEQvda">Tarot Cards of Tech</a></span><span class="c4">&nbsp;are a fun tool that provides specific questions about the unintended impacts that your technologies might have on society. Explore the different cards and answer the questions with your research team to uncover potential harms that could arise from your research.</span></li><li class="c3 c5 li-bullet-0"><span>Use </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.ideo.com/journal/ai-ethics-collaborative-activities-for-designers&amp;sa=D&amp;source=editors&amp;ust=1734479425924668&amp;usg=AOvVaw3NYcXILshVK10yuDUa53-f">IDEO&rsquo;s AI Ethics Cards</a></span><span>&nbsp;to aid in more ethical design of your research questions. These cards include four core design principles and ten activities that can be completed alone or with the research team.</span></li></ul><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c30 c37 c96 c52">Stage #2</span></p><h1 class="c29" id="h.s4qvt2eeu8b6"><span class="c52 c77">Dataset</span></h1><p class="c48 subtitle" id="h.bmmsnucw38gc"><span class="c41">Curation, Collection, Creation, Annotation</span></p><p class="c3"><span>This stage of the AI lifecycle focuses on all aspects related to the datasets that will be used for the model design, development, deployment, and associated research. Whether you are collecting and curating your own datasets, or adapting previously made datasets for your use, this section of the playbook outlines the potential harms that could arise during these decision-making processes, and describes current mitigation strategies for reducing the impact of those harms.</span></p><hr><p class="c3 c33"><span class="c30 c72 c37 c95"></span></p><p class="c3"><span class="c30 c82 c75 c71">Topics of Interest</span></p><ul class="c2 lst-kix_w7uhve9aueba-0"><li class="c3 c5 li-bullet-0"><span class="c45 c37 c52"><a class="c8" href="#h.i7b3lav4w7b2">Bias &amp; Diversity</a></span></li><li class="c3 c5 li-bullet-0"><span class="c45 c37 c52"><a class="c8" href="#h.og35q642jfc6">Exclusion Criteria</a></span></li><li class="c3 c5 li-bullet-0"><span class="c45 c37 c52"><a class="c8" href="#h.x9ydy0b5h3li">Data Quality</a></span></li><li class="c3 c5 li-bullet-0"><span class="c45 c37 c52"><a class="c8" href="#h.suf1io3w85su">Data Collection</a></span></li></ul><hr><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><h3 class="c43 c88" id="h.yz29xr9tdcfk"><span class="c30 c49 c47"></span></h3><h2 class="c59" id="h.63t55ltdi1rn"><span class="c30 c37 c52 c71">Transparency &amp; Documentation Checklist</span></h2><p class="c48 subtitle" id="h.h18gz87ce9oj"><span class="c30 c73 c36">Dataset</span></p><p class="c3"><span>As you are working with your dataset(s), you will be faced with certain choices like </span><span class="c55">should I anonymize this? Or should I exclude this?</span><span class="c4">&nbsp;As you make these decisions, document these things clearly and justify why these decisions were made.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span>In this stage of the AI lifecycle, we recommend you discuss and document the following:</span></p><ul class="c2 lst-kix_nfy673qapcua-0 start"><li class="c3 c5 li-bullet-0"><span>Fill out a datasheet for this dataset</span><span>&nbsp;(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1803.09010&amp;sa=D&amp;source=editors&amp;ust=1734479425928012&amp;usg=AOvVaw1PmQ-fwlUSsZyVf3Mn-_zo">paper</a></span><span>) (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://github.com/fau-masters-collected-works-cgarbin/datasheet-for-dataset-template&amp;sa=D&amp;source=editors&amp;ust=1734479425928470&amp;usg=AOvVaw2q7iFJBlD7Ke2FVFGXx4wS">template</a></span><span class="c4">)</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Describe any limitations of your approaches (e.g., use of filtering tools).</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Describe any risks and harms that might result from use of this dataset.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Explain how you checked for offensive content and identifiers (e.g., with a script, manually on a sample, etc.).</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Explain how you anonymized the data, i.e., removed identifying information like names, phone and credit card numbers, addresses, user names, etc. Examples are monodirectional hashes, replacement, or removal of data points. If anonymization is not possible due to the nature of the research (e.g., author identification), explain why.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">List any further privacy protection measures you are using: separation of author metadata from text, licensing, etc.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">If any personal data is used: specify the standards applied for its storage and processing, and any anonymization efforts.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">If individual speakers remain identifiable via search: discuss possible harms from misuse of this data, and your mitigation strategies.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.</span></li><li class="c3 c5 li-bullet-0"><span>If you are using human subjects to annotate this dataset, document the basic demographic and geographic characteristics of the annotator population. You can do this by filling out a </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/Q18-1041.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425929888&amp;usg=AOvVaw2uWbBoctHw3x2L-dXr3NsA">data statement</a></span><span>&nbsp;that describes the basic demographic and geographic characteristics of the annotators and the population they are intended to represent.</span></li><li class="c3 c5 li-bullet-0"><span class="c72 c75">Document the harms that may ensue from the limitations of the data collection methodology, especially concerning marginalized/vulnerable populations, and specifies the scope within which the data can be used safely.</span><hr style="page-break-before:always;display:none;"></li></ul><h3 class="c43" id="h.i7b3lav4w7b2"><span class="c37 c53 c60">Bias &amp; </span><span class="c37 c53 c60">Diversity</span></h3><p class="c3"><span class="c45 c37 c47">Definitions / Relevant Terms</span></p><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c37">Bias: </span><span>systematic and unfair preferences or distortions in the data, algorithms, or outputs that result in skewed representations or discriminatory outcomes, potentially reflecting and perpetuating societal inequalities and prejudices.</span></p><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c37">Diversity: </span><span class="c4">the representation of varied perspectives, experiences, and identities within the data, algorithms, or outputs, aiming to encompass a broad range of backgrounds and viewpoints to mitigate biases and promote inclusivity and equitable representation.</span></p><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.x0f1ecg3v7qt"><span class="c30 c39 c37">Common considerations</span></h4><ul class="c2 lst-kix_91vk0di44se5-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">How can bias be embedded into my datasets?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What are the best practices for measuring dataset bias?</span></li><li class="c3 c5 li-bullet-0"><span>Does my dataset have a </span><span>diverse</span><span class="c4">&nbsp;representation of text/images?</span></li><li class="c3 c5 li-bullet-0"><span>How diverse should my dataset be for my model&rsquo;s task?</span></li></ul><h4 class="c16" id="h.6fe3gsmv9lr4"><span class="c30 c39 c37">Examples of harms and implications</span></h4><h5 class="c21" id="h.obzpldrhv5jd"><span class="c11">Example #1: Filtering text</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c57">#QualityOfServiceHarm</span></p><p class="c3"><span class="c14">&rarr; Increased labor</span></p><p class="c3"><span class="c14">&rarr; Service or benefit loss</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>In the C4 dataset, </span><span>they filter out documents containing &ldquo;bad words&rdquo;, which has a side effect of filtering out text that is African American (AAE) vernacular, Hispanic English vernacular, and some LGBTQ+ identity words at a higher likelihood, disproportionately filtering out certain voices</span><span>&nbsp;and identities. </span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>The model trained on this data is less able to process text from those kinds of people, which means the tools we build from this dataset will no longer work for this population. It is not equitably distributing benefits, and there is inequity in certain demographic groups&rsquo; ability to use this technology (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3531146.3533088&amp;sa=D&amp;source=editors&amp;ust=1734479425935007&amp;usg=AOvVaw3tIdJ15aBfW8DBwZCOwHRK">source</a></span><span>).</span></p></td></tr></table><p class="c3 c33"><span class="c4"></span></p><p class="c3 c33"><span class="c19"></span></p><h5 class="c21" id="h.99805ps1vax5"><span class="c11">Example #2: Scraping What Data is Available</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c14">&rarr; Erasing Social Groups</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Some previous research has scraped data from Reddit and Twitter because it historically has been more readily available to scrape than other social media platforms, such as Facebook or LinkedIn.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c4">The choice to only scrape data that is more easily available introduces unknown biases into the system, and can make it challenging to know what/who is being left out of the data because of this. For example, scraping from certain platforms might exclude multilingual data, or might only include information from users of a certain demographic.</span></p></td></tr></table><p class="c3 c33"><span class="c19"></span></p><h4 class="c16" id="h.asqdca887l5x"><span class="c37 c39">Mitigation strategies</span></h4><p class="c3"><span class="c34">Improving Dataset Diversity</span></p><p class="c3"><span class="c4">We note that, in some ways, &quot;diversity&quot; is really dependent on the task itself. When answering the questions &quot;does the dataset have a diverse representation of text or images,&quot; or a &quot;diversity of topics being discussed&quot;, one should revisit what the collection is supposed to represent, and and then consider different facets of that (e.g., who is the intended population for the task?).</span></p><p class="c3"><span>To improve diversity of the dataset with respect to the data creators and labelers (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2112.07475&amp;sa=D&amp;source=editors&amp;ust=1734479425939516&amp;usg=AOvVaw0zgLK_OUAlZSVi_nFGyX7K">source</a></span><span class="c4">), we recommend you utilize the following tools:</span></p><ul class="c2 lst-kix_bf3umtcrzzqw-0 start"><li class="c3 c5 li-bullet-0"><span>Fill out a datasheet for this dataset</span><span>&nbsp;(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1803.09010&amp;sa=D&amp;source=editors&amp;ust=1734479425940408&amp;usg=AOvVaw0ugMflzn9Of6QgAQ_rdgvy">paper</a></span><span>) (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://github.com/fau-masters-collected-works-cgarbin/datasheet-for-dataset-template&amp;sa=D&amp;source=editors&amp;ust=1734479425941115&amp;usg=AOvVaw2qn5v6J4OiuA2JQkBD0MUt">template</a></span><span class="c4">). All data has a context; there is no &quot;raw&quot; data. Too frequently, data sharing in ML takes data out of those contexts, or loses those contexts. Datasheets for datasets and other data documentation practices are critical for maintaining/understanding data&#39;s context.</span></li><li class="c3 c5 li-bullet-0"><span>If you are using human subjects to annotate this dataset, document the basic demographic and geographic characteristics of the annotator population. You can do this by filling out a </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/Q18-1041.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425942195&amp;usg=AOvVaw3JciIA80b899a1ZUwX6RUM">data statement</a></span><span>&nbsp;that describes the basic demographic and geographic characteristics of the annotators and the population they are intended to represent.</span><span>&nbsp;In addition, specify whether you are explicitly trying to operate under a prescriptive paradigm (if so, detail) or a descriptive paradigm. &ldquo;The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it&rdquo; (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2022.naacl-main.13/&amp;sa=D&amp;source=editors&amp;ust=1734479425943037&amp;usg=AOvVaw3raDrgOrcfVH9fxP_g64q6">source</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span class="c4">We also suggest that you document the known diversity of your dataset by answering questions such as the following:</span></li></ul><ul class="c2 lst-kix_bf3umtcrzzqw-1 start"><li class="c3 c13 li-bullet-0"><span class="c4">Who are the authors/photographers/artists who created the data I am using? What identities do they represent or not represent?</span></li><li class="c3 c13 li-bullet-0"><span class="c4">What diversity of language/dialect do the authors of this dataset capture?</span></li><li class="c3 c13 li-bullet-0"><span class="c4">Does the dataset have a diversity of topics that are being discussed?</span></li><li class="c3 c13 li-bullet-0"><span class="c4">Does the dataset have a diversity of images and/or tags? </span></li><li class="c3 c13 li-bullet-0"><span class="c4">Which features in this dataset are diverse and which are not?</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c34">&ldquo;Debiasing&rdquo; a dataset</span></p><p class="c3"><span>We note that there is no way to actually remove bias entirely. In the context of NLP, computer vision, and generative AI, a</span><span>&nbsp;dataset is a sample from a population</span><span>, you can make that sample unbiased with respect to a population, but you can never completely </span><span>unbias</span><span>&nbsp;it. Keep in mind that more data does not always equate to more diverse data (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3442188.3445922&amp;sa=D&amp;source=editors&amp;ust=1734479425945469&amp;usg=AOvVaw07eFPETnaHwNsVGyDP2oow">source</a></span><span>, Section 4.1</span><span class="c4">).</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c34">Strategies to measure dataset bias</span></p><ul class="c2 lst-kix_bf3umtcrzzqw-0"><li class="c3 c5 li-bullet-0"><span>Measure diversity with respect to demographic </span><span>identit</span><span>ies captured by the dataset. </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2022.emnlp-main.625/&amp;sa=D&amp;source=editors&amp;ust=1734479425946648&amp;usg=AOvVaw3N8kZ9SZvqzBHZoH82HzhH">This paper</a></span><span>&nbsp;introduces an</span><span class="c4">&nbsp;inclusive bias measurement dataset, HolisticBias, which can be used as a standardized method for measuring bias in NLP systems. This is considered a more low-effort mitigation strategy, for teams who have less time to conduct an audit for bias evaluation.</span></li><li class="c3 c5 li-bullet-0"><span>If you claim that your data covers languages and/or literature from around the world, include better representation of </span><span>different languages in your datasets</span><span>&nbsp;(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2112.04426&amp;sa=D&amp;source=editors&amp;ust=1734479425947700&amp;usg=AOvVaw3Gu3t5q54REwbIVWb_r8uR">source</a></span><span>, </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2108.03265&amp;sa=D&amp;source=editors&amp;ust=1734479425948211&amp;usg=AOvVaw0VPXvvQdO-8fJSef1Qi9N-">source</a></span><span>, </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2010.11934&amp;sa=D&amp;source=editors&amp;ust=1734479425948612&amp;usg=AOvVaw1srpoOTY5ejdzrFwHarT6j">source</a></span><span>)</span><span>. We note a caveat, that sometimes simply opting to include more representative data can in itself &nbsp;be problematic &ndash; it is worth exploring this if you are concerned about potential unintended consequences of increased representation in your dataset (e.g., </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.wired.co.uk/article/maori-language-tech&amp;sa=D&amp;source=editors&amp;ust=1734479425949207&amp;usg=AOvVaw0kwIn4E6cCMLKHikUTEb9r">when M&#257;ori language data was gathered for datasets</a></span><span class="c4">&nbsp;and led to negative outcomes for people who speak that language).</span></li></ul><ul class="c2 lst-kix_bf3umtcrzzqw-1 start"><li class="c3 c13 li-bullet-0"><span class="c55">Note: </span><span class="c4">If you only claim that your dataset covers certain specific languages or texts, this mitigation strategy is less applicable.</span></li><li class="c3 c13 li-bullet-0"><span class="c55">Note: </span><span class="c4">If the research team is from an English-speaking country, we advise heightened responsibility to consider diversity of languages than those from countries whose main language is not English, as there is already so much work focusing on English, and less-so focused on other languages.</span></li></ul><hr style="page-break-before:always;display:none;"><h3 class="c43 c88" id="h.49xms47rn5ro"><span class="c30 c37 c53 c60"></span></h3><h3 class="c43" id="h.og35q642jfc6"><span class="c37 c53 c60">Exclusion Criteria</span></h3><p class="c3"><span class="c45 c37 c47">Definitions / Relevant Terms</span></p><p class="c3"><span class="c37">Personal Information (PI) data</span><span>&nbsp;(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2023.trustnlp-1.18.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425950952&amp;usg=AOvVaw0gE5aJtnUJmZz3lw73B0bg">source</a></span><span class="c4">) includes any of the following:</span></p><ul class="c2 lst-kix_n1ax58vdrowu-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">Birth-centered characteristics (e.g., nationality, gender)</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Society-centered characteristics (e.g., immunization status)</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Social-based characteristics (e.g., membership on a sports team)</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Character-based characteristics (e.g., email address)</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Records-based characteristics (e.g., health records)</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Situation-based characteristics (e.g., GPS location)</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c14">We note that this section of the playbook provides guidance for some explicit scenarios where you might want to exclude certain kinds of data from your dataset. This list is, however, not all encompassing, and we encourage your team to consider additional, context-specific exclusion criteria.</span></p><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.i8zr1lrset48"><span class="c30 c39 c37">Common considerations</span></h4><ul class="c2 lst-kix_n1ax58vdrowu-0"><li class="c3 c5 li-bullet-0"><span>Does the dataset include hate speech, toxic images, PI data, or violence</span><span class="c4">? How can I measure if these are included in my dataset?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">If the dataset does contain hate speech, toxic images, PI data, or violence&mdash;should these data be excluded?</span></li><li class="c3 c5 li-bullet-0"><span>Are we appropriately handling </span><span>legal concerns </span><span>related to data collection and use and meeting legal requirements</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span>&nbsp;</span><span>that are</span><span class="c4">&nbsp;region specific?</span></li></ul><h4 class="c16" id="h.6rcxxdnp0xiz"><span class="c39 c37">Examples of harms and implications</span></h4><h5 class="c21" id="h.ok49pbf506a2"><span class="c11">Example #1: Using Training Data The Contains P</span><span class="c11">ersonal Information</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c28 c22">#InterpersonalHarm</span></p><p class="c3"><span class="c14">&rarr; Privacy violations</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>It has been shown that if private data exists in the training data, it can be remembered and leaked by LMs (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2012.07805&amp;sa=D&amp;source=editors&amp;ust=1734479425955951&amp;usg=AOvVaw2hK0CpOyH8Sfemc1Ojlm19">source</a></span><span>). It has also been shown that Co-pilot (a GPT-3 based tool) was found to leak functional API keys (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://analyticsdrift.com/github-copilot-ai-is-leaking-functional-api-keys/&amp;sa=D&amp;source=editors&amp;ust=1734479425956458&amp;usg=AOvVaw2DysPgRxklce414sfZAYu5">source</a></span><span>).</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This ability to remember private information can create a cascading effect from dataset to model use. For example, if a user wanted to obtain specific PI (e.g., email addresses, phone numbers, and physical addresses), they can sometimes do so by prompting trained language models that do not exclude this in their training datasets. This can lead to harms such as identity theft or discrimination based on sensitive characteristics. </span></p></td></tr></table><p class="c3 c33"><span class="c19"></span></p><h5 class="c21" id="h.h697jp6jvovy"><span class="c11">Example #2: Using Training Data That Contains </span><span class="c11">Hate Speech</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c14">&rarr; Demeaning Social Groups</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This example is more related to datasets that will be used to train public-facing generative language models (e.g., public facing chatbots), rather than general purpose LLMs. Imagine a chatbot that is trained on data that contains hate speech or other offensive language.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>For public-facing language models, there can be a c</span><span>ascading effect from dataset to model use, where offensive language can be generated from LLMs even if it is </span><span>unprompted</span><span>&nbsp;(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2009.11462.pdf?fileGuid%3DzX3zH5DBtXQa0ktq&amp;sa=D&amp;source=editors&amp;ust=1734479425961504&amp;usg=AOvVaw0SQRz0dT5tMmtyYB1fbxv_">source</a></span><span>).</span><span>&nbsp;This type of language, if </span><span>generated</span><span>&nbsp;and shown to humans without properly contextualizing the outputs, can cause offense, psychological harm, or can incite hate or violence. (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3531146.3533088&amp;sa=D&amp;source=editors&amp;ust=1734479425962201&amp;usg=AOvVaw1q0QLZP5WxfQ-GRcdJsaf8">source</a></span><span class="c4">)</span></p><p class="c3"><span class="c55">We note a caveat that there is currently no unified consensus on what is considered hate speech versus not hate speech, and this is important to keep in mind when labeling, categorizing, or filtering based on this heuristic (</span><span class="c1 c55"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2111.07997&amp;sa=D&amp;source=editors&amp;ust=1734479425962862&amp;usg=AOvVaw2oAtxmn_khduxfS-jYjmO-">source</a></span><span class="c55">).</span></p></td></tr></table><p class="c3 c33"><span class="c14"></span></p><h4 class="c16" id="h.y7lzbx4o6bba"><span class="c30 c39 c37">Mitigation strategies</span></h4><p class="c3"><span class="c45 c37">Best practices for removing versus keeping toxic/hateful/violent content</span></p><ul class="c2 lst-kix_bf3umtcrzzqw-0"><li class="c3 c5 li-bullet-0"><span class="c4">Decide how much you want to change your dataset to minimize harms versus trying to maintain the distribution of the data as you found it.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Recognize that changes to the data might impact model performance and it&#39;s hard to know to what extent until changes are made.</span></li><li class="c3 c5 li-bullet-0"><span>Recognize that there will always be a tradeoff (e.g., free speech vs. censorship, or misinterpreting the geographical/cultural context of language), and you have to decide how you want to strike this balance.</span></li><li class="c3 c5 li-bullet-0"><span class="c55">We note that there are some contexts where offensive content existing in the dataset is not necessarily bad. For example, swear words occur naturally in text data, or if the task is to create a model for hate speech detection, this would require using data that includes hate speech </span><span class="c55">(</span><span class="c1 c55"><a class="c8" href="https://www.google.com/url?q=https://aclrollingreview.org/responsibleNLPresearch/&amp;sa=D&amp;source=editors&amp;ust=1734479425964584&amp;usg=AOvVaw2Wa2KxrUD794Hcm8D8NaRN">source</a></span><span class="c55">)</span><span class="c55">. </span><span class="c1 c55"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1608.08868.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425965163&amp;usg=AOvVaw3Jm6ktXzr1zHyMHgmUe4xg">Other research</a></span><span class="c14">&nbsp;has also shown that African American English (AAE) can be more likely to be incorrectly labeled as hate speech, which implies that automated hate speech detection and filtering can pose its own disparate harms if done without careful consideration of the relationship between hate-speech identification and ethnicity.</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c34">Methods to filter toxic statements from training corpora</span></p><ul class="c2 lst-kix_bf3umtcrzzqw-0"><li class="c3 c5 li-bullet-0"><span class="c4">This is generally really challenging, but there are some methods for doing this through model training, after training models, by filtering LM outputs, decoding techniques, and prompt design.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2109.07445.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425966291&amp;usg=AOvVaw0sjsSr3ddFz0VTK-G0wDHd">This work</a></span><span class="c4">&nbsp;critically evaluates and analyzes several of these approaches for evaluating LM toxicity and can be used as a helpful reference.</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c45 c37">Tools to help detect PI data</span><span class="c45 c37">&nbsp;(</span><span class="c1 c37"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2023.trustnlp-1.18.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425966940&amp;usg=AOvVaw3_qX72Qk8x7pTm4sQXAZqz">source</a></span><span class="c34">)</span></p><ul class="c2 lst-kix_bf3umtcrzzqw-0"><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/1179601.1179608&amp;sa=D&amp;source=editors&amp;ust=1734479425967349&amp;usg=AOvVaw1WupAWFcCaTWpESwmerBSy">Named Entity Recognition</a></span><span>: Uses regular expressions to achieve fair accuracy on detecting </span><span>PI</span><span class="c4">&nbsp;data.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://github.com/tokern/piicatcher&amp;sa=D&amp;source=editors&amp;ust=1734479425967876&amp;usg=AOvVaw1Vdi3T0hBSxZtaXUGEvGYp">PIICatcher</a></span><span>&nbsp;and </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://github.com/PovertyAction/PII_detection&amp;sa=D&amp;source=editors&amp;ust=1734479425968357&amp;usg=AOvVaw1Nj8cvY91Q6RywrpA8v7wy">PII Detection Tool</a></span><span class="c4">: These detect PI data in text, and use pattern-matching and statistical models to detect different kinds of PI. Works best on tables and dataframes.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://github.com/microsoft/presidio&amp;sa=D&amp;source=editors&amp;ust=1734479425968847&amp;usg=AOvVaw2CJPbsxvokde0PLrAhp9r9">Presidio</a></span><span class="c4">: Identifies entities in unstructured text, uses pattern-matching and ML models to detect character-based types of personal information.</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c34">Strategies for preventing privacy leaks</span></p><ul class="c2 lst-kix_bf3umtcrzzqw-0"><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2009.10031&amp;sa=D&amp;source=editors&amp;ust=1734479425969502&amp;usg=AOvVaw3fHX66fGx9_vdI_llIVGqr">This research</a></span><span class="c4">&nbsp;showcases one method for training production LMs without memorizing user data by using differential privacy methods.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/2976749.2978318&amp;sa=D&amp;source=editors&amp;ust=1734479425969991&amp;usg=AOvVaw0rIrs3svmbMqOeFO_8tTcb">This research</a></span><span>&nbsp;showcases new algorithmic techniques for training deep learning models while minimizing privacy costs while also using differential privacy methods. and a refined analysis of privacy costs within the framework of differential privacy.</span></li></ul><hr style="page-break-before:always;display:none;"><h3 class="c43 c88" id="h.nrmgjjucivrs"><span class="c30 c47 c49"></span></h3><h3 class="c43" id="h.6bh4thjpmg98"><span class="c37 c53 c60">Data Quality</span></h3><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.selacq78p13x"><span class="c30 c39 c37">Common considerations</span></h4><ul class="c2 lst-kix_8pfaa7u0e2zg-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">How much should we filter our dataset to make it higher quality?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">How much does &ldquo;cleaning&rdquo; our dataset have an impact on representation, bias, hate speech, etc.?</span></li></ul><h4 class="c16" id="h.y7vgdhbjnifs"><span class="c30 c39 c37">Examples of harms and implications</span></h4><h5 class="c21" id="h.iijip197n1r6"><span class="c11">Example #1: Using a Language ID System</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c14">&rarr; Erasing social groups</span></p><p class="c3"><span class="c35 c22 c57">#QualityOfServiceHarm</span></p><p class="c3"><span class="c14">&rarr; Service or benefit loss</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Imagine someone wants to exclude non-English from their English-language dataset. If they use a language ID system to complete this task, it will give them a prediction and level of confidence of whether the data is in English or not. It is common practice to select an acceptable confidence threshold and to exclude data below this threshold.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This system would filter out all English that is non-standard or minority English, and the resulting model will perform poorly with respect to those filtered out language types.</span></p></td></tr></table><h4 class="c16" id="h.3d8vf7vqssud"><span class="c30 c39 c37">Mitigation strategies</span></h4><p class="c3"><span class="c45 c37">Best practices for filtering your dataset</span></p><p class="c3"><span>We note that there is currently not a &ldquo;best&rdquo; or &ldquo;correct&rdquo; way to do data filtering yet. If you are using </span><span>a language ID system</span><span class="c4">, you should be aware that this is incorporating more bias into your dataset. Previous research does, however, provide some currently accepted best-practices and techniques:</span></p><ul class="c2 lst-kix_bf3umtcrzzqw-0"><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2010.14571&amp;sa=D&amp;source=editors&amp;ust=1734479425975070&amp;usg=AOvVaw0XjzG9qv1sWKmM4LvR0NE7">This research</a></span><span class="c4">&nbsp;proposes relevant mitigation techniques for cleaning and filtering datasets without compromising their quality.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00447/109285/Quality-at-a-Glance-An-Audit-of-Web-Crawled&amp;sa=D&amp;source=editors&amp;ust=1734479425975727&amp;usg=AOvVaw268ZWuGYkzU1pZKDQZa-r0">This research</a></span><span class="c4">&nbsp;discusses techniques to evaluate and improve multilingual datasets for data quality.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c34">Tools for documenting and evaluating your dataset</span></p><ul class="c2 lst-kix_3oqam8vafvbn-0 start"><li class="c74 c5 li-bullet-0"><span>The </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://huggingface.co/blog/data-measurements-tool&amp;sa=D&amp;source=editors&amp;ust=1734479425976500&amp;usg=AOvVaw1kdR3XVmOfyZmf6qOZ8ZZX">Data Measurements Tool (DMT)</a></span><span class="c22">&nbsp;</span><span class="c4">is an interactive interface and open-source library that lets dataset creators and users automatically calculate metrics that are meaningful and useful for responsible data development.</span></li><li class="c74 c5 li-bullet-0"><span>The </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://wimbd.allen.ai/%2520https://arxiv.org/pdf/2310.20707.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425977209&amp;usg=AOvVaw3MrSEXCymXjVlGFBWjlBbV">WIMBD</a></span><span class="c4">&nbsp;is a tool that can be used to retrieve some automatic documentation of the contents of a dataset along a number of dimensions, including toxic language, duplicate documents, PII, etc. This tool was designed to work on large datasets, but can easily be applied to small ones as well.</span></li></ul><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c38 c53 c62"></span></p><h3 class="c43" id="h.suf1io3w85su"><span class="c37 c53 c60">Data Collection</span></h3><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.bm9zyaxi5wb8"><span class="c30 c39 c37">Common considerations</span></h4><ul class="c2 lst-kix_xnct1xfph69a-0"><li class="c3 c5 li-bullet-0"><span class="c4">What are the best practices for using human subjects to annotate / generate data?</span></li><li class="c3 c5 li-bullet-0"><span>What are the best practices for collecting already existing data (e.g., scraping the web)?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Is &ldquo;publicly available data&rdquo; okay to include in a dataset? When should this kind of data be excluded from datasets?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">How might data annotations embed bias into my dataset?</span></li></ul><h4 class="c16" id="h.e8ypd4wgxbae"><span class="c30 c39 c37">Examples of harms and implications</span></h4><h5 class="c21" id="h.qlpkd1ltbdad"><span class="c11">Example #1: Annotation guidelines as a source for dataset bias. </span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c14">&rarr; Erasing social groups</span></p><p class="c3"><span class="c14">&rarr; Denying people the opportunity to self-identify</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2020.acl-main.418/&amp;sa=D&amp;source=editors&amp;ust=1734479425981558&amp;usg=AOvVaw2jn1VKvJpv4nxV4IWh2fJV">This paper</a></span><span>&nbsp;shows how people&rsquo;s conceptions of gender are reproduced in coreference resolution systems that assume a strict gender dichotomy.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>When systems assume a strict gender binary, this can increase </span><span>cisnormativity</span><span>&nbsp;(excluding people who do not identify on a gender binary, which may be a population you want to include), and lead to feelings of exclusion or erasure of people who identify outside of that binary (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2021.emnlp-main.150.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425982824&amp;usg=AOvVaw2bivJA14xUhJmDxpXS2Lqf">source</a></span><span>).</span></p></td></tr></table><p class="c3 c33"><span class="c4"></span></p><h5 class="c21" id="h.rv5yblq028ak"><span class="c11">Example #2: Nonconsensual data collection and use</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c14">&rarr; Denying people the opportunity to self-identify</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Imagine a dataset that includes images of people, was collected without their consent, and is now used to train models on attributes that were not self-identified by those people in the dataset.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This example showcases how the people in this dataset do not have autonomy to correct classifications of themselves, nor do they have the opportunity to opt-out of their data being used. This kind of harm has in some cases been shown to negatively impact marginalized communities</span><span>&nbsp;(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3411764.3445498&amp;sa=D&amp;source=editors&amp;ust=1734479425986011&amp;usg=AOvVaw0FGTfx6bAllJ--00L426cS">source</a></span><span>)</span><span>.</span></p></td></tr></table><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.pblx2uxp7l64"><span class="c30 c39 c37">Mitigation strategies</span></h4><p class="c3"><span class="c34">Assessing Whether Publicly Available Data is Appropriate For Use</span></p><p class="c3"><span class="c4">Although publicly available data may be legally allowed to be scraped and used to train models, we recommend that you critically evaluate if this kind of data is appropriate for use for your given task. Assess the following questions:</span></p><ul class="c2 lst-kix_dmf65t97trau-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">Did your data come from ethical places? (e.g., are the people who are represented by this data aware that their data is on this website?)</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Was this data scraped using legal means?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Did the people who are represented by this data give consent for third parties to collect it? If consent was given, is that consent commonly known? (e.g., was consent given by accepting a terms of service, and users are actually largely unaware that their data can be scraped).</span></li><li class="c3 c5 li-bullet-0"><span>Consider this </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://journals.sagepub.com/doi/full/10.1177/20539517231177620&amp;sa=D&amp;source=editors&amp;ust=1734479425987628&amp;usg=AOvVaw2lCNNS0cVfHAUKuuhxXAPP">Supply Chain Analogy</a></span><span class="c4">: are you expecting that ethical &nbsp;data practices are being done by others before/after you rather than interrogating if this publicly available data should not be publicly available for use? How can you take ownership / responsibility of this data to ensure that it is ethical to scrape, collect, and/or use it?</span></li></ul><p class="c3 c33"><span class="c34"></span></p><p class="c3"><span class="c34">Best practices for using human annotators or human participants</span></p><ul class="c2 lst-kix_bigsmldlbgzk-0 start"><li class="c3 c5 li-bullet-0"><span>If human subjects are shown offensive content or if </span><span>PI data</span><span>&nbsp;is collected from them, they </span><span>should be warned</span><span>. We also note that although there is a common default to collect demographic information about human subjects, this can also become an invasion of privacy, and there is a possibility that other less-sensitive/protected information might get at a more meaningful measurement of people&rsquo;s lived experiences (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.06626&amp;sa=D&amp;source=editors&amp;ust=1734479425988603&amp;usg=AOvVaw1kaYzZQLER8Mx2z3Kl32Pd">source</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span>Human subjects should be </span><span>fairly compensated for their labor</span><span class="c4">. Consider the governmental hourly wage of subjects based on their local setting, the hourly wage where the research is being conducted, and any risks associated with their labor that would warrant additional compensation.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">If you use or curate data from human subjects, consent should be obtained first. Consent should include informing human subjects how this data will be used.</span></li><li class="c3 c5 li-bullet-0"><span>If you are collecting data from human subjects, your methods should be approved by an ethics review board (e.g., IRB), or they should be determined exempt from review by an ethics review board.</span><span>&nbsp;If an IRB does not apply (e.g., you are collecting data from humans, but it is not considered human subjects research, or you are not affiliated with an organization that has an IRB), then we suggest you still follow the IRB practices to ensure ethical practices in data collection and use. The </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://about.citiprogram.org/&amp;sa=D&amp;source=editors&amp;ust=1734479425989651&amp;usg=AOvVaw2mbPHJtdTFlV8JI3lhEXOp">CITI Program</a></span><span class="c4">&nbsp;provides courses and resources to learn about these practices, regardless of institutional affiliation.</span></li><li class="c3 c5 li-bullet-0"><span>You may consider sharing results or otherwise involving communities who have provided data to make data stewardship and use more participatory (e.g. </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.adalovelaceinstitute.org/report/participatory-data-stewardship/&amp;sa=D&amp;source=editors&amp;ust=1734479425990240&amp;usg=AOvVaw2WK8FEtzg_VeO8OWmhcWLK">Participatory Data Stewardship</a></span><span class="c4">). </span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://pervade.umd.edu/pervade-data-ethics-tool/&amp;sa=D&amp;source=editors&amp;ust=1734479425990705&amp;usg=AOvVaw2Bm2dHGWKoCNmq0K7nx-Gr">PERVADE Decision Support Tool</a></span><span>: </span><span class="c4">Use this tool to help think through data collection best practices.</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c34">Example of one way to reduce dataset bias from annotations</span></p><ul class="c2 lst-kix_bigsmldlbgzk-0"><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/P19-1163&amp;sa=D&amp;source=editors&amp;ust=1734479425991372&amp;usg=AOvVaw0St5f6L6DyxjAHJv7wuxQN">This paper</a></span><span>&nbsp;focuses on the effect of priming annotators with information about possible dialectal differences when asking them to apply toxicity labels to sample tweets. They learned that annotators who are primed with this social context are significantly less likely to mistake tweets containing features associated with African-American English as</span><span>&nbsp;offensive.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c34">Assessing Power Relationships Between Researchers and People in Data</span></p><ul class="c2 lst-kix_ssxyep2erusf-0 start"><li class="c74 c5 li-bullet-0"><span>The </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://pervade.umd.edu/pervade-data-ethics-tool/&amp;sa=D&amp;source=editors&amp;ust=1734479425992105&amp;usg=AOvVaw1g7RbjPxJH17XST6-oDfyd">PERVADE tool</a></span><span class="c4">&nbsp;helps researchers to reflect on whether you are studying or producing models about people with more, less, or equal social power, much like anthropologists practice reflexivity about relative power in fieldwork. This (like defining diversity) can be challenging to do, but is a useful starting point for assessing if power dynamics should influence specific research strategies or methodologies.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c30 c37 c96 c81">Stage #3</span></p><h1 class="c29" id="h.4rkoug7h5qeh"><span class="c30 c82 c77 c81">Model Design</span></h1><p class="c48 subtitle" id="h.lf65qz4nuign"><span class="c41">Implications of Design Choices</span></p><p class="c3"><span>In this stage of the AI lifecycle, we explore potential harms and risks that might arise when designing your model. The Model Design stage is in some ways similar to the </span><span class="c1"><a class="c8" href="#h.k77hicxwqq1z">Problem Formulation</a></span><span class="c4">&nbsp;stage. However, these stages differ in that the Problem Formulation stage is concerned with defining the problem and setting project goals, while the Model Design stage focuses on the technical implementation of the AI model to solve the defined problem. The Problem Formulation stage lays the groundwork for the project, while the Model Design stage involves the actual development and construction of the AI solution.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span>While many technical considerations in model design may not initially prioritize ethical concerns, the Model Design stage offers a crucial opportunity to proactively evaluate potential ethical implications before finalizing technical decisions. In this section, we concentrate on these aspects of model design to assist in steering clear of </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.wired.com/story/opinion-ethical-tech-starts-with-addressing-ethical-debt/&amp;sa=D&amp;source=editors&amp;ust=1734479425993631&amp;usg=AOvVaw1MnXBDC_eYXya4I9067f7T">ethical debt</a></span><span class="c4">&mdash;a scenario akin to technical debt, where early unethical design choices may necessitate extensive system architecture overhauls later to address resulting harms.</span></p><hr><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c71">Topics of Interest</span></p><ul class="c2 lst-kix_w7uhve9aueba-0"><li class="c3 c5 li-bullet-0"><span class="c45 c37 c53"><a class="c8" href="#h.d5pmk1b9mln7">Model Design Bias &amp; Diversity</a></span></li></ul><hr><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><h3 class="c43 c88" id="h.67ogk7oo2lfl"><span class="c30 c49 c47"></span></h3><h2 class="c59" id="h.h7r6zpciseg6"><span class="c30 c37 c81 c71">Transparency &amp; Documentation Checklist</span></h2><p class="c48 subtitle" id="h.5l5dlv4302bw"><span class="c30 c73 c36">Model Design</span></p><p class="c3"><span>As you are designing and architecting your model(s), you will be faced with certain choices like </span><span class="c55">should I allow certain language to be generated by my model </span><span>or </span><span class="c55">should I attempt to minimize the perpetuation of certain kinds of biases through this model?</span><span class="c4">&nbsp;As you make these decisions, document these things clearly and justify why these decisions were made.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c4">In this stage of the AI lifecycle, we recommend you discuss and document the following:</span></p><ul class="c2 lst-kix_xrzk5cfxtw4m-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">If you plan to reduce model bias: which definition of bias are you using? State your motivation for using this definition.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What kind of model bias are you hoping to minimize in your system?</span></li><li class="c3 c5 li-bullet-0"><span class="c72 c75">If your model/dataset is going to be deployed as a product, work with that product team to fill out an </span><span class="c1 c72"><a class="c8" href="https://www.google.com/url?q=https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479425995540&amp;usg=AOvVaw2DFgcR-LpoF2A4nnu6ZGGk">impact assessment</a></span><span class="c4">&nbsp;for the model at this stage in the life cycle.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Discuss and document any restrictions you plan to design for your model&rsquo;s output, because of their capacity to perpetuate bias, reduce diversity, and/or cause harm.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Document the implications of these restrictions, and unintended consequences or harm that could result from implementing them.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Discuss if you have selected the most appropriate model for this task (e.g., is a simpler model like logistic regression actually more appropriate than generative AI?).</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><h3 class="c43 c88" id="h.qlw53tl19kym"><span class="c30 c49 c47"></span></h3><h3 class="c43" id="h.d5pmk1b9mln7"><span class="c37 c53 c60">Model Design </span><span class="c37 c53 c60">Bias </span><span class="c37 c53 c60">&amp; Diversity</span></h3><p class="c3"><span class="c45 c37 c47">Definitions / Relevant Terms</span></p><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c37">Bias: </span><span class="c4">systematic and unfair preferences or distortions in the data, algorithms, or outputs that result in skewed representations or discriminatory outcomes, potentially reflecting and perpetuating societal inequalities and prejudices.</span></p><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c37">Diversity: </span><span>the representation of varied perspectives, experiences, and identities within the data, algorithms, or outputs, aiming to encompass a broad range of backgrounds and viewpoints to mitigate biases and promote inclusivity and equitable representation.</span></p><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.94xliwb2tx9b"><span class="c30 c9">Common considerations</span></h4><ul class="c2 lst-kix_jxs8idtrfx9u-0"><li class="c3 c5 li-bullet-0"><span class="c4">What kind of bias could my model capture from the training data?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Could my model cause and/or perpetuate representational harm from a lack of appropriate diversity?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Do I want my model to be able to generate content that includes certain tokens from the vocabulary (e.g., slurs, swear words, hate speech)? Or do I want to restrict this from being able to be generated?</span></li></ul><h4 class="c16" id="h.atjm50ahdd7h"><span class="c9 c30">Examples of harms and implications</span></h4><h5 class="c21" id="h.x7yl6tud30v8"><span class="c11">Example #1: Choosing tokenisation well-suited for English</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c57">#QualityOfServiceHarm</span></p><p class="c3"><span class="c14">&rarr; Service or benefit loss</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Imagine model designers who have an intention to make a model that is multilingual, but instead choose to architect an LM where the tokenisation is more well-suited to English, and not morphologically more complex languages (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3531146.3533088&amp;sa=D&amp;source=editors&amp;ust=1734479425999635&amp;usg=AOvVaw1F30AB4ySEHwPguFcPqIBn">source</a></span><span>). In many cases, current state of the art LMs are primarily trained in English or Mandarin Chinese and perform better in these compared to any other languages (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2109.07684&amp;sa=D&amp;source=editors&amp;ust=1734479426000186&amp;usg=AOvVaw0Ud3NUXE4OJ7y8A8jriRdu">source</a></span><span>).</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This design choice could result in r</span><span>epresentational harm</span><span>, as the model will not work as well for languages other than English, which can systematically </span><span>underserve</span><span>&nbsp;people who speak a different language.</span></p></td></tr></table><p class="c3 c33"><span class="c19"></span></p><h5 class="c21" id="h.om309sbn5hcs"><span class="c11">Example #2: Automated inference of data</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c55">&rarr; Denying people the opportunity to self-identify</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Reconsider </span><span>automatic inference</span><span>&nbsp;of user attributes, a common and interesting NLP task, whose solution also holds promise for many useful applications, such as recommendation engines and fraud or deception detection</span><span>&nbsp;</span><span>(</span><span class="c70"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/P16-2096.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426003918&amp;usg=AOvVaw2mGYCa1g_MsWaiOE1i5Lrq">source</a></span><span>)</span><span>.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>In practice, relying on models that produce false positives may lead to bias confirmation and overgeneralization. Would we accept the same error rates if the system was used to predict sexual orientation or religious views, rather than age or gender? Given the right training data, this is just a matter of changing the target variable. In addition, automatic inference of attributes denies people the ability to self-identify and correct for inaccuracies.</span></p></td></tr></table><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.5lk3gzh58rsy"><span class="c9">Mitigation strategies</span></h4><p class="c3"><span class="c45 c37">Designing your model to allow or restrict certain language</span></p><ul class="c2 lst-kix_5hd2wrb71s38-0"><li class="c3 c5 li-bullet-0"><span>Look at the vocabulary of your model, there might be words that are slurs, swear words, etc. in the vocab itself. Ask yourself &ndash; do I want the model to be able to generate content that includes those tokens from the vocabulary? Or do you want to restrict this from being able to be generated?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Think about the context of your model&rsquo;s use:</span></li></ul><ul class="c2 lst-kix_5hd2wrb71s38-1 start"><li class="c3 c13 li-bullet-0"><span class="c4">It&rsquo;s not possible to understand an utterance or a prediction without context</span></li><li class="c3 c13 li-bullet-0"><span>Almost every word that appears in a vocabulary can be used in a context that is not offensive. E.g., should the model be </span><span class="c55">able </span><span class="c4">to generate a word like &ldquo;Nazi&rdquo;? It can be offensive in some contexts but also can be useful in others (e.g., talking about history). Consider the tradeoffs of including or excluding terms like this.</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c45 c37">Articulate and document your conceptualizations of &ldquo;bias&rdquo; (</span><span class="c1 c37"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2020.acl-main.485.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426007105&amp;usg=AOvVaw23jvSRCU3cINb-fbADEYeB">source</a></span><span class="c34">)</span></p><p class="c3"><span class="c4">Work analyzing &ldquo;bias&rdquo; in NLP systems should provide explicit statements of why the system behaviors that are described as &ldquo;bias&rdquo; are harmful, in what ways, and to whom, as well as the normative reasoning underlying these statements. We recommend you discuss and answer the following questions:</span></p><ul class="c2 lst-kix_5hd2wrb71s38-0"><li class="c3 c5 li-bullet-0"><span>What kinds of system behaviors are described as &ldquo;bias&rdquo;?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What are their potential sources (e.g., general assumptions, task definition, data)?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">In what ways are these system behaviors harmful, to whom are they harmful, and why?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What are the social values (obvious or not) that underpin this conceptualization of &ldquo;bias?&rdquo; (It could be useful to think of bias as a difference between existing system behavior and &ldquo;ideal&rdquo; system behavior; which includes spelling out what you want the ideal system behavior to be and why.)</span></li><li class="c3 c5 li-bullet-0"><span>How does this model reproduce or transform </span><span>language ideologies</span><span>&nbsp;(any sets of beliefs about languages as they are used in their social worlds)? (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2020.acl-main.485.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426008491&amp;usg=AOvVaw0AuxDAd6zZVyKN1O5vyZND">source</a></span><span>) Which language varieties or practices are deemed good or bad? Might &ldquo;good&rdquo; language simply mean language that is easily handled by existing NLP systems? For example, linguistic phenomena arising from many language practices </span><span>(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/N13-1037&amp;sa=D&amp;source=editors&amp;ust=1734479426009021&amp;usg=AOvVaw0q02-CLWODCNP1uwcKOnoK">source</a></span><span>) </span><span class="c4">are described as &ldquo;noisy text&rdquo; and often viewed as a target for &ldquo;normalization.&rdquo; How do the language ideologies that are reproduced by NLP systems maintain social hierarchies? </span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c45 c37">Identifying </span><span class="c45 c37">potential representational harm</span><span class="c34">&nbsp;from the model</span></p><ul class="c2 lst-kix_5hd2wrb71s38-0"><li class="c3 c5 li-bullet-0"><span>Document the ways that your model will be</span><span>&nbsp;</span><span>useful</span><span class="c4">&nbsp;for different kinds of people, note if there might be any disparities in utility based on someone&rsquo;s identity or other demographic characteristics.</span></li><li class="c3 c5 li-bullet-0"><span class="c55">Positionality Reflection: </span><span>The viewpoints of researchers and model creators can skew what is prioritized during model development and throughout the design decision-making process. We encourage the team to reflect on and document ways in which this team composition may differ from the background(s) of intended users of the model.</span></li><li class="c3 c5 li-bullet-0"><span>Consider existing power dynamics related to social and language ideologies that are embedded in your model. </span><span>Does your system have the potential to reproduce these kinds of power dynamics or hierarchies or ideologies</span><span>&nbsp;(for example, if a facial recognition system is used by a governmental agency to disproportionately incarcerate BIPOC communities, and reify existing inequality and racism&ndash;this would be reproducing unethical power dynamics)? If so, document this information and discuss the consequences of this reproduction </span><span>(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2020.acl-main.485.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426010374&amp;usg=AOvVaw340n8tokgwhbzQOrwTGyKX">source</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span>Consider stereotypes of different groups of people that can be generated by an image generation system, and which of those stereotypes you would like to prevent the model from generating. Previous work has audited text-to-image generation systems for stereotypical outputs and has described some of the implications of certain stereotypical outputs and why they might need to be improved (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2302.07159.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426010896&amp;usg=AOvVaw2mToIdPuXLdlfLz3nIp2xR">source</a></span><span class="c4">).</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c34">Addressing Over-simplification of data</span></p><ul class="c2 lst-kix_5hd2wrb71s38-0"><li class="c3 c5 li-bullet-0"><span>Ask yourself when attempting to infer attributes or simulate data for your model,</span><span>&nbsp;&ldquo;would a false answer be worse than no answer?&rdquo; (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/P16-2096.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426011551&amp;usg=AOvVaw2AT6J_QhxNeqZhioo3ZKPp">source</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Detail the potential worst-case consequences of opting to simplify data and/or the model in this way.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c30 c37 c96 c84">Stage #4</span></p><h1 class="c29" id="h.ygi6jm11l2da"><span class="c77 c84">Model Training</span></h1><p class="c48 subtitle" id="h.hrvp3yi0s1b"><span class="c41">Initial Evaluation of Outputs</span></p><p class="c3"><span class="c4">This stage of the AI lifecycle focuses on decisions that can lead to harms/risks while training your model. We note that model training and model evaluation are closely related to one another. In fact, these two stages of the AI lifecycle are cyclical and often iterative. Decisions and observations made during model training will influence further evaluation, and results from evaluation might influence future training iterations. We suggest that each iteration involves critical reflection about the potential impacts of decisions, as well as transparency and documentation around which decisions are and are not made.</span></p><p class="c3 c33"><span class="c30 c72 c37 c84"></span></p><hr><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c30 c82 c75 c71">Topics of Interest</span></p><ul class="c2 lst-kix_w7uhve9aueba-0"><li class="c3 c5 li-bullet-0"><span class="c45 c37 c53"><a class="c8" href="#h.y70kr9wh3fi7">Environmental Impact</a></span></li><li class="c3 c5 li-bullet-0"><span class="c45 c37 c53"><a class="c8" href="#h.5dqnewax72r0">Evaluation During Training</a></span></li><li class="c3 c5 li-bullet-0"><span class="c45 c37 c53"><a class="c8" href="#h.yigvcaogz85o">Biases from Objective Function</a></span></li></ul><hr><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><h3 class="c43 c88" id="h.xpdxz8tdc2zh"><span class="c0"></span></h3><h2 class="c59" id="h.idcrsqibujv9"><span class="c30 c37 c84 c71">Transparency &amp; Documentation Checklist</span></h2><p class="c48 subtitle" id="h.dbwmvrypvr3k"><span class="c30 c73 c36">Model Training</span></p><p class="c3"><span class="c4">During this stage of the AI lifecycle, we recommend you discuss and document the following:</span></p><ul class="c2 lst-kix_91vk0di44se5-0"><li class="c3 c5 li-bullet-0"><span>Document all of the potential decisions made during training, not just the final decisions. E.g., if you chose </span><span class="c55">not </span><span class="c4">to do something, document why you made that decision and what the implications of that decision are.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Document the environmental impact of your model (e.g., record CO2 emissions of training and/or retraining the model).</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Write down your objective function(s) and methods for optimizing your model to achieve those objectives. Discuss and document the consequences and potential harms of this choice of objective function, and any plans to mitigate harms that arise.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Discuss what kinds of outputs would warrant a halt on the current training run, and your plans to improve the model in that event.</span></li></ul><hr style="page-break-before:always;display:none;"><h3 class="c43 c88" id="h.6cj2ylf2ij76"><span class="c0"></span></h3><h3 class="c43" id="h.y70kr9wh3fi7"><span class="c37 c53 c60">Environmental Impact</span></h3><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.yfix4ospmdg9"><span class="c12">Common considerations</span></h4><ul class="c2 lst-kix_8pvyoajrhbtr-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">Is the environmental cost of training my model too high or unethical?</span></li><li class="c3 c5 li-bullet-0"><span>How can I lower the environmental impact of training my model?</span></li></ul><h4 class="c16" id="h.2njwvrnvd6t3"><span class="c12">Examples of harms and implications</span></h4><h5 class="c21" id="h.td6gtsjwkj4"><span class="c11">Example #1: </span><span class="c30 c11 c36">Choosing To Build a Very Large Model</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c69">#SocietalHarm</span></p><p class="c3"><span class="c55">&rarr; Environmental Harms</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c4">Imagine a task that requires creating a very large language model, with billions of parameters, and a very large training dataset. Every time this model is trained and retrained, it requires a lot of compute power, which is intensive both energetically and financially.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Choosing to train and retrain a large language model can lead to negative environmental impact. As Bender et. al shared, </span><span class="c55">&nbsp;&ldquo;The majority of cloud compute providers&rsquo; energy is not sourced from renewable sources and many energy sources in the world are not carbon neutral. In addition, renewable energy sources are still costly to the environment, and data centers with increasing computation requirements take away from other potential uses of green energy, underscoring the need for energy efficient model architectures and training paradigms&rdquo;</span><span>&nbsp;</span><span class="c70"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3442188.3445922&amp;sa=D&amp;source=editors&amp;ust=1734479426018446&amp;usg=AOvVaw3SRl9JnyWnMStRpMri_e-X">(</a></span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3442188.3445922&amp;sa=D&amp;source=editors&amp;ust=1734479426019040&amp;usg=AOvVaw0shOfj4FxpdTErok3nvrdA">source</a></span><span class="c70"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3442188.3445922&amp;sa=D&amp;source=editors&amp;ust=1734479426019450&amp;usg=AOvVaw3A94eOzQRDR3yB-XYx_Ep1">)</a></span><span>. There is also research that shows that operating large models can be just as, if not more, energy intensive than training those large models (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2104.10350&amp;sa=D&amp;source=editors&amp;ust=1734479426019948&amp;usg=AOvVaw3ZhANNlqu24P_cnD1-HEoU">source</a></span><span>).</span></p></td></tr></table><p class="c3 c33"><span class="c34"></span></p><h4 class="c16" id="h.2ozsxccll8ez"><span class="c12">Mitigation Strategies</span></h4><p class="c3"><span class="c34">Methods to measure and document environmental cost</span></p><ul class="c2 lst-kix_8pvyoajrhbtr-0"><li class="c3 c5 li-bullet-0"><span>Utilize the methods introduced in</span><span>&nbsp;</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/P19-1355.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426021373&amp;usg=AOvVaw2Iz2DprnkHxUoAipQECp73">this paper</a></span><span class="c4">&nbsp;to quantify and measure the computation and environmental cost of training your NLP model</span></li><li class="c3 c5 li-bullet-0"><span>The following two papers / tools also provide online tools to help you benchmark your model&rsquo;s energy usage (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2002.05651&amp;sa=D&amp;source=editors&amp;ust=1734479426022351&amp;usg=AOvVaw0Y8gjArEzc6tsflqNHbQv7">paper 1</a></span><span>, </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1911.08354%23:~:text%3DEnergy%2520Usage%2520Reports%253A%2520Environmental%2520awareness%2520as%2520part%2520of%2520algorithmic%2520accountability,-Kadan%2520Lottick%252C%2520Silvia%26text%3DThe%2520carbon%2520footprint%2520of%2520algorithms,active%2520role%2520in%2520environmental%2520sustainability.&amp;sa=D&amp;source=editors&amp;ust=1734479426023027&amp;usg=AOvVaw1WGRBfIEIgWRFc-kPp4YRu">paper 2</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Document the environmental impact of training your model (e.g., recording CO2 emissions)</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c45 c37">Strategies to lower the environmental impact of training</span><span class="c34">&nbsp;and operating models</span></p><ul class="c2 lst-kix_8pvyoajrhbtr-0"><li class="c3 c5 li-bullet-0"><span>Previous work has shown that large LMs can be segmented into smaller LMs that search and retrieve information from a data corpus (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2112.04426&amp;sa=D&amp;source=editors&amp;ust=1734479426024045&amp;usg=AOvVaw1R3aORwijmkcYLaVPgOSp6">source</a></span><span>, </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2007.01282&amp;sa=D&amp;source=editors&amp;ust=1734479426024418&amp;usg=AOvVaw2ZcqnWDnNcoaYx5--bDt_t">source</a></span><span>, </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2020.emnlp-main.550/&amp;sa=D&amp;source=editors&amp;ust=1734479426024743&amp;usg=AOvVaw1njczAWxnLaR1LurrB_R1I">source</a></span><span>, </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1911.00172&amp;sa=D&amp;source=editors&amp;ust=1734479426025059&amp;usg=AOvVaw0aWqdETTsA4tgInAVtqIlA">source</a></span><span class="c4">), which can reduce the environmental impact of these models. </span></li><li class="c3 c5 li-bullet-0"><span>Other research has explored how to conduct low precision computations for deep learning (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://proceedings.mlr.press/v37/gupta15.html&amp;sa=D&amp;source=editors&amp;ust=1734479426025578&amp;usg=AOvVaw1IQkI-g8yFOwfSp4hYjMld">source</a></span><span class="c4">), which is an energy-efficient approach to building large-scale deep neural networks with relatively low required computational power.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/pdf/10.1145/3531146.3533088&amp;sa=D&amp;source=editors&amp;ust=1734479426026120&amp;usg=AOvVaw3vN8-Vvxh2ojEFX2cI6SIp">This paper</a></span><span>&nbsp;describes how previous research has introduced methods for improving efficiency during training and inference (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2102.07988&amp;sa=D&amp;source=editors&amp;ust=1734479426026451&amp;usg=AOvVaw1QVribC_EO4A5vvbkxgWao">source</a></span><span>) by pruning (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2005.07683&amp;sa=D&amp;source=editors&amp;ust=1734479426026766&amp;usg=AOvVaw3XiJHM_pD9BNVj5vXGzZUd">source</a></span><span>), distillation (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1909.10351&amp;sa=D&amp;source=editors&amp;ust=1734479426027140&amp;usg=AOvVaw3ExfH1JtpsfGtDuc8JAgtA">source</a></span><span>, </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2002.10957&amp;sa=D&amp;source=editors&amp;ust=1734479426027475&amp;usg=AOvVaw1YOnKoyEq7mbk8mqtcf5Za">source</a></span><span>), or fine-tuning (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2112.11446&amp;sa=D&amp;source=editors&amp;ust=1734479426027766&amp;usg=AOvVaw3f3gB3NqyGrp2tyTe9aAxD">source</a></span><span>). </span><span class="c55">We note that the authors also caution that reducing energy costs in these ways may also lead to the unintended consequence of more labor, which might increase energy usage in the long run. An alternative option could be to select data centers with lower CO2 emissions / energy sources, or to target this issue at the organizational level (e.g., by shifting to more sustainable energy company-wide).</span></li></ul><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c4"></span></p><h3 class="c43" id="h.5dqnewax72r0"><span class="c37 c53 c60">Evaluation During Training</span></h3><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.qbc94nrebert"><span class="c37 c47 c84">Common considerations</span></h4><ul class="c2 lst-kix_99xvjj4uyl11-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">How can I evaluate early on if my model is beginning to generate harmful content such as toxic or hate speech? If it begins to generate these outputs, when should I stop my model training process?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">How can I evaluate if my model is overfitting to certain demographic biases during training?</span></li></ul><h4 class="c16" id="h.wmg18xcswh2o"><span class="c12">Examples of harms and implications</span></h4><h5 class="c21" id="h.q0ac6gbjydlq"><span class="c11">Example #1: Generation of toxic/hate speech</span><span class="c30 c11 c36">&nbsp;during training</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c14">&rarr; Demeaning social groups</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c4">Though it is not as common (and some research has shown that it can degrade model performance), there may be times when a model is built to learn on its own outputs, or when a model is being evaluated during the training process. In this context, during model training, you might notice the model beginning to generate a lot of toxic or hate speech.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Representationalharm is a downstream impact of this kind of generated content. Although the outputs of the model are not shown to people at this stage in the life cycle, if </span><span>the model continues to learn these outputs</span><span>, it will eventually lead to harm.</span></p></td></tr></table><h4 class="c16" id="h.9ut95fnyoyca"><span class="c12">Mitigation strategies</span></h4><p class="c3"><span class="c45 c37">Preventing a model from generating certain kinds of outputs (e.g., hate speech)</span></p><p class="c3"><span>We note that discussions on model evaluation and validation, as well as best practices for model monitoring and maintenance, can also provide guidance on when to intervene during training if problematic outputs are detected. While specific guidance on stopping training due to problematic outputs may not be extensively covered in existing literature, we suggest that you and your team have a discussion about what kinds of outputs would warrant a halt on the current training run.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span>If you plan to audit your model&rsquo;s outputs during evaluation, b</span><span class="c4">egin by measuring your model&rsquo;s propensity to generate hate speech / toxic speech / toxic outputs.</span></p><ul class="c2 lst-kix_bb18g463362n-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">Strategy #1: Look at the confidence the model uses to predict this kind of output in any generation. This is a good measurement of how likely the model is to generate slurs, hate speech, toxic speech, etc.</span></li><li class="c3 c5 li-bullet-0"><span>Strategy #2: Use a probing dataset. Probing datasets can be used to measure the models&rsquo; propensity to complete a sentence. For example, you can probe with a sentence like &ldquo;Asian people are ____&rdquo; (model fills in the blank), and then measure the likelihood that your model will generate stereotypes. This is also true of generating images (e.g</span><span>., images</span><span class="c4">&nbsp;of nurses that are women versus nurses that are men). This kind of strategy can also be used to measure a model&rsquo;s ability to generate language of a certain dialect.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Strategy #3: This probing strategy can also be used to measure a model&rsquo;s ability to generate language of a certain dialect. This strategy can also work for text to image or text to video models, where the probes are prompts fed into the model, and you will need to manually audit the generated results.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c45 c37">Check if your model is going to pick up harmful data such as toxic or hate speech</span></p><ul class="c2 lst-kix_5hd2wrb71s38-0"><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.13169&amp;sa=D&amp;source=editors&amp;ust=1734479426033450&amp;usg=AOvVaw3tekk5XWsOvL0C88UJV0MD">This paper</a></span><span class="c4">&nbsp;explores a pre-training method to filter training data for toxicity, and shows that using a toxicity filter can make you worse at identifying toxic language, while using an inverse toxicity filter can be more effective.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2021.acl-long.522/&amp;sa=D&amp;source=editors&amp;ust=1734479426034027&amp;usg=AOvVaw3SsOOa2UtvKh0r8Fu2cjSx">DExperts</a></span><span>&nbsp;is a method that operates on the output of a pretrained LM and combines a </span><span>pre trained</span><span class="c4">&nbsp;language model with &ldquo;expert&rdquo; LMs and/or &ldquo;anti-expert&rdquo; LMs, which is shown to work well for language detoxification.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c4"></span></p><h3 class="c43" id="h.yigvcaogz85o"><span class="c37 c53 c60">Biases From Objective Function</span></h3><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.8d9go4wr8fpw"><span class="c37 c47 c84">Common considerations</span></h4><ul class="c2 lst-kix_hswfzmci3dc-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">How does my choice of objective function lead to model bias?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">How does my method for objective function optimization lead to model bias?</span></li></ul><h4 class="c16" id="h.gl0eatx95mdk"><span class="c12">Examples of harms and implications</span></h4><h5 class="c21" id="h.hwohmbathavn"><span class="c30 c11 c36">Example #1: Optimizing For the Majority</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c69">#SocietalHarm</span></p><p class="c3"><span class="c4">&rarr; Information harms</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c4">Imagine a scenario where a machine learning developer is tasked with building a sentiment analysis model for customer reviews, and the developer optimizes the model solely to predict the most common sentiment in the dataset.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>If the developer optimizes the model solely to predict the most common sentiment in the dataset without considering the diversity of opinions, it may overlook minority viewpoints and perpetuate biases, potentially leading to inaccurate or unfair predictions.</span></p></td></tr></table><h4 class="c16" id="h.r8wva2g86svm"><span class="c12">Mitigation strategies</span></h4><p class="c3"><span class="c34">Mitigating Objective Function Bias</span></p><p class="c3"><span class="c4">In general, to mitigate bias that results from selecting and optimizing your objective function, the key is to actively consider and address potential biases during the training and optimization process, ensuring that the resulting models are fair, inclusive, and representative of diverse perspectives. Here we provide several examples of objective functions that could lead to harmful bias, and mitigation strategies for minimizing these unintended biases.</span></p><ul class="c2 lst-kix_ee0rw9sgmv1o-0 start"><li class="c3 c5 li-bullet-0"><span class="c37">For Language and/or Generative Text Models: </span><span>Suppose you&#39;re training a language model to generate text responses in a chatbot. Instead of optimizing solely for generating responses that are most commonly seen in the training data, you could incorporate techniques like &quot;debiasing&quot; where you actively identify and correct for biases in the training data (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1507.05259&amp;sa=D&amp;source=editors&amp;ust=1734479426037927&amp;usg=AOvVaw2kNRPh0wqK1jyY3NdmPNmA">source</a></span><span>). This could involve techniques such as reweighting the training samples based on demographic factors or utilizing adversarial training to identify and counteract biased language patterns (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1801.07593&amp;sa=D&amp;source=editors&amp;ust=1734479426038436&amp;usg=AOvVaw1xpEXVHwYhkGvimjD8HqrF">source</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Generative Text: </span><span>Consider a scenario where you&#39;re training a text generation model to create product descriptions. To mitigate biases, you could implement techniques like &quot;diversity-promoting&quot; training, where you encourage the model to generate a diverse range of descriptions that encompass various perspectives and characteristics of the product (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/D18-1428/&amp;sa=D&amp;source=editors&amp;ust=1734479426038966&amp;usg=AOvVaw14iSuMnPmufmVUB7-PzfnK">source</a></span><span>). Additionally, you could fine-tune the model on a diverse dataset that includes a wide range of voices and viewpoints to ensure the generated text is inclusive and representative (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1909.05858&amp;sa=D&amp;source=editors&amp;ust=1734479426039290&amp;usg=AOvVaw2-8wvQyBu0cx5UfEtFjwMO">source</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Image Generation: </span><span>Imagine you&#39;re developing an image generation model to create realistic images of human faces. To address biases, you could adopt techniques like &quot;data augmentation&quot; where you synthetically generate additional training examples by applying transformations such as flipping, rotation, or color adjustments to diversify the dataset (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1811.09030&amp;sa=D&amp;source=editors&amp;ust=1734479426039764&amp;usg=AOvVaw3xaZvXQSWg4m9IJzCYHy7J">source</a></span><span>). Additionally, you could use &quot;fairness-aware training&quot; methods that explicitly incorporate fairness constraints into the training process, ensuring that the generated images represent diverse demographics and avoid perpetuating stereotypes or biases present in the training data (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://proceedings.mlr.press/v28/zemel13.html&amp;sa=D&amp;source=editors&amp;ust=1734479426040085&amp;usg=AOvVaw00r5A9HwWm-VtEK92FouY9">source</a></span><span>).</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><h1 class="c29 c93" id="h.l6vo0s4fgdmh"><span class="c30 c82 c60 c75"></span></h1><p class="c3"><span class="c30 c37 c50 c96">Stage #5</span></p><h1 class="c29" id="h.t5fhdsqwi37v"><span class="c30 c82 c77 c50">Model Evaluation</span></h1><p class="c48 subtitle" id="h.r6fbaaa0rr8t"><span class="c41">Evaluation of Model Processing and Generation</span></p><p class="c3"><span class="c4">In this section, we explore the harms/risks that can arise during model evaluation. Outside of ethics evaluation, there are commonly used benchmarks that evaluate models&rsquo; capabilities; some of these benchmarks work well for general performance evaluation, while others cater well to ethics evaluation. While evaluating performance can sometimes be a part of ethics evaluation, in this section, we focus solely on evaluation topics as they relate to mitigating harms and improving ethics. We note that any evaluation suite is going to be incomplete, and not all will include evaluation metrics/techniques for ethics evaluation.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span>Ethical questions will arise at every stage, and we recommend you document all of them as they arise. We encourage awareness of best practices in evaluation and also the gaps in evaluation. Remember that </span><span class="c55">not</span><span>&nbsp;evaluating for something has downstream impacts and consequences; choosing </span><span class="c55">what to evaluate </span><span>has ethical consequences and choosing what </span><span class="c55">not to evaluate </span><span class="c4">has ethical consequences. Here we provide guidance on how to discern what evaluation methods may be best for your context, and how to specifically evaluate certain model harms and risks.</span></p><hr><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c30 c82 c75 c71">Topics of Interest</span></p><ul class="c2 lst-kix_w7uhve9aueba-0"><li class="c3 c5 li-bullet-0"><span class="c45 c37 c53"><a class="c8" href="#h.ug7hppc74ogp">Biases from Evaluation Choices</a></span></li><li class="c3 c5 li-bullet-0"><span class="c45 c37 c53"><a class="c8" href="#h.b0vgzb6xgpym">Measuring Bias</a></span></li><li class="c3 c5 li-bullet-0"><span class="c45 c37 c53"><a class="c8" href="#h.pk6e20monrx6">Evaluating Problematic Outputs</a></span></li><li class="c3 c5 li-bullet-0"><span class="c45 c37 c53"><a class="c8" href="#h.e8r84e5mhv5x">Measuring Societal Harm</a></span></li></ul><hr><p class="c3 c33"><span class="c4"></span></p><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c4"></span></p><h2 class="c98" id="h.53x9d0xwn97m"><span class="c30 c37 c50 c71">Transparency &amp; Documentation Checklist</span></h2><p class="c48 subtitle" id="h.45hhxtyr22em"><span class="c30 c73 c36">Model Evaluation</span></p><p class="c3"><span class="c4">In this stage of the AI lifecycle, we recommend you discuss and document the following:</span></p><ul class="c2 lst-kix_s8kajunwpmir-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">What you are choosing to evaluate and why.</span></li><li class="c3 c5 li-bullet-0"><span>What you are choosing </span><span class="c55">not </span><span class="c4">to evaluate and why.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Which specific evaluation metrics you are using on this model and why, and which metrics you explicitly chose not to use and why.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Document the tradeoffs and implications of these evaluation decisions.</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What domain(s) do you claim your model works for? What evaluation methods have you used (or intend to use) to evaluate that the model performs well across and within your intended domain(s)?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Which outputs would be considered problematic for your model&rsquo;s context? How are you planning to evaluate if your model is generating these problematic outputs?</span></li><li class="c3 c5 li-bullet-0"><span>How might your model capture or perpetuate bias from the training data? How are you planning to evaluate if this kind of bias is being captured or generated from the model?</span><hr style="page-break-before:always;display:none;"></li></ul><h3 class="c43" id="h.ug7hppc74ogp"><span class="c37 c53 c60">Biases From Evaluation Choices</span></h3><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.58m4udfoccl1"><span class="c37 c50 c47">Common considerations</span></h4><ul class="c2 lst-kix_i83pglsmrtnv-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">How can my choice of evaluation metrics lead to model bias?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Which evaluation metrics should I use to ethically evaluate my model?</span></li></ul><h4 class="c16" id="h.sncqssxxd6gy"><span class="c30 c37 c50 c47">Examples of harms and implications</span></h4><h5 class="c21" id="h.v6jkqf5vngy7"><span class="c30 c11 c36">Example #1: Global Accuracy Washes Out Performance</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm </span><span class="c35 c22">Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c57">#QualityOfServiceHarm</span></p><p class="c3"><span class="c14">&rarr; Service or benefit loss</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Imagine a machine learning practitioner who evaluates a sentiment analysis model&#39;s performance using global accuracy.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This choice to use global accuracy could mask the model&#39;s effectiveness across different demographic groups and could lead to biased conclusions, as the model might perform well overall but poorly on specific groups, reinforcing disparities and inequalities in predictions (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1610.02413&amp;sa=D&amp;source=editors&amp;ust=1734479426046116&amp;usg=AOvVaw28jtxSNtIMy_BPIEfdy1dU">source</a></span><span>).</span></p></td></tr></table><h5 class="c21" id="h.tac47ek3ivut"><span class="c30 c11 c36">Example #2: Over Reliance on Reference Gold Standard</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c14">&rarr; Erasing social groups</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c4">Imagine a machine learning practitioner who heavily relies on ROUGE/BLEU scores to evaluate the performance of a text generation model, which compares model outputs to a reference gold standard.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This could potentially bias the evaluation towards certain linguistic styles or biases present in the reference data and may overlook the model&#39;s ability to produce diverse and contextually relevant outputs, leading to the reinforcement of specific language patterns or biases present in the reference data (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/P02-1040/&amp;sa=D&amp;source=editors&amp;ust=1734479426049183&amp;usg=AOvVaw17Xp04t4iXi0VIwHY0wyAQ">source</a></span><span class="c4">).</span></p></td></tr></table><p class="c3 c33"><span class="c4"></span></p><h5 class="c21" id="h.ywge82zf5jzu"><span class="c30 c11 c36">Example #3: Using In-Distribution Data for Evaluation</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c69">#SocietalHarm</span></p><p class="c3"><span class="c14">&rarr; Political and civil harms</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c4">Suppose a facial recognition model is trained and evaluated using in-distribution data that closely resembles the demographics of a specific population, such as the training data predominantly consisting of images of individuals from a certain racial or ethnic group.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>If this model is then deployed in real-world settings to identify individuals from diverse backgrounds, it may disproportionately misidentify or underperform for individuals from underrepresented groups due to the distribution shift in the real-world data. This could lead the practitioner to potentially overestimate the model&#39;s real-world performance, and may lead to a false sense of confidence in the model&#39;s capabilities (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing&amp;sa=D&amp;source=editors&amp;ust=1734479426052345&amp;usg=AOvVaw2ETczpK8AcraCiQ2KWlYSH">source</a></span><span class="c4">).</span></p></td></tr></table><h4 class="c16" id="h.skku9h7a93jo"><span class="c30 c37 c50 c47">Mitigation Strategies</span></h4><p class="c3"><span class="c34">Implementing Group-Specific Evaluation Metrics</span></p><p class="c3"><span>If you are worried that your model might perform better or worse for certain groups of users, employing group-specific evaluation metrics such as demographic parity or equal opportunity to ensure fair assessment across different subgroups (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1610.02413&amp;sa=D&amp;source=editors&amp;ust=1734479426053250&amp;usg=AOvVaw0QBUtwi-i9P0SiMxqpciMK">source</a></span><span class="c4">).</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c34">Improving Evaluation Metrics</span></p><ul class="c2 lst-kix_hg8waqqzu11r-0 start"><li class="c3 c5 li-bullet-0"><span class="c22">Automatic Evaluation: </span><span class="c4">Although automatic evaluation can be efficient and useful, we recommend you consider adopting methods for incorporating human evaluation or diverse reference datasets to complement automatic evaluation metrics and to capture the nuanced quality of model outputs.</span></li><li class="c3 c5 li-bullet-0"><span class="c22">Unseen Data Distributions: </span><span>Employing techniques such as out-of-distribution detection or adversarial evaluation can help you assess your model&rsquo;s robustness and generalization performance on unseen data distributions (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1705.08378&amp;sa=D&amp;source=editors&amp;ust=1734479426054043&amp;usg=AOvVaw30pAXs0KnKtoYmc7-PqTGi">source</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span class="c22">Dynamic Evaluation: </span><span>If your model is generating outputs in a context that is expected to change, you might want to avoid evaluating your model statically. Previous work has shown methods to implement</span><span>&nbsp;dynamic evaluation mechanisms that continuously collect user feedback and adjust evaluation criteria based on evolving linguistic trends and consumer preferences, ensuring that the model&#39;s outputs remain relevant and engaging (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1904.08378&amp;sa=D&amp;source=editors&amp;ust=1734479426054549&amp;usg=AOvVaw0IOGVhqhQ8FQ0pnINcFFAN">source</a></span><span>).</span></li></ul><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c4"></span></p><h3 class="c43" id="h.b0vgzb6xgpym"><span class="c37 c53 c60">Measuring Bias</span></h3><p class="c3"><span class="c45 c37 c47">Definitions / Relevant Terms</span></p><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c37">Bias: </span><span>systematic and unfair preferences or distortions in the data, algorithms, or outputs that result in skewed representations or discriminatory outcomes, potentially reflecting and perpetuating societal inequalities and prejudices.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c37">Domain</span><span>: </span><span>the specific context or area of application in which a model is intended to operate, characterized by its unique features, data distribution, task requirements, and the origin of the training data.</span></p><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.me6jkwnhi1be"><span class="c30 c37 c50 c47">Common considerations</span></h4><ul class="c2 lst-kix_8pvyoajrhbtr-0"><li class="c3 c5 li-bullet-0"><span class="c4">What kinds of bias should we be measuring and/or evaluating?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Does our model incorporate biases (e.g., related to gender/identity characteristics)?</span></li><li class="c3 c5 li-bullet-0"><span>What domains do I claim that my model works well for? Have I evaluated the model within all of those domains?</span><span class="c4">&nbsp;Should I evaluate my model with data from different domains?</span></li><li class="c3 c5 li-bullet-0"><span>Can my model perform similarly for inputs from different languages?</span></li></ul><h4 class="c16" id="h.n1q86jwz07j"><span class="c37 c50 c47">Examples of harms and implications</span></h4><h5 class="c21" id="h.3j9wezuv90ui"><span class="c30 c11 c36">Example #1: Evaluating Bias in Image Generation</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c14">&rarr; Erasing social groups</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Imagine a machine learning practitioner who trains an image generation model for creating realistic human faces and evaluates its accuracy by comparing the generated images to a dataset of real human faces.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>If there are underlying biases in the training data, the practitioner might assume that high accuracy in replicating facial features indicates successful model performance without considering representational fairness or demographic diversity. This oversight may perpetuate biases in facial recognition technology and exacerbate disparities in visual representation, as the model may disproportionately generate faces resembling certain demographic groups over others. Furthermore, relying solely on accuracy as an evaluation metric may fail to capture the model&#39;s failures to generate diverse and inclusive representations, leading to biased and exclusionary outcomes that reinforce societal inequalities (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426058405&amp;usg=AOvVaw33uAw1j_e7WO1ev96_ax8x">source</a></span><span>). </span></p></td></tr></table><h5 class="c21" id="h.j5xvwdnnw70u"><span class="c11">Example #2: Domain-Specific Evaluations</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c68">#AllocativeHarm</span></p><p class="c3"><span class="c14">&rarr; Opportunity loss</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c4">At Amazon, there was an attempt to build a model to filter applicants&rsquo; resumes to see who would get interviews. Amazon ranked the candidates from that model, but it excluded and down ranked women candidates (even if the name was masked out).</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c4">Had this model been put to use, it would have systematically harmed an already vulnerable population by not giving them the same opportunities as others, solely based on protected demographic classes.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span>We note that in this case-study, </span><span class="c55">because </span><span class="c4">Amazon decided to evaluate their model in the correct domain, they evaluated according to ethical metrics and found that performance across those metrics was worse than they were willing to deploy. This is an example of a positive use-case.</span></p></td></tr></table><p class="c3 c33"><span class="c4"></span></p><h5 class="c21" id="h.bdqp55xz7z3i"><span class="c11">Example #3: </span><span class="c30 c11 c36">Model accepts inputs from different languages</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22">Harm Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c57">#QualityOfServiceHarm</span></p><p class="c3"><span class="c14">&rarr; Increased labor</span></p><p class="c3"><span class="c14">&rarr; Service or benefit loss</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Imagine someone creates a text-to-image generation model that allows for inputs from various languages. The model is able to produce images for text inputs other than English.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>It has been shown that some popular text-to-image generation models have significant performance degradation when the input text is from a language other than English, which can lead to lowered system utility accuracy for users who speak languages other than English (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2208.09333.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426064081&amp;usg=AOvVaw3wm2sJXR6BcXdIcNBmnxws">source</a></span><span class="c4">). In these scenarios, it is important to test the performance of the model on all acceptable input languages and modalities.</span></p></td></tr></table><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.8oqy9kxgb9ds"><span class="c30 c37 c50 c47">Mitigation strategies</span></h4><p class="c3"><span class="c34">Developing an Evaluation Plan for Bias &amp; Diversity</span></p><p class="c3"><span class="c4">To begin your evaluation process, we first recommend that you discuss the following questions with your team to determine what kinds of bias or harms might be useful to evaluate:</span></p><ul class="c2 lst-kix_8pvyoajrhbtr-0"><li class="c3 c5 li-bullet-0"><span class="c4">Could my model disproportionately perform better or worse for certain users based on demographic characteristics?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Is my model stereotyping or excluding certain identity characteristics of certain groups of people?</span></li><li class="c3 c5 li-bullet-0"><span>We also recommend you look through the considerations, case studies, and mitigation strategies included in the </span><span class="c1"><a class="c8" href="#h.d5pmk1b9mln7">Model Design Bias &amp; Diversity</a></span><span class="c4">&nbsp;section of this playbook.</span></li></ul><p class="c3 c33"><span class="c34"></span></p><p class="c3"><span class="c34">Measuring the model&rsquo;s processing capabilities</span></p><p class="c3"><span>Measure the model&#39;s ability to take input that is different types of text </span><span>(e.g., from different languages, dialects, vernacular)</span><span class="c4">&nbsp;or the model&rsquo;s ability to take input of images that do not exist in or vastly differ from the training data. Here we provide some examples of previous research that has conducted evaluations of this kind:</span></p><ul class="c2 lst-kix_5gev2o922b7x-0 start"><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2212.08011&amp;sa=D&amp;source=editors&amp;ust=1734479426065887&amp;usg=AOvVaw3FKyEnKV1uzcsohgxKm2qO">This research</a></span><span class="c4">&nbsp;introduces a suite of resources (Multi-VALUE, a controllable rule-based translation system spanning 50 English dialects and 189 unique linguistic features) for evaluating and achieving English dialect invariance. </span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2204.03031&amp;sa=D&amp;source=editors&amp;ust=1734479426066438&amp;usg=AOvVaw3u_nJUhUSRE5AKVNKmN6tf">This research</a></span><span class="c4">&nbsp;introduces the VernAcular Language Understanding Evaluation (VALUE) benchmark, which includes rules for 11 features of African American Vernacular English (AAVE).</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2023.emnlp-main.421/&amp;sa=D&amp;source=editors&amp;ust=1734479426066868&amp;usg=AOvVaw31jnMzuL9gjQcfnUuisfwj">This research</a></span><span>&nbsp;uses the </span><span>CORAAL</span><span>&nbsp;dataset to evaluate how well LLMs understand African American Language (AAL) in comparison to White Mainstream English (WME).</span><span class="c4">&nbsp;</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2312.10523&amp;sa=D&amp;source=editors&amp;ust=1734479426067324&amp;usg=AOvVaw2oRduEkekvhyk7zZmjVyWp">This research</a></span><span class="c4">&nbsp;evaluates LM performance of nine corpora of English from different countries with the ICE dataset.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.pnas.org/doi/full/10.1073/pnas.1915768117&amp;sa=D&amp;source=editors&amp;ust=1734479426067702&amp;usg=AOvVaw3ZleLOkAqFx9n6evdkjmxV">This research</a></span><span class="c4">&nbsp;evaluates automated speech recognition (ASR) systems on cross-dialectal speech (which is a different modality from text).</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.researchgate.net/profile/Kemal-Oflazer/publication/263083068_A_Multidialectal_Parallel_Corpus_of_Arabic/links/0a85e539b532075644000000/A-Multidialectal-Parallel-Corpus-of-Arabic.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426068278&amp;usg=AOvVaw3Zm2UtVP7JBz0ZHsFt3mrM">This research</a></span><span class="c4">&nbsp;introduces a multi-dialectical Arabic corpus of statements.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.zora.uzh.ch/id/eprint/174601/1/gscl2015.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426068702&amp;usg=AOvVaw1XJQHt6ThCub6Sni8-7_It">This research</a></span><span class="c4">&nbsp;introduces a corpus with hand-labeled, part-of-speech tags for Swiss-German dialects.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber%3D7889589%26casa_token%3DQV1grXDCu5gAAAAA:suqs_BVEPdwUZXrx79fMdnR4S8WUWndk-b5W-9-lQFObB2xtPqyOhdJhNanaQjZUeF0-bQ6ggA%26tag%3D1&amp;sa=D&amp;source=editors&amp;ust=1734479426069171&amp;usg=AOvVaw1d9RMYDW3TtaC04rSKZbia">This research</a></span><span>&nbsp;introduces a corpus for Ecuadorian dialects of Spanish.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c34">Benchmark Bias Evaluation Strategies &amp; Limitations for NLP</span></p><p class="c3"><span>When evaluating bias and/or toxicity of your model, we first recommend that you decide whether you are trying to evaluate the model itself or the generated outputs from the model. Strategies may differ depending on your intended evaluation target.</span></p><ul class="c2 lst-kix_wa5eplpvj99e-0 start"><li class="c3 c5 li-bullet-0"><span class="c37">Template-based prompts</span><span>&nbsp;can be used to evaluate bias in NLP systems by systematically generating a set of prompts that cover various aspects of bias, such as gender, race, religion, and other sensitive attributes. These prompts can then be used to assess the model&#39;s responses and identify any biases or inconsistencies in its output (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.12757&amp;sa=D&amp;source=editors&amp;ust=1734479426069899&amp;usg=AOvVaw1MIx5HQPsBjdK3J3WuNmz5">source</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Stereotype Benchmark Datasets </span><span class="c4">are datasets that can be used to detect and mitigate social stereotypes about groups of people in NLP models.</span></li></ul><ul class="c2 lst-kix_wa5eplpvj99e-1 start"><li class="c3 c13 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2021.acl-long.81.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426070387&amp;usg=AOvVaw0mfe0WkF6K2nDy68XrLmE1">This research</a></span><span>&nbsp;discusses four stereotype benchmark datasets that are used for evaluating bias and fairness in NLP, and also describes some key limitations of using these datasets for evaluation of bias in practice. These four datasets include (1) </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2004.09456&amp;sa=D&amp;source=editors&amp;ust=1734479426070682&amp;usg=AOvVaw02Ar5-DyRROEESNIV2aNND">StereoSet</a></span><span>; (2) </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2020.emnlp-main.154/&amp;sa=D&amp;source=editors&amp;ust=1734479426070989&amp;usg=AOvVaw1QDGcBln2LG_OZ40OQrnRE">CrowS-Pairs</a></span><span>; (3) </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/N18-2003/&amp;sa=D&amp;source=editors&amp;ust=1734479426071250&amp;usg=AOvVaw3bOjCv3kzH7bDDfftwSD3W">WinoBias</a></span><span>; and (4) </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/N18-2002/&amp;sa=D&amp;source=editors&amp;ust=1734479426071507&amp;usg=AOvVaw1cGVLNBoT2QA8TKcF3DN0_">WinoGender</a></span><span class="c4">.</span></li><li class="c3 c13 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.11840&amp;sa=D&amp;source=editors&amp;ust=1734479426071842&amp;usg=AOvVaw27D0lRssQDymlDw0u9Lj-i">This research</a></span><span class="c4">&nbsp;introduces SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c45 c37">Types of </span><span class="c45 c37">domains</span><span class="c34">&nbsp;to evaluate on</span></p><p class="c3"><span>Not all problem scenarios or tasks will require evaluating on a variety of domains, and it is also possible that the granularity of domains depends on the application. For example, some have called different fields of science across scientific text separate domains. We recommend that if you claim that your task(s) work across or between a variety of domains, you document all of these domains, and what data you would need to evaluate your model in this domains (e.g., if you claim that your model works for all English language, evaluate your model across different dialects of English to ensure that your entire domain is captured in your evaluation).</span></p><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><h3 class="c43 c88" id="h.bq7be2in2ilg"><span class="c0"></span></h3><h3 class="c43" id="h.pk6e20monrx6"><span class="c37 c53 c60">Evaluating Problematic Outputs</span></h3><p class="c3"><span class="c78 c45 c37 c75 c47">Definitions / Relevant Terms</span></p><p class="c3 c33"><span class="c78 c45 c37 c75 c47"></span></p><p class="c3"><span class="c37">Problematic Output: </span><span class="c4">Though there is no universally accepted list of problematic generated outputs, in this playbook we describe problematic outputs as generated outputs that include hate speech, personal information, offensive speech, stereotypes, misinformation, or other outputs that can lead to harm. These outputs may be problematic or harmful in certain contexts, and may not be in other contexts.</span></p><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.gpxg12w4e36"><span class="c30 c37 c50 c47">Common considerations</span></h4><ul class="c2 lst-kix_8pvyoajrhbtr-0"><li class="c3 c5 li-bullet-0"><span class="c4">What kinds of generated outputs are problematic? How should we be measuring and/or evaluating these kinds of outputs?</span></li><li class="c3 c5 li-bullet-0"><span>How can we evaluate if our model is able to generate hate speech, personal information, offensive speech, stereotypes, misinformation, or other problematic outputs?</span></li></ul><h4 class="c16" id="h.a8oy4cyxv4xy"><span class="c37 c50 c47">Examples of harms and implications</span></h4><h5 class="c21" id="h.qis2rueprql"><span class="c30 c11 c36">Example #1: A model that memorizes data</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c28 c22">#InterpersonalHarm</span></p><p class="c3"><span class="c14">&rarr; Privacy violations</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Imagine a model is memorizing data. This can be good for factual information, but is bad for personal information and/or copyrighted</span><sup><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span>&nbsp;data that you do </span><span class="c55">not </span><span>want the model to memorize.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>If the model is capable of memorizing personal information, it could accidentally share sensitive PI data such as home addresses, mobile phone numbers, or even credit card information. This could lead to malicious uses of this information such as stalking or identity theft.</span></p></td></tr></table><h5 class="c21" id="h.76wfygtnnsss"><span class="c30 c11 c36">Example #2: A model that outputs stereotypes</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c14">&rarr; Stereotyping social groups</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>The model captures language&rsquo;s social categories and norms, such as defining the term &ldquo;family&rdquo; as heterosexual married parents with a blood-related child, which denies the existence of families to whom these criteria do not apply. Or referring to &ldquo;women doctors&rdquo; rather than &ldquo;doctors&rdquo; (implying that doctors are not typically women).</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>If this system generates images of female nurses and male doctors&mdash;it can reinforce stereotypes that women can only be nurses and not doctors (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1707.09457&amp;sa=D&amp;source=editors&amp;ust=1734479426078186&amp;usg=AOvVaw3IReNfdwDJi3grX8Twtyy2">source</a></span><span>, </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3442188.3445922&amp;sa=D&amp;source=editors&amp;ust=1734479426078637&amp;usg=AOvVaw2F557FuIl9lTtQQlPJvivr">source</a></span><span>).</span><span>&nbsp;This could also reinforce exclusionary norms by outputting language that excludes or silences certain identities that fall outside of certain categories, and can also place additional burden on people who don&rsquo;t comply with norms or people who are actively trying to change those norms </span><span>(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3531146.3533088&amp;sa=D&amp;source=editors&amp;ust=1734479426079103&amp;usg=AOvVaw01Dt8_j8XVaHRTA23w8yw5">source</a></span><span>)</span><span>.</span></p></td></tr></table><h5 class="c21" id="h.hf0yazf8mvnw"><span class="c30 c11 c36">Example #3: A model that confuses people and animals</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c14">&rarr; Demeaning social groups</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Infamously, in the past, some image tagging systems have accidentally confused certain groups of people with animals </span><span>(</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai&amp;sa=D&amp;source=editors&amp;ust=1734479426081676&amp;usg=AOvVaw18_G4TTYey-VHL4eBuU7Cc">source</a></span><span>)</span><span>.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This kind of output can demean, marginalize, or oppress the groups of people who are being incorrectly tagged.</span></p></td></tr></table><h5 class="c21" id="h.9qh75t2rvk32"><span class="c30 c11 c36">Example #4: A model that produces hate speech / offensive language</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c14">&rarr; Demeaning social groups</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Previous research has shown that certain large LMs can degenerate into offensive language, even if the prompts themselves are not harmful or offensive (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2009.11462&amp;sa=D&amp;source=editors&amp;ust=1734479426085878&amp;usg=AOvVaw2MOLukeO21-tcTbCFDCeoE">source</a></span><span>).</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This kind of output can demean, marginalize, or oppress certain groups of people, and in some cases may even be illegal to generate.</span></p></td></tr></table><h5 class="c21" id="h.1elh8i8f5g67"><span class="c30 c11 c36">Example #5: Image Generation &amp; Exclusion</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c6">#RepresentationalHarm</span></p><p class="c3"><span class="c14">&rarr; Erasing social groups</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Imagine an image generation system that does not output images that depict identities such as transgender or non-binary people. Or a system that, when prompted with the text &ldquo;family&rdquo; outputs only families with heteronormative family structures such as one biological mother and one biological father. Or a system that assumes the American default of &ldquo;whiteness&rdquo; by interpreting an input of &ldquo;diverse cultures&rdquo; to mean cultures that are diverse relative to whiteness (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/pdf/10.1145/3593013.3594095?casa_token%3Du0s8QQkbp58AAAAA:oDY8ZNxuXELd2-sy06nl9qx3LrO9WWwYgq-qcyUfPIE9xtohXUGwB1lEvWOPYLsl2kqIvkvDpu8stw&amp;sa=D&amp;source=editors&amp;ust=1734479426090503&amp;usg=AOvVaw3lLUgx5cIwQJUFeiUQnYg1">source</a></span><span>).</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>When systems are not able to generate diverse images that represent different identities and ways of being, this can lead to erasure of these identities, and can further marginalize or exclude groups of people from the system.</span></p></td></tr></table><h5 class="c21" id="h.zbg0jyrjj34t"><span class="c11">Example #6: Misinformation Generation</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c69">#SocietalHarm</span></p><p class="c3"><span class="c14">&rarr; Information harms</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c4">Imagine someone uses a chatbot to ask about a medical symptom they are experiencing, and they are given false information as a response.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>False information can induce or reinforce false beliefs. In certain domains, this can be especially harmful, such as medicine or law. &ldquo;For example, misinformation on medical dosages may lead a user to cause harm to themselves&hellip; False legal advice, e.g. on permitted ownership of drugs or weapons, may lead a user to unwillingly commit a crime.&rdquo; (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3531146.3533088&amp;sa=D&amp;source=editors&amp;ust=1734479426094492&amp;usg=AOvVaw0cDHr1hIboY501AxkRIiVo">source</a></span><span class="c4">) If a system gives incorrect medical advice or makes incorrect health inferences, this can lead to physical and emotional harms. In some cases, if models output this information, it can also make disinformation cheaper and more effective.</span></p></td></tr></table><h4 class="c16" id="h.cf1eo97zp0wu"><span class="c30 c37 c50 c47">Mitigation Strategies</span></h4><p class="c3"><span class="c34">Using perplexity evaluation to assess the model&rsquo;s ability to generate problematic outputs</span></p><ul class="c2 lst-kix_5gev2o922b7x-0"><li class="c3 c5 li-bullet-0"><span class="c37">Perplexity</span><span>&nbsp;is a language specific metric, it is how language models are trained (to predict the token that is next in the sequence). The probability of the next token is the perplexity.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://crfm.stanford.edu/helm/latest/&amp;sa=D&amp;source=editors&amp;ust=1734479426095697&amp;usg=AOvVaw1QeiGrx-yvA9nKPgyrHI5k">HELM</a></span><span class="c4">&nbsp;is a suite that has a bunch of evaluation metrics for evaluating NLP models and LLMs, from Stanford. Good at evaluating downstream tasks and perplexity</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Evaluating Problematic Inputs and Problematic Outputs are somewhat similar and related to each other &ndash; When the model gives high probability to (e.g., text from a White Supremacy forum), it would also be likely to generate text that looks like text from that forum.</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c34">Evaluating whether your model is generating personal information</span></p><ul class="c2 lst-kix_5gev2o922b7x-0"><li class="c3 c5 li-bullet-0"><span>If models are able to memorize parts of the training data, you can evaluate the model&rsquo;s ability to generate something that was in the training data &ndash; if this is PI or copyrighted data this is problematic. </span><span class="c1"><a class="c8" href="#h.og35q642jfc6">We provide several strategies for detecting PI data in the dataset section of this playbook.</a></span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c74"><span class="c34">Evaluating Misinformation/Hallucination</span></p><p class="c3"><span class="c37">Hallucinations</span><span>&nbsp;</span><span>are factually incorrect pieces of information outputted from models, sometimes</span><span>&nbsp;generated with confidence</span><span>. It leads people to think the text is correct, when it is not (e.g., if an LM outputs &ldquo;Bigfoot is real and was seen in 1950&rdquo;). This term is also often conflated with &ldquo;misinformation.&rdquo;</span><span class="c37">&nbsp;Deep Fakes </span><span>are f</span><span>ake images that are generated by machines. Not all hallucinations or deep fakes are inherently problematic, here we provide some guidance on how to discern whether this type of generated output could cause harm or not.</span></p><ul class="c2 lst-kix_8pvyoajrhbtr-0"><li class="c3 c5 li-bullet-0"><span>When inspecting</span><span>&nbsp;</span><span>the </span><span class="c55">generation </span><span class="c4">of false outputs from a model, you can ask &ndash; Do these things sound/look real? Would they be interpreted as real by a person? What are the potential harmful consequences that could arise if someone were to think a generated output was real that was not?</span></li><li class="c3 c5 li-bullet-0"><span>For deep fake evaluation, split fake generated images into &ldquo;Convincing images that look real&rdquo; and &ldquo;Images that don&rsquo;t look real,&rdquo; </span><span>discuss</span><span class="c4">&nbsp;the different types of impact that could occur from each of these categories.</span></li><li class="c3 c5 li-bullet-0"><span>In general, evaluating hallucinations and misinformation is a nascent research discipline that would benefit from additional research.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c4"></span></p><h3 class="c43" id="h.e8r84e5mhv5x"><span class="c37 c53 c60">Measuring Societal Harm</span></h3><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.p2q15gblp38v"><span class="c30 c37 c50 c47">Common considerations</span></h4><ul class="c2 lst-kix_8pvyoajrhbtr-0"><li class="c3 c5 li-bullet-0"><span class="c4">How can we evaluate certain harms that are subjective, contested, or cannot be directly observed (e.g., fairness evaluation)?</span></li><li class="c3 c5 li-bullet-0"><span>Even if the model performs &ldquo;well&rdquo;, how is this model impacting society?</span></li></ul><h4 class="c16" id="h.pwjrkudapvtd"><span class="c37 c50 c47">Examples of harms and implication</span></h4><h5 class="c21" id="h.nc003dlg32gj"><span class="c11">Example #1: An Unfair Image Recognition Model</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c57">#QualityOfServiceHarm</span></p><p class="c3"><span class="c14">&rarr; Increased labor</span></p><p class="c3"><span class="c14">&rarr; Service or benefit loss</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>It has been shown that several widely-used image recognition models were trained on datasets containing primarily images of light-skinned individuals, and an underrepresentation of images of people with darker skin tones (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426100541&amp;usg=AOvVaw3_hzjAW3HQogG3dlOBQkh8">source</a></span><span>).</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>When deployed in real-world applications such as facial recognition systems or object detection, the model consistently performs poorly on images of individuals with darker skin tones, leading to misidentifications, errors, and biases in the model&#39;s predictions. As a result, individuals with darker skin tones may be disproportionately impacted by the model&#39;s inaccuracies and face increased risks of misidentification and discrimination in scenarios where the model is deployed, such as in surveillance or security systems. This scenario would require a sociotechnical evaluation to see how the disparity in system performance disparately impacts individuals with dark skin color.</span></p></td></tr></table><h4 class="c16" id="h.764y0t6cd07l"><span class="c37 c50 c47">Mitigation Strategies</span></h4><p class="c3"><span class="c34">Evaluating a models&rsquo; impact on society</span></p><ul class="c2 lst-kix_8pvyoajrhbtr-0"><li class="c3 c5 li-bullet-0"><span>Evaluation can be done to </span><span>see how the model performs as a technical entity vs. how it performs in the &ldquo;wild&rdquo; for people.</span><span class="c4">&nbsp;For example, evaluating the models&rsquo; technical performance through basic accuracy metrics might not provide information about if the model could have negative impacts on society. In contrast, evaluating the model through user-centric evaluation methods (such as user studies like focus groups, or even running a randomized controlled trial in &ldquo;real world&rdquo; environments), could provide more insight into potential impacts a model could have on society. Remember that in this context, you aren&rsquo;t evaluating &ldquo;is this going to be a good product&rdquo;, but you are also trying to evaluate &ldquo;is this going to cause harm&rdquo;? It is a technical evaluation, but is more focused on the downstream impacts.</span></li><li class="c3 c5 li-bullet-0"><span>If possible and appropriate, e</span><span>valuate your model in the same scenarios that the model will be used</span><span class="c4">. We note that this is often not possible, or at times might be inappropriate (e.g., you wouldn&rsquo;t want real students to use a model that you know is imperfect at being factual). A useful starting point could be to reflect on the ways in which your evaluation setup might or might not generalize to the &ldquo;real world&rdquo;.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2306.03100.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426102906&amp;usg=AOvVaw1ZMuhdr_iMJpHhLDAk1RtW">This research</a></span><span class="c4">&nbsp;provides four concrete recommendations for evaluating LLMs through a sociotechnical lens:</span></li></ul><ul class="c2 lst-kix_8pvyoajrhbtr-1 start"><li class="c3 c13 li-bullet-0"><span class="c4">Develop evaluation metrics that prioritize understanding people&#39;s actual needs and values in downstream use cases, rather than solely relying on automatic metrics like &quot;accuracy.&quot;</span></li><li class="c3 c13 li-bullet-0"><span class="c4">Ensure that methods for evaluating language models&#39; performance in real-world settings are informed by and validated with insights from studies focusing on measuring the requirements for better outcomes, which might require further formalization and abstraction of automatic metrics.</span></li><li class="c3 c13 li-bullet-0"><span class="c4">Investigate the concept of &quot;use cases&quot; to determine their discriminative power and appropriate level of abstraction for comprehensive benchmarking across different applications, considering both descriptive and generative power.</span></li><li class="c3 c13 li-bullet-0"><span class="c4">Prioritize lowering evaluation costs based on the technology development stage and claimed contributions, while also considering types of costs such as computing and environmental impact, ensuring responsible evaluation practices across different stages of model development and deployment.</span></li></ul><ul class="c2 lst-kix_8pvyoajrhbtr-0"><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2212.09746.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426103871&amp;usg=AOvVaw31fJahANZYFjJrqtS7SOIt">This research</a></span><span class="c4">&nbsp;introduces &ldquo;Human-AI Language-based Interaction Evaluation (HALIE)&rdquo;, which defines the components of interactive systems and dimensions to consider when designing evaluation metrics for human-language model interaction.</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c45 c37">How to </span><span class="c45 c37">select the most appropriate metric</span><span class="c34">&nbsp;to attempt to measure certain kinds of harm:</span></p><p class="c3"><span>Harm is not always measured in a metric-based way, but is often measured through typical model performance metrics (e.g., precision and recall). If there is a large difference in model performance across or within groups, then this could be an indication of harm. For example, </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://aclanthology.org/2021.tacl-1.74.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426104746&amp;usg=AOvVaw0tswaE-KN2lQVXWN32MmFM">this research</a></span><span>&nbsp;compares different fairness metrics that are used in NLP, and showcases how performance metrics can be adapted for evaluating social constructs such as fairness.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c34">Measuring inherently unobservable constructs</span></p><ul class="c2 lst-kix_15risnu9xchg-0 start"><li class="c3 c5 li-bullet-0"><span>Measurement theory from the social sciences provides a multi-step process that can be completed to assess how well a chosen measurement captures an unobservable / theoretical construct. </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/pdf/10.1145/3442188.3445901&amp;sa=D&amp;source=editors&amp;ust=1734479426105549&amp;usg=AOvVaw2hb7M3cEZ3rbsyJ20jPA0H">This paper</a></span><span class="c4">&nbsp;describes each of these steps in detail, and shows examples of how measurement theory can be used for a machine learning context when attempting to measure the unobservable construct of fairness. Adapted from that paper, the following table details each step to evaluate construct validity in machine learning models:</span></li></ul><p class="c3 c33"><span class="c4"></span></p><table class="c86"><tr class="c7"><td class="c83" colspan="1" rowspan="1"><p class="c17"><span class="c19">Steps</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c17"><span class="c19">Description</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c17"><span class="c19">Probes to Evaluate</span></p></td></tr><tr class="c7"><td class="c83" colspan="1" rowspan="1"><p class="c17"><span class="c19">Face Validity</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c17"><span class="c4">Subjective first pass to check if the measurements obtained from a measurement model look reasonable.</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c17"><span class="c4">Do the measurements look like a reasonable depiction of the theoretical construct?</span></p></td></tr><tr class="c7"><td class="c83" colspan="1" rowspan="1"><p class="c17"><span class="c19">Content Validity</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c17"><span class="c4">A check to see how well the measurement captures the theoretical construct (or which interpretation of that construct is being measured).</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c17"><span class="c4">Do the measurements capture the specific definition of fairness we have chosen? Do our proxies capture only observable variables related to the original construct and nothing more? Does our measurement capture the relationship between the non-observable construct and its proxy variables?</span></p></td></tr><tr class="c7"><td class="c83" colspan="1" rowspan="1"><p class="c17"><span class="c19">Convergent Validity</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c17"><span class="c4">A check to see how much the current mea- surement differs from previous measurements of a similar construct.</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c17"><span class="c4">Do the measurements give the same results as a previous metric (if so, is this new metric necessary?) Do the measurements give different results as a previous metric (if so, are these differences justified?) Do the measurements give subtly different results as previous metrics (if so, are those differences justified?)</span></p></td></tr><tr class="c7"><td class="c83" colspan="1" rowspan="1"><p class="c17"><span class="c19">Discriminant Validity</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c17"><span class="c4">A check to see how this measurement might unintentionally measure other constructs.</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c17"><span class="c4">Are there any correlations between our measurements and measurements of other constructs that are not related to our fairness definition?</span></p></td></tr><tr class="c7"><td class="c83" colspan="1" rowspan="1"><p class="c17"><span class="c19">Predictive Validity</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c17"><span class="c4">A check to see how useful the measurements are.</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c17"><span class="c4">How well do these measurements predict observable / non-observable properties related to our theoretical construct?</span></p></td></tr><tr class="c7"><td class="c83" colspan="1" rowspan="1"><p class="c17"><span class="c19">Consequential Validity</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c17"><span class="c4">A check to acknowledge the consequences of this measurement model.</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c17"><span class="c4">How is the world shaped by these measurements? What world do we wish to live in?</span></p></td></tr></table><p class="c3 c33"><span class="c4"></span></p><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c30 c37 c96 c89">Stage #6</span></p><h1 class="c29" id="h.77xzxk80tguh"><span class="c30 c82 c77 c89">Model Use &amp; Monitoring</span></h1><p class="c48 subtitle" id="h.nw38ifr22v12"><span class="c41">Mitigating Risks To and From Users</span></p><p class="c3"><span>In this stage of the AI lifecycle, we focus on the risks that can arise after a model has been deployed. It is recommended that these risks are evaluated and mitigated </span><span class="c55">before</span><span class="c4">&nbsp;deployment, however, you can still utilize these mitigation strategies after deployment as well.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span>Once a model is deployed and being used, it is essential to continue monitoring its use and potential risks, especially considering that user behavior is unpredictable and sometimes intentionally malicious. </span><span>Although we are not responsible for every unanticipated consequence of a system, we as practitioners are responsible to prevent as many harms as possible (anticipated or otherwise). For example, it is impossible to determine all of the ways that a model could be maliciously used, but as a practitioner, did you do everything in your power to prevent malicious use? In this section, we center you &ndash; the practitioner &ndash; and provide guidance to help you identify and mitigate potential harms and risks that could arise from the use of your systems.</span></p><hr><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c30 c82 c75 c71">Topics of Interest</span></p><ul class="c2 lst-kix_w7uhve9aueba-0"><li class="c3 c5 li-bullet-0"><span class="c45 c37 c53"><a class="c8" href="#h.ntis6v3astby">Refusals and Safeguards</a></span></li><li class="c3 c5 li-bullet-0"><span class="c45 c37 c53"><a class="c8" href="#h.a01wsow1ftqv">Harms from Use</a></span></li><li class="c3 c5 li-bullet-0"><span class="c45 c37 c53"><a class="c8" href="#h.gua72c6fk6ns">Appeals &amp; Recourse to Humans</a></span></li></ul><hr><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><h3 class="c43 c88" id="h.f1us3xao30x6"><span class="c0"></span></h3><h2 class="c59" id="h.djgt6qygpmml"><span class="c20">Transparency &amp; Documentation Checklist</span></h2><p class="c48 subtitle" id="h.r031f1ba2jl0"><span class="c30 c73 c36">Model Use &amp; Monitoring</span></p><p class="c3"><span class="c4">In this stage of the AI lifecycle, we recommend you discuss and document the following:</span></p><ul class="c2 lst-kix_8b09c5wbiylz-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">What are the anticipated malicious uses of your model? What are unanticipated malicious uses of your model?</span></li></ul><ul class="c2 lst-kix_qzcbcbds5p8j-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">What have you done to prevent these potential malicious uses?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What safeguards and/or refusals might be appropriate or necessary for your model?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What method do you plan to use to incorporate refusals and/or safeguards into your model&rsquo;s design?</span></li><li class="c3 c5 li-bullet-0"><span class="c72 c75">What recourse mechanisms could you incorporate into the design of your system, and which mechanisms do you plan to incorporate?</span><hr style="page-break-before:always;display:none;"></li></ul><h3 class="c43" id="h.ntis6v3astby"><span class="c37 c53 c60">Refusals and Safeguards</span></h3><p class="c3"><span class="c45 c37 c47">Definitions / Relevant Terms</span></p><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c37">Safeguards: </span><span class="c4">general measures implemented to ensure responsible, ethical, and equitable use of AI systems.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c37">Refusals:</span><span>&nbsp;mechanisms implemented to reject or exclude inputs deemed potentially harmful or problematic, thereby mitigating the risk of generating undesirable or unethical outputs</span></p><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.44fwajgf9cqy"><span class="c10">Common considerations</span></h4><ul class="c2 lst-kix_8pvyoajrhbtr-0"><li class="c3 c5 li-bullet-0"><span class="c4">Does my model need to have refusals and/or safeguards incorporated into its design?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What kind of refusals and/or safeguards are necessary to prevent potential harm?</span></li><li class="c3 c5 li-bullet-0"><span>How can I design refusals and/or safeguards to prevent harmful use of my system?</span></li></ul><h4 class="c16" id="h.mgcu157pym4t"><span class="c10">Examples of harms and implications</span></h4><h5 class="c21" id="h.x4nk9zeb3o6d"><span class="c11">Example #1: Model&rsquo;s refusals are easily circumventable</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c28 c22">#InterpersonalHarm</span></p><p class="c3"><span class="c14">&rarr; Technology-facilitated violence / malicious uses</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>If a user asks a model to generate stereotypes about a race, refusals might be in place to prevent the model from generating that output. However, a user might be able to circumvent this refusal. For example, if the user tells the model that it is a joke, the language model might now generate this problematic output.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Once refusals are created, it can be easy to rest on the idea that the model will no longer generate certain outputs. However, if these refusals are circumventable, then the refusal is not functioning in the capacity it is expected to, and the harm is not fully mitigated.</span></p></td></tr></table><h5 class="c21" id="h.jqtkzqqi2efx"><span class="c11">Example #2: Too much refusal</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c35 c22 c69">#SocietalHarm</span></p><p class="c3"><span class="c14">&rarr; Culture harms</span></p><p class="c3"><span class="c28 c22">#InterpersonalHarm</span></p><p class="c3"><span class="c14">&rarr; Loss of agency/social control</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>A system is created with overuse of refusals, in an effort to be overly safe.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Creating and implementing refusals, when done to an extreme, can lead to censorship or loss of freedom of speech, as well as preventing certain types of discussions (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3173574.3173889&amp;sa=D&amp;source=editors&amp;ust=1734479426124568&amp;usg=AOvVaw2-KFTHKvZVOAbQ-yWgg-hf">source</a></span><span>).</span></p></td></tr></table><h4 class="c16" id="h.8ekuanv6nbpd"><span class="c30 c10">Mitigation Strategies</span></h4><p class="c3"><span class="c4">For both language and image generation models, we recommend including an explicit indication in the user interface to clarify that outputs may not be factual or real, emphasizing the need for human verification.</span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c34">Designing Safeguards</span></p><p class="c3"><span class="c4">To begin designing safeguards, we recommend you start by answering the following questions, and determining if any safeguards need to be incorporated into the system to prevent these from occurring:</span></p><ul class="c2 lst-kix_q24skcnjivy0-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">Could my model induce or reinforce false beliefs?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Could my model output highly sensitive information (e.g., violence or racism)?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">Could my model mislead users?</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c4">There are several ways to design triggers for problematic inputs/outputs. Here we describe several examples and approaches.</span></p><ul class="c2 lst-kix_ykbty9k091s5-0 start"><li class="c3 c5 li-bullet-0"><span class="c37">Content moderation: </span><span>You can utilize content moderation as a method to establish safeguards for language or generative models by systematically reviewing and filtering user-generated content to ensure adherence to guidelines, ethics, and legal standards. This process enhances the model&#39;s ability to produce safe and responsible outputs by identifying and removing inappropriate or harmful inputs. </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://journals.sagepub.com/doi/pdf/10.1177/2053951719897945?utm_campaign%3DEverything%2520in%2520Moderation%26utm_medium%3Demail%26utm_source%3DRevue%2520newsletter&amp;sa=D&amp;source=editors&amp;ust=1734479426127242&amp;usg=AOvVaw1VYMZbEHO4c2euqSKfIrt8">This paper</a></span><span class="c4">&nbsp;provides a technical primer for how to conduct algorithmic content moderation.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Keyword engineering</span><span class="c4">&nbsp;entails strategically selecting and incorporating keywords into the input data to guide language or generative models towards desired outputs, facilitating the design of safeguards by ensuring keywords align with safe and responsible content generation.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Prompt engineering</span><span>&nbsp;involves crafting precise and tailored prompts to guide language or generative models towards desired outputs, enabling the design of safeguards by structuring prompts to mitigate the generation of problematic inputs. Prompt engineering can be applied after model training (which makes it less expensive and time-consuming than pre-training or during-training approaches). It was </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2&amp;sa=D&amp;source=editors&amp;ust=1734479426128196&amp;usg=AOvVaw2TIUJTU1hcBcyNutOOwQL4">reported</a></span><span>&nbsp;that prompt engineering allowed DALL-E 2 to improve diversity of generated humans by 12x without model retraining. </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/pdf/10.1145/3593013.3594095?casa_token%3DAEBlrc0iekMAAAAA:Ct5TEhFvbZoKFvsde-BX06DSdghab9Dn5nZkD0fo22Xuz6JHIX6dohEUqZ5UK7nCgVNUx4h8coWaQA&amp;sa=D&amp;source=editors&amp;ust=1734479426128740&amp;usg=AOvVaw2WotwQu13sUQDb2HSncXw7">Other research</a></span><span>&nbsp;has also shown how prompt engineering can be used to understand how generative models might perpetuate social biases, and </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2210.09150.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426129136&amp;usg=AOvVaw2BrVkbfMHulLU6hB2Mp8Zh">this research</a></span><span class="c4">&nbsp;found that well-engineered prompts can lower social bias in chatbot responses.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span class="c45 c37">Red teaming</span></p><ul class="c2 lst-kix_v8i0z8oris9z-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">Red-teaming is one method to evaluate a model that seeks to uncover model vulnerabilities that could lead to harm, this could include simulating what users might look like, seeing what interactions a potential user could have and mitigating harms based on that.</span></li><li class="c3 c5 li-bullet-0"><span>HuggingFace provides this </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://huggingface.co/blog/red-teaming&amp;sa=D&amp;source=editors&amp;ust=1734479426129924&amp;usg=AOvVaw2XvWmk3agk785-Pa8P9Lc0">overview</a></span><span class="c4">&nbsp;of red-teaming in the context of LLMs, that also includes resources for how to conduct your own red-teaming evaluation.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2209.07858.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426130426&amp;usg=AOvVaw0LabhnLFwCsbdJS6XuHxlv">This study</a></span><span>&nbsp;also conducted extensive red teaming on various kinds of LMs, and provides their instructions, processes, and statistical methodologies for doing so.</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c45 c37">Designing Refusals</span></p><p class="c3"><span>Here we describe t</span><span class="c4">wo separate approaches for designing refusals. While both of these approaches are circumventable, they are the currently accepted best practices:</span></p><ul class="c2 lst-kix_v8i0z8oris9z-0"><li class="c3 c5 li-bullet-0"><span class="c37">Option 1:</span><span>&nbsp;</span><span>Build a manual filter</span><span>&nbsp;that blocks queries that contain certain words / certain topics. This is common to do during the dataset curation stage (e.g., C4, RefinedWeb, or ROOTS). </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00434/108865/Self-Diagnosis-and-Self-Debiasing-A-Proposal-for&amp;sa=D&amp;source=editors&amp;ust=1734479426131373&amp;usg=AOvVaw3qpnqMR-Qh9PX2QBNZSLkB">This paper</a></span><span class="c4">&nbsp;reuses the C4 list as a baseline approach for filtering outputs.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Option 2: </span><span>Block certain generated responses from the model. We recommend that if you design refusals for this purpose, you evaluate your model&rsquo;s ability to generate accurate refusals (and to avoid over generating refusals). For example, </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2307.09288.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426131903&amp;usg=AOvVaw3oI37B-rYkpTss2LNqhBtY">this research</a></span><span class="c4">&nbsp;used OpenAI&rsquo;s Borderline Dataset to prompt a language model using prompts that might trick a system into refusal even when the prompt is not adversarial.</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c34">Conduct Empirical Manual Audits</span></p><ul class="c2 lst-kix_v8i0z8oris9z-0"><li class="c3 c5 li-bullet-0"><span class="c4">Take and note observations from a small subset of data from user interactions with the model. Annotate noticeable model failures. This might help you discern if you need to adjust the training data, the filters, the refusals, etc.</span></li><li class="c3 c5 li-bullet-0"><span>You can look at actual queries from users and manually detect if any problematic inputs/outputs are being allowed when they should not be.</span><hr style="page-break-before:always;display:none;"></li></ul><h3 class="c43" id="h.a01wsow1ftqv"><span class="c37 c53 c60">Harms from Use</span></h3><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.eqrnjl7jr94z"><span class="c10">Common considerations</span></h4><ul class="c2 lst-kix_8pvyoajrhbtr-0"><li class="c3 c5 li-bullet-0"><span class="c4">What ways can my model be maliciously used? In what ways is it already being maliciously used?</span></li><li class="c3 c5 li-bullet-0"><span class="c4">What kinds of harm could occur / are occuring from the users of this model?</span></li><li class="c3 c5 li-bullet-0"><span>How can I prevent harm caused by users?</span></li></ul><h4 class="c16" id="h.j8en1a9vyfy9"><span class="c10">Examples of harms and implications</span></h4><h5 class="c21" id="h.srpx60ydpjle"><span class="c11">Example #1: Malicious uses</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c28 c22">#InterpersonalHarm</span></p><p class="c3"><span class="c14">&rarr; Technology-facilitated violence / malicious uses</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Users might use an LM to create targeted disinformation campaigns, commit fraud, generate malware or other code that leads to cybersecurity threats (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3531146.3533088&amp;sa=D&amp;source=editors&amp;ust=1734479426135093&amp;usg=AOvVaw2j4rpaoyPThPQqqKouMKUZ">source</a></span><span>).</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>If an LM is capable of generating these kinds of outputs, it cannot prevent malicious users from abusing it and causing harm. The model and its designers are implicit in these malicious actions if there are no preventative safeguards to attempt to stop this behavior.</span></p></td></tr></table><h5 class="c21" id="h.mhys5ak9zwkf"><span class="c11">Example #2: Poisoning attacks</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c28 c22">#InterpersonalHarm</span></p><p class="c3"><span class="c14">&rarr; Technology-facilitated violence / malicious uses</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Users are able to manipulate the training data for a generative model by injecting poison samples into the training pipeline</span><span>&nbsp;</span><span>(</span><span class="c70"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2310.13828.pdf?utm_source%3Dnewsletter%26utm_medium%3Demail%26utm_campaign%3Dsendto_newslettertest_technology%26stream%3Dtop&amp;sa=D&amp;source=editors&amp;ust=1734479426138235&amp;usg=AOvVaw0t6KrDa-B9v9uT64AxRh_b">source</a></span><span>)</span><span>.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>These kinds of attacks can destabilize a model at best, and can render a model useless or increase the models&rsquo; ability to produce problematic outputs at worst.</span></p></td></tr></table><h5 class="c21" id="h.9oi4v7j7sgb"><span class="c11">Example #3: Inciting violence or inappropriate behavior</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c28 c22">#InterpersonalHarm</span></p><p class="c3"><span class="c14">&rarr; Technology-facilitated violence / malicious uses</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Imagine if someone is able to use an image generation system to create non-consensual sexual imagery of someone for their own use or to share with others.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This can lead to sexual harassment, discomfort, or violence against victims &ndash; all as a result of actions that occurred without their consent.</span></p></td></tr></table><p class="c3 c33"><span class="c19"></span></p><h4 class="c16" id="h.hrl5h7s6mg8o"><span class="c10">Mitigation Strategies</span></h4><p class="c3"><span class="c45 c37">Red teaming</span></p><ul class="c2 lst-kix_v8i0z8oris9z-0"><li class="c3 c5 li-bullet-0"><span class="c4">Red-teaming is one method to evaluate a model that seeks to uncover model vulnerabilities that could lead to harm, this could include simulating what users might look like, seeing what interactions a potential user could have and mitigating harms based on that.</span></li><li class="c3 c5 li-bullet-0"><span>HuggingFace provides this </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://huggingface.co/blog/red-teaming&amp;sa=D&amp;source=editors&amp;ust=1734479426143302&amp;usg=AOvVaw0AMISWiI1sDlfratS5-CE0">overview</a></span><span class="c4">&nbsp;of red-teaming in the context of LLMs, that also includes resources for how to conduct your own red-teaming evaluation.</span></li><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2209.07858.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426144098&amp;usg=AOvVaw3zRhO1oi6TYzB5dOLExa6g">This study</a></span><span class="c4">&nbsp;also conducted extensive red teaming on various kinds of LMs, and provides their instructions, processes, and statistical methodologies for doing so.</span></li><li class="c3 c5 li-bullet-0"><span>Simulate what users might look like</span><span>, seeing what interactions a potential user could have and mitigating harms based on that. Simulating users could imply manual auditing (taking on user personas and interacting with a system), or technical simulation of users, such as </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/2310.11501&amp;sa=D&amp;source=editors&amp;ust=1734479426144976&amp;usg=AOvVaw0goZKrdPDod7XlJXU0ac3e">this research</a></span><span class="c4">, which introduces CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic.</span></li></ul><p class="c3 c33"><span class="c19"></span></p><p class="c3"><span class="c45 c37">Tracking</span><span class="c34">&nbsp;model usage</span></p><ul class="c2 lst-kix_v8i0z8oris9z-0"><li class="c3 c5 li-bullet-0"><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/pdf/10.1145/3461702.3462566&amp;sa=D&amp;source=editors&amp;ust=1734479426145756&amp;usg=AOvVaw3xpg0wSmeWQOTfaS76vBOZ">This previous work</a></span><span>&nbsp;discusses and explores &ldquo;Misuse Indicators&rdquo; as a method to uncover problematic, harmful, or malicious user behavior.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3 c33"><span class="c4"></span></p><hr style="page-break-before:always;display:none;"><p class="c3 c33"><span class="c4"></span></p><h3 class="c43" id="h.gua72c6fk6ns"><span class="c37 c53 c60">Appeals &amp; Recourse to Humans</span></h3><hr><p class="c3 c33"><span class="c4"></span></p><h4 class="c16" id="h.q0wzl8893lyq"><span class="c10">Common considerations</span></h4><ul class="c2 lst-kix_5jkev5cbxpxc-0 start"><li class="c3 c5 li-bullet-0"><span class="c4">When is recourse necessary when building an AI system?</span></li><li class="c3 c5 li-bullet-0"><span>If the humans who interact with this model disagree with the system&rsquo;s outputs, what mechanisms should be in place to allow for human correction or recourse?</span></li></ul><h4 class="c16" id="h.wwflu35p60t1"><span class="c10">Examples of harms and implications</span></h4><h5 class="c21" id="h.n5ia46ykaa1c"><span class="c11">Example #1: No human recourse mechanisms</span></h5><table class="c23"><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c22">Harm</span><span class="c35 c22">&nbsp;Type(s)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span class="c28 c22">#InterpersonalHarm</span></p><p class="c3"><span class="c55">&rarr; Loss of agency / social control</span></p><p class="c3"><span class="c14">&rarr; Technology-facilitated violence / malicious uses</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Case Study</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>Imagine a social media platform implements an AI-powered image generation feature that allows users to create highly realistic images of people who do not exist (deep fakes). This feature is intended for entertainment purposes, such as creating avatars or fictional characters, but it quickly becomes popular for creating deceptive content.</span></p></td></tr><tr class="c7"><td class="c26" colspan="1" rowspan="1"><p class="c3"><span class="c19">Harms &amp; Implications</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c3"><span>This feature might allow users to start misusing the AI image generation feature to create fake images of real individuals, superimposing their faces onto explicit or controversial content without their consent. These fake images, if shared, can lead to reputational harm, harassment, and even threats to the individuals depicted in the images. Without an ability for humans to correct or report misuse of the system, victims of this misuse find themselves unable to effectively address the issue solely through the platform&#39;s automated reporting systems. This kind of AI-generated content is often not detected as violating platform policies, leading to a lack of meaningful recourse for the affected individuals. Despite clear evidence of harm, this AI system lacks mechanisms for human correction or recourse. Victims find it challenging to have the fake images removed or to prevent their further dissemination, which can lead to prolonged distress and damage to their reputation.</span></p></td></tr></table><h4 class="c16" id="h.w77qdjmg360c"><span class="c10">Mitigation Strategies</span></h4><p class="c3"><span class="c45 c37">Designing and implementing human </span><span class="c37 c45">recourse</span><span class="c34">&nbsp;mechanisms:</span></p><p class="c3"><span class="c4">Human moderators or review mechanisms are needed to evaluate reported content, assess its authenticity and potential harm, and take appropriate action. Recourse mechanisms should also ensure transparency by clearly informing users about the nature of the AI-generated content they encounter. This transparency empowers users to make informed decisions about their interactions with such content and seek assistance when needed. Here we provide some methods for incorporating recourse mechanisms into the design of your system:</span></p><ul class="c2 lst-kix_m2rwr78ppx87-0 start"><li class="c3 c5 li-bullet-0"><span class="c37">Human-in-the-loop Moderation</span><span>: Incorporate human moderators into the content moderation process, where flagged or disputed content is reviewed by human reviewers to assess its compliance with platform policies and potential harm (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://idl.iscram.org/files/daniellink/2016/1401_DanielLink_etal2016.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426150813&amp;usg=AOvVaw2lWhheYPoVODQg2dUUm5ii">source</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Crowdsourced Verification: </span><span>Utilize crowdsourcing platforms to enlist human annotators for verifying the authenticity or appropriateness of AI-generated content, particularly in cases of disputed or controversial content (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/pdf/10.1145/2441776.2441923?casa_token%3DZM3Sj9iga5oAAAAA:SOvhSTVjLCAl72eeY2SksD86553-gEyFPw7KDEJ65XdkzzPucAd5ywlydjwWvmnNye-eE18Nrq-13g&amp;sa=D&amp;source=editors&amp;ust=1734479426151563&amp;usg=AOvVaw0zngSgteOgIBBdMJzdVbR2">source</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Transparency and Explanation Interfaces: </span><span>Implement interfaces that provide users with explanations of AI-generated content, including its origin, purpose, and potential implications, enabling users to make informed decisions and report inaccuracies or misuse (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber%3D8466590&amp;sa=D&amp;source=editors&amp;ust=1734479426152101&amp;usg=AOvVaw0PFCdbJIdGu30pZg2qemgg">source</a></span><span class="c4">).</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Community Reporting Systems: </span><span>Establish community-driven reporting systems where users can flag problematic content and provide context or evidence to support their claims, facilitating more informed moderation decisions. </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/pdf/10.1145/3274301&amp;sa=D&amp;source=editors&amp;ust=1734479426152605&amp;usg=AOvVaw0qDBzxkopwNihYWfF2ehEd">This paper</a></span><span>&nbsp;showcases how community driven and reporting systems can be utilized by users through an analysis of 2.8 million removed comments on Reddit.</span></li><li class="c3 c5 li-bullet-0"><span class="c37">Appeals Mechanisms: </span><span>Implement formal appeals processes where users can challenge moderation decisions regarding AI-generated content, providing them with an opportunity to present additional information or context. </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://dl.acm.org/doi/pdf/10.1145/3491102.3517606?casa_token%3DQHWWHwxpHTYAAAAA:mspOwxYnuuUIRDra1rdcKAj-GZyhNWW-4oooap0hglCoMY4Q7Cyo7vZRLBbvLGcACSTc3ddiqDTFZQ&amp;sa=D&amp;source=editors&amp;ust=1734479426153252&amp;usg=AOvVaw0-h8wssnn69L4Y9JqS0N0x">This research</a></span><span>&nbsp;showcases that humans prefer human review, even if it takes longer for recourse to occur. </span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://www.nature.com/articles/s41746-023-00906-8.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426153684&amp;usg=AOvVaw1-I8Kyx2n7iqeemZ-v-82b">This research</a></span><span>&nbsp;describes how human expert judges can be included in appeals processes for reviewing algorithmic decisions.</span></li></ul><p class="c3 c33"><span class="c4"></span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3 c33"><span class="c4"></span></p><p class="c3"><span>This work would not have been possible without the support of many individuals and institutions. We express gratitude for the helpful discussions and feedback from our teammates and close collaborators, including Lucy Li, Remi Denton, David Widder, Katie Shilton, Maarten Sap, Artem A Trotsyuk, Dawn Bloxwich, Stephanie Bell, Quinn Waeiss, Will Smith, Margaret Levi, Yacine Jernite, Aviya Skowron, Luca Soldaini, Su Lin Blodgett, Margaret Mitchell, Casey Fiesler, Robin Burke, Motahhare Eslami, Hoda Heidari.</span></p><p class="c3 c33"><span class="c4"></span></p><div><p class="c18"><span class="c30 c56 c37"></span></p><p class="c64"><span class="c56 c37"><br>Contact </span><span class="c1 c37 c76"><a class="c8" href="mailto:Jessie.Smith-1@colorado.edu">Jessie.Smith-1@colorado.edu</a></span><span class="c56 c37">, </span><span class="c1 c37 c76"><a class="c8" href="mailto:hanwend@andrew.cmu.edu">hanwend@andrew.cmu.edu</a></span><span class="c56 c37">,</span><span class="c37 c56">&nbsp;</span><span class="c1 c37 c76"><a class="c8" href="mailto:jessed@allenai.org">jessed@allenai.org</a></span><span class="c30 c56 c37">&nbsp; &nbsp;</span></p><p class="c18"><span class="c30 c56 c37"></span></p><p class="c51"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 86.40px; height: 28.51px;"><img alt="" src="images/image1.jpg" style="width: 86.40px; height: 28.51px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c18"><span class="c30 c87 c37 c47"></span></p></div><hr class="c99"><div><p class="c17 c85"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c91">&nbsp;T</span><span>he law is often guided by ethical principles, and many of the mitigations and suggestions in this playbook might not be legally required (yet). Even though the law is slow, it eventually catches up with innovation. If some decisions are technically legal but aren&rsquo;t best practices, we recommend reflecting on whether this is a good design decision. We also recommend incorporating your organizations&rsquo; values into the design of your technologies, e.g., when choosing to respect copyright law or excluding personal information from a dataset (</span><span class="c1"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/2206.03216.pdf&amp;sa=D&amp;source=editors&amp;ust=1734479426155121&amp;usg=AOvVaw3PdDJ-0hV5np5V5THWHq_5">source</a></span><span>).</span></p></div><div><p class="c17 c85"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c30 c65">&nbsp;We note that copyright laws might impact a model&rsquo;s legally allowed ability to generate copyrighted outputs such as text or images.</span></p></div></body></html>